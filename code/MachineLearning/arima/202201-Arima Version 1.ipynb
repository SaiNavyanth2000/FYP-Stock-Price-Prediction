{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "from datetime import date\n",
    "import yfinance as yf\n",
    "\n",
    "yf.pdr_override()\n",
    "import pandas as pd\n",
    "\n",
    "ticker_list=[\"AAPL\", \"MSFT\", \"AMZN\", \"TSLA\", \"GOOGL\"]\n",
    "today = date.today()\n",
    "# We can get data by our choice by giving days bracket\n",
    "start_date= \"2015-01-01\"\n",
    "end_date=\"2020-11-30\"\n",
    "\n",
    "files=[]\n",
    "def getData(ticker):\n",
    "    data = pdr.get_data_yahoo(ticker, start=start_date, end=today)\n",
    "    # dataname= ticker+\"_\"+str(today)\n",
    "    files.append((data,ticker))\n",
    "    \n",
    "for tik in ticker_list:\n",
    "    getData(tik)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-12-31</th>\n",
       "      <td>18.233213</td>\n",
       "      <td>19.121429</td>\n",
       "      <td>18.178572</td>\n",
       "      <td>19.006071</td>\n",
       "      <td>16.439852</td>\n",
       "      <td>659492400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>19.779285</td>\n",
       "      <td>19.821428</td>\n",
       "      <td>19.343929</td>\n",
       "      <td>19.608213</td>\n",
       "      <td>16.960695</td>\n",
       "      <td>560518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>19.567142</td>\n",
       "      <td>19.631071</td>\n",
       "      <td>19.321428</td>\n",
       "      <td>19.360714</td>\n",
       "      <td>16.746613</td>\n",
       "      <td>352965200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>19.177500</td>\n",
       "      <td>19.236786</td>\n",
       "      <td>18.779642</td>\n",
       "      <td>18.821428</td>\n",
       "      <td>16.280138</td>\n",
       "      <td>594333600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-07</th>\n",
       "      <td>18.642857</td>\n",
       "      <td>18.903570</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>18.710714</td>\n",
       "      <td>16.184374</td>\n",
       "      <td>484156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-24</th>\n",
       "      <td>160.020004</td>\n",
       "      <td>162.300003</td>\n",
       "      <td>154.699997</td>\n",
       "      <td>161.619995</td>\n",
       "      <td>161.619995</td>\n",
       "      <td>162706700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-25</th>\n",
       "      <td>158.979996</td>\n",
       "      <td>162.759995</td>\n",
       "      <td>157.020004</td>\n",
       "      <td>159.779999</td>\n",
       "      <td>159.779999</td>\n",
       "      <td>115798400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-26</th>\n",
       "      <td>163.500000</td>\n",
       "      <td>164.389999</td>\n",
       "      <td>157.820007</td>\n",
       "      <td>159.690002</td>\n",
       "      <td>159.690002</td>\n",
       "      <td>108275300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-27</th>\n",
       "      <td>162.449997</td>\n",
       "      <td>163.839996</td>\n",
       "      <td>158.279999</td>\n",
       "      <td>159.220001</td>\n",
       "      <td>159.220001</td>\n",
       "      <td>116691400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-28</th>\n",
       "      <td>165.710007</td>\n",
       "      <td>170.350006</td>\n",
       "      <td>162.800003</td>\n",
       "      <td>170.330002</td>\n",
       "      <td>170.330002</td>\n",
       "      <td>179485800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2287 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2012-12-31   18.233213   19.121429   18.178572   19.006071   16.439852   \n",
       "2013-01-02   19.779285   19.821428   19.343929   19.608213   16.960695   \n",
       "2013-01-03   19.567142   19.631071   19.321428   19.360714   16.746613   \n",
       "2013-01-04   19.177500   19.236786   18.779642   18.821428   16.280138   \n",
       "2013-01-07   18.642857   18.903570   18.400000   18.710714   16.184374   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2022-01-24  160.020004  162.300003  154.699997  161.619995  161.619995   \n",
       "2022-01-25  158.979996  162.759995  157.020004  159.779999  159.779999   \n",
       "2022-01-26  163.500000  164.389999  157.820007  159.690002  159.690002   \n",
       "2022-01-27  162.449997  163.839996  158.279999  159.220001  159.220001   \n",
       "2022-01-28  165.710007  170.350006  162.800003  170.330002  170.330002   \n",
       "\n",
       "               Volume  \n",
       "Date                   \n",
       "2012-12-31  659492400  \n",
       "2013-01-02  560518000  \n",
       "2013-01-03  352965200  \n",
       "2013-01-04  594333600  \n",
       "2013-01-07  484156400  \n",
       "...               ...  \n",
       "2022-01-24  162706700  \n",
       "2022-01-25  115798400  \n",
       "2022-01-26  108275300  \n",
       "2022-01-27  116691400  \n",
       "2022-01-28  179485800  \n",
       "\n",
       "[2287 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = 'AAPL'\n",
    "data = pdr.get_data_yahoo(ticker, start=start_date, end=today)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup firebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "\n",
    "cred_obj = firebase_admin.credentials.Certificate('../../../fyp2022-stockpriceprediction-firebase-adminsdk-ku62m-f9ed330292.json')\n",
    "fyp_app = firebase_admin.initialize_app(cred_obj, {\n",
    "\t'databaseURL':\"https://fyp2022-stockpriceprediction-default-rtdb.asia-southeast1.firebasedatabase.app/\",\n",
    "\t'storageBucket': 'fyp2022-stockpriceprediction.appspot.com'\n",
    "\t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://cloud.google.com/storage/docs/downloading-objects#storage-download-object-python\n",
    "from firebase_admin import storage\n",
    "b = storage.bucket()\n",
    "for ind, file in enumerate(b.list_blobs()):\n",
    "    if(i<3):\n",
    "        continue\n",
    "    #print(file.download_to_filename('1.zip'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://storage.googleapis.com/fyp2022-stockpriceprediction.appspot.com/normalizers/TSLA/multi_lstm.pkl?Expires=1644330859&GoogleAccessId=firebase-adminsdk-ku62m%40fyp2022-stockpriceprediction.iam.gserviceaccount.com&Signature=lWzBiW7D3%2Foa%2Fi8ySOsUe8JGUzMFPS%2BYBDqrEDzLDM%2BBX1BcVtWKqZHDc7zQzc42QDlcYwP2Bivb7ohRhoDUFXvfgbUUQlZDZe3kxVvC45ZR8%2BcMcpuf6seUChm4l24K6ZGGS4m4b73OTAJo3lpvCIJL6BfkajvsqRxcxxYDtXJS7cAlHCuQTZh3IA0pMWbVXDaHeqKpcuO4dfWpbkFLHmin9jpwxTQZHX9ttbuPQ%2Fb791v%2FnlUyzT8aatWMYkh%2BzmWzjB4v94u3RmSbcxc3kvdVk4aHPE1YAYI3bNDBWd4nYEYQ9iR4OSDRC2%2B7Ll9qbEEP8cC9%2F0gQbMFRcHzLxg%3D%3D'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/53304517/how-to-retrieve-image-from-firebase-storage-using-python\n",
    "file.generate_signed_url(timedelta(seconds=300), method='GET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://storage.googleapis.com/fyp2022-stockpriceprediction.appspot.com/normalizers/TSLA/multi_lstm.pkl?Expires=1644330859&GoogleAccessId=firebase-adminsdk-ku62m%40fyp2022-stockpriceprediction.iam.gserviceaccount.com&Signature=lWzBiW7D3%2Foa%2Fi8ySOsUe8JGUzMFPS%2BYBDqrEDzLDM%2BBX1BcVtWKqZHDc7zQzc42QDlcYwP2Bivb7ohRhoDUFXvfgbUUQlZDZe3kxVvC45ZR8%2BcMcpuf6seUChm4l24K6ZGGS4m4b73OTAJo3lpvCIJL6BfkajvsqRxcxxYDtXJS7cAlHCuQTZh3IA0pMWbVXDaHeqKpcuO4dfWpbkFLHmin9jpwxTQZHX9ttbuPQ%2Fb791v%2FnlUyzT8aatWMYkh%2BzmWzjB4v94u3RmSbcxc3kvdVk4aHPE1YAYI3bNDBWd4nYEYQ9iR4OSDRC2%2B7Ll9qbEEP8cC9%2F0gQbMFRcHzLxg%3D%3D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle as cp\n",
    "from urllib.request import urlopen\n",
    "loaded_pickle_object = cp.load(urlopen(url, 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cloudpickle\n",
      "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: cloudpickle\n",
      "Successfully installed cloudpickle-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrebase\n",
    "\n",
    "config = {\n",
    "  \"apiKey\": firebase_admin.credentials.Certificate('../../../fyp2022-stockpriceprediction-firebase-adminsdk-ku62m-f9ed330292.json'),\n",
    "  \"authDomain\": \"fyp2022-stockpriceprediction.firebaseapp.com\",\n",
    "  'databaseURL':\"https://fyp2022-stockpriceprediction-default-rtdb.asia-southeast1.firebasedatabase.app/\",\n",
    "  'storageBucket': 'fyp2022-stockpriceprediction.appspot.com',\n",
    "  'serviceAccount': '../../../fyp2022-stockpriceprediction-firebase-adminsdk-ku62m-f9ed330292.json'\n",
    "}\n",
    "\n",
    "firebase = pyrebase.initialize_app(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/48181580/download-all-files-from-firebase-storage-using-python\n",
    "#https://github.com/thisbejim/Pyrebase#add-pyrebase-to-your-application\n",
    "storage = firebase.storage()\n",
    "for file in storage.child('models/AAPL/').list_files():\n",
    "    with open(storage.child(file.name)) as f:\n",
    "        word = f\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data into firebase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firebase_admin import db\n",
    "from datetime import datetime as dt\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    tick = file[1]\n",
    "    data = file[0]\n",
    "    data['Ticker'] = tick\n",
    "    \n",
    "    #convert date index to string, as firebsae cant have datetime as an index\n",
    "    data.index = data.index.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    #convert the dataframe to json/dictionary\n",
    "    data_dict = data.to_dict(orient=\"index\")\n",
    "    \n",
    "    #upload it to the database\n",
    "    ref = db.reference(\"/data/\"+tick)\n",
    "    ref.set(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from firebase database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from firebase_admin import db\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "ref = db.reference(\"/data\")\n",
    "data = ref.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_msft = {}\n",
    "data_msft['MSFT'] = data['MSFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_file_paths(directory):\n",
    "  \n",
    "    # initializing empty file paths list\n",
    "    file_paths = []\n",
    "  \n",
    "    # crawling through directory and subdirectories\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for fileName_model in files:\n",
    "            # join the two strings in order to form the full filepath.\n",
    "            filepath = os.path.join(root, fileName_model)\n",
    "            file_paths.append(filepath)\n",
    "  \n",
    "    # returning all file paths\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import math\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import os\n",
    "from firebase_admin import storage\n",
    "from sklearn.metrics import mean_squared_error\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "  # Specify an invalid GPU device\n",
    "  #with tf.device('/device:GPU:2'):\n",
    "  \n",
    "#   with tf.device('/device:GPU:2'):\n",
    "    for tick, stock_data in data_msft.items():\n",
    "      print(tick)\n",
    "      df = pd.DataFrame(stock_data).T\n",
    "      df = df.reset_index()['Close']\n",
    "      scaler = MinMaxScaler(feature_range=(0,1))\n",
    "      df=scaler.fit_transform(np.array(df).reshape(-1,1))\n",
    "      train_data = df[:int(0.9*len(df))]\n",
    "      test_data = df[int(0.9*len(df)):]\n",
    "      train_data = train_data.reshape(-1,1)\n",
    "      test_data = test_data.reshape(-1,1)\n",
    "      \n",
    "      def create_dataset(dataset, time_step=1):\n",
    "          dataX, dataY = [], []\n",
    "          for i in range(len(dataset)-time_step-1):\n",
    "              a = dataset[i:(i+time_step), 0]\n",
    "              dataX.append(a)\n",
    "              dataY.append(dataset[i + time_step, 0])\n",
    "          return np.array(dataX), np.array(dataY)\n",
    "      time_step = 60\n",
    "      X_train, y_train = create_dataset(train_data, time_step)\n",
    "      X_test, ytest = create_dataset(test_data, time_step)\n",
    "      \n",
    "      X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "      X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "      \n",
    "      model=Sequential()\n",
    "      model.add(LSTM(50,return_sequences=True,input_shape=(100,1)))\n",
    "      model.add(LSTM(50,return_sequences=True))\n",
    "      model.add(LSTM(50))\n",
    "      model.add(Dense(1))\n",
    "      model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "      history = model.fit(X_train, y_train,epochs=100, validation_split = 0.2)\n",
    "      # break\n",
    "      \n",
    "      # calling function to get all file paths in the directory\n",
    "      filepath_model = \"../../../data/models/\" + tick + \"/lstm\"\n",
    "      model.save(filepath_model)\n",
    "      file_paths = get_all_file_paths(filepath_model)\n",
    "\n",
    "      \n",
    "      #took help from this: https://www.geeksforgeeks.org/working-zip-files-python/\n",
    "      with ZipFile(filepath_model + \".zip\",'w') as zip:\n",
    "              # writing each file one by one\n",
    "              for file in file_paths:\n",
    "                  zip.write(file)\n",
    "      \n",
    "      \n",
    "      fileName_model = \"lstm\"\n",
    "      bucket = storage.bucket()\n",
    "    #   #upload models\n",
    "      blob = bucket.blob(\"models/\" + tick + \"/\" + fileName_model)\n",
    "      blob.upload_from_filename(fileName_model+\".zip\")\n",
    "      \n",
    "      #upload normalizer\n",
    "      filepath_normalizer = \"../../../data/normalizers/\" + tick + \"/lstm.pkl\"\n",
    "      pickle.dump(scaler, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "      filename_normalizer = \"lstm.pkl\"\n",
    "      blob = bucket.blob(\"normalizers/\" + tick + \"/\" + filename_normalizer)\n",
    "      blob.upload_from_filename(filepath_normalizer)\n",
    "      \n",
    "      # Opt : if you want to make public access from the URL\n",
    "      #blob.make_public()\n",
    "    # checking whether folder exists or not\n",
    "      folder_path = filepath_model\n",
    "      if os.path.exists(folder_path):\n",
    "\n",
    "          # checking whether the folder is empty or not\n",
    "          if len(os.listdir(folder_path)) == 0:\n",
    "              # removing the file using the os.remove() method\n",
    "              os.rmdir(folder_path)\n",
    "          else:\n",
    "              # messaging saying folder not empty\n",
    "              print(\"Folder is not empty\")\n",
    "      else:\n",
    "          # file not found message\n",
    "          print(\"File not found in the directory\")\n",
    "        \n",
    "except RuntimeError as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1721f8d1d60>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op OptionsDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_predict_function_291545 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_predict_function_291545 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_predict_function_291545 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_predict_function_291545 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ConcatV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.64311682215732"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict=model.predict(X_test)\n",
    "test_predict=scaler.inverse_transform(test_predict)\n",
    "\n",
    "ytest = scaler.inverse_transform(ytest.reshape(-1,1))\n",
    "error = math.sqrt(mean_squared_error(ytest,test_predict))\n",
    "error    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 100, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0307830607276087"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predict=model.predict(X_train)\n",
    "train_predict=scaler.inverse_transform(train_predict)\n",
    "y_train = scaler.inverse_transform(y_train.reshape(-1,1))\n",
    "math.sqrt(mean_squared_error(y_train,train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_predict)\n",
    "plt.plot(ytest)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                             Modified             Size\n",
      "AAPL_model/keras_metadata.pb                   2022-01-12 19:16:08        17068\n",
      "AAPL_model/saved_model.pb                      2022-01-12 19:16:08      2382689\n",
      "AAPL_model/variables/variables.data-00000-of-00001 2022-01-12 19:16:08       618372\n",
      "AAPL_model/variables/variables.index           2022-01-12 19:16:08         2731\n",
      "Extracting all the files now...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# specifying the zip file name\n",
    "file_name = \"AAPL_model.zip\"\n",
    "  \n",
    "# opening the zip file in READ mode\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "    # printing all the contents of the zip file\n",
    "    zip.printdir()\n",
    "  \n",
    "    # extracting all the files\n",
    "    print('Extracting all the files now...')\n",
    "    zip.extractall()\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Qr in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DiagPart in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sign in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Transpose in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ConcatV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Qr in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DiagPart in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sign in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Transpose in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ConcatV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Qr in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DiagPart in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sign in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Transpose in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ConcatV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op OptionsDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_predict_function_332981 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[174.37128]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the last 100 days and make a prediction\n",
    "tick = 'AAPL'\n",
    "def getTestData(ticker, start):\n",
    "    data = pdr.get_data_yahoo(ticker, start=start, end=today)\n",
    "    # dataname= ticker+\"_\"+str(today)\n",
    "    return data[-100:]\n",
    "    \n",
    "   \n",
    "   \n",
    "from datetime import datetime, timedelta\n",
    "start = d = today - timedelta(days=190)\n",
    "\n",
    "df = getTestData(tick,start) \n",
    "\n",
    "#df = pd.DataFrame(stock_data).T\n",
    "df = df.reset_index()['Close']\n",
    "df=scaler.transform(np.array(df).reshape(-1,1))\n",
    "test_data = df.reshape(-1,1)\n",
    "\n",
    "import keras.models\n",
    "model = keras.models.load_model(tick + '_model')\n",
    "prediction = model.predict( np.array( [test_data,] )  )\n",
    "scaler.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(1,1,1)(0,1,1)[12]             : AIC=inf, Time=1.47 sec\n",
      " ARIMA(0,1,0)(0,1,0)[12]             : AIC=8903.869, Time=0.05 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12]             : AIC=8357.856, Time=0.23 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12]             : AIC=inf, Time=0.92 sec\n",
      " ARIMA(1,1,0)(0,1,0)[12]             : AIC=8891.064, Time=0.08 sec\n",
      " ARIMA(1,1,0)(2,1,0)[12]             : AIC=8009.664, Time=0.47 sec\n",
      " ARIMA(1,1,0)(2,1,1)[12]             : AIC=inf, Time=2.58 sec\n",
      " ARIMA(1,1,0)(1,1,1)[12]             : AIC=inf, Time=1.20 sec\n",
      " ARIMA(0,1,0)(2,1,0)[12]             : AIC=8023.083, Time=0.39 sec\n",
      " ARIMA(2,1,0)(2,1,0)[12]             : AIC=8011.656, Time=0.56 sec\n",
      " ARIMA(1,1,1)(2,1,0)[12]             : AIC=8011.619, Time=1.13 sec\n",
      " ARIMA(0,1,1)(2,1,0)[12]             : AIC=8009.624, Time=0.50 sec\n",
      " ARIMA(0,1,1)(1,1,0)[12]             : AIC=8358.974, Time=0.24 sec\n",
      " ARIMA(0,1,1)(2,1,1)[12]             : AIC=inf, Time=2.62 sec\n",
      " ARIMA(0,1,1)(1,1,1)[12]             : AIC=inf, Time=1.39 sec\n",
      " ARIMA(0,1,2)(2,1,0)[12]             : AIC=8011.623, Time=0.53 sec\n",
      " ARIMA(1,1,2)(2,1,0)[12]             : AIC=8013.286, Time=1.57 sec\n",
      " ARIMA(0,1,1)(2,1,0)[12] intercept   : AIC=8011.601, Time=1.44 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,1)(2,1,0)[12]          \n",
      "Total fit time: 17.367 seconds\n"
     ]
    }
   ],
   "source": [
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "stock_data = data['AAPL']\n",
    "df = pd.DataFrame(stock_data).T\n",
    "\n",
    "data = df.sort_index(ascending=True, axis=0)\n",
    "\n",
    "train = data[:-2]\n",
    "valid = data[-2:]\n",
    "\n",
    "training = train['Close']\n",
    "validation = valid['Close']\n",
    "\n",
    "model = auto_arima(training, start_p=1, start_q=1,max_p=3, max_q=3, m=12,start_P=0, seasonal=True,d=1, D=1, trace=True,error_action='ignore',suppress_warnings=True)\n",
    "model.fit(training)\n",
    "\n",
    "forecast = model.predict(n_periods=len(valid))\n",
    "forecast = pd.DataFrame(forecast,index = valid.index,columns=['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(forecast['Prediction'],label='Prediction')\n",
    "plt.plot(valid['Close'], label='True value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SVOBBI~1\\AppData\\Local\\Temp/ipykernel_13036/2386101552.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#importing prophet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfbprophet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProphet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstock_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AAPL'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fbprophet'"
     ]
    }
   ],
   "source": [
    "#importing prophet\n",
    "from fbprophet import Prophet\n",
    "\n",
    "stock_data = data['AAPL']\n",
    "df = pd.DataFrame(stock_data).T\n",
    "\n",
    "#creating dataframe\n",
    "new_data = pd.DataFrame(index=range(0,len(df)),columns=['Date', 'Close'])\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    new_data['Date'][i] = data['Date'][i]\n",
    "    new_data['Close'][i] = data['Close'][i]\n",
    "\n",
    "new_data['Date'] = pd.to_datetime(new_data.Date,format='%Y-%m-%d')\n",
    "new_data.index = new_data['Date']\n",
    "\n",
    "#preparing data\n",
    "new_data.rename(columns={'Close': 'y', 'Date': 'ds'}, inplace=True)\n",
    "\n",
    "#train and validation\n",
    "train = new_data[:-10]\n",
    "valid = new_data[-10:]\n",
    "\n",
    "#fit the model\n",
    "model = Prophet()\n",
    "model.fit(train)\n",
    "\n",
    "#predictions\n",
    "close_prices = model.make_future_dataframe(periods=len(valid))\n",
    "forecast = model.predict(close_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom ANN (wih MA, H-L, O-C, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import os\n",
    "from firebase_admin import storage\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['AAPL', 'AMZN', 'AVGO', 'CSCO', 'FB', 'GOOG', 'GOOGL', 'MSFT', 'NVDA', 'TSLA'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0251 - val_loss: 6.2169e-04\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.6893e-04 - val_loss: 5.1575e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 2.8317e-04 - val_loss: 5.5462e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 2.3498e-04 - val_loss: 2.5689e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 2.0628e-04 - val_loss: 2.4210e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 2.1313e-04 - val_loss: 2.4511e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.9792e-04 - val_loss: 2.5153e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.8857e-04 - val_loss: 1.8776e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.6274e-04 - val_loss: 2.9878e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.7159e-04 - val_loss: 3.0600e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.7318e-04 - val_loss: 1.8479e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.4473e-04 - val_loss: 1.4452e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.5436e-04 - val_loss: 1.7844e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.7113e-04 - val_loss: 1.2822e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2482e-04 - val_loss: 2.1185e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.5218e-04 - val_loss: 1.4033e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6937e-04 - val_loss: 1.0847e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4820e-04 - val_loss: 1.3885e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2437e-04 - val_loss: 8.5589e-05\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3821e-04 - val_loss: 1.2464e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5595e-04 - val_loss: 1.1295e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3046e-04 - val_loss: 2.1224e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2413e-04 - val_loss: 7.1044e-05\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2730e-04 - val_loss: 1.0441e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2166e-04 - val_loss: 7.6708e-05\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6776e-04 - val_loss: 4.7085e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3276e-04 - val_loss: 1.4980e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2427e-04 - val_loss: 2.5587e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2761e-04 - val_loss: 1.6143e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4532e-04 - val_loss: 1.0660e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2452e-04 - val_loss: 9.5431e-05\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8363e-05 - val_loss: 7.2834e-05\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.3652e-04 - val_loss: 1.0398e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0119e-04 - val_loss: 1.1169e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2147e-04 - val_loss: 2.2692e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3885e-04 - val_loss: 8.8561e-05\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2675e-04 - val_loss: 1.1131e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1164e-04 - val_loss: 1.2084e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0422e-04 - val_loss: 7.1610e-05\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0386e-04 - val_loss: 1.3797e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1930e-04 - val_loss: 6.3445e-05\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1317e-04 - val_loss: 1.6170e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2910e-04 - val_loss: 1.6972e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2160e-04 - val_loss: 2.2060e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0697e-04 - val_loss: 7.2459e-05\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0949e-04 - val_loss: 8.4367e-05\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3328e-04 - val_loss: 9.0280e-05\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1212e-04 - val_loss: 2.0313e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0660e-04 - val_loss: 7.7732e-05\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8153e-05 - val_loss: 2.5344e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2280e-04 - val_loss: 1.1870e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0999e-04 - val_loss: 1.0901e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8050e-05 - val_loss: 1.4644e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0687e-04 - val_loss: 7.3014e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2256e-04 - val_loss: 8.0273e-05\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 9.2854e-05 - val_loss: 7.6858e-05\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1884e-04 - val_loss: 1.0973e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8715e-05 - val_loss: 5.4538e-05\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4565e-05 - val_loss: 5.9716e-05\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 8.2958e-05 - val_loss: 9.3364e-05\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2621e-04 - val_loss: 1.2758e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0261e-04 - val_loss: 6.4621e-05\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0806e-04 - val_loss: 8.6169e-05\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1021e-04 - val_loss: 1.3260e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1673e-04 - val_loss: 1.4411e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0149e-04 - val_loss: 1.5266e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2051e-04 - val_loss: 1.0661e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.4874e-05 - val_loss: 6.2257e-05\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.8527e-05 - val_loss: 6.4708e-05\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.2591e-05 - val_loss: 1.7838e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5516e-05 - val_loss: 6.1778e-05\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.9633e-05 - val_loss: 7.7326e-05\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.3652e-05 - val_loss: 7.1211e-05\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.9796e-05 - val_loss: 6.0358e-05\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.4945e-05 - val_loss: 8.1875e-05\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.3053e-04 - val_loss: 5.1560e-05\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2802e-05 - val_loss: 2.1672e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5942e-05 - val_loss: 6.7354e-05\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.9078e-05 - val_loss: 1.2008e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9792e-05 - val_loss: 1.1755e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0957e-04 - val_loss: 7.1221e-05\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3658e-05 - val_loss: 1.2347e-04\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.4316e-05 - val_loss: 9.0327e-05\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.5099e-05 - val_loss: 6.7066e-05\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0044e-05 - val_loss: 8.9639e-05\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 9.6963e-05 - val_loss: 1.0976e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0086e-04 - val_loss: 5.0299e-05\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.4871e-05 - val_loss: 4.8600e-05\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4560e-05 - val_loss: 8.7773e-05\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3316e-05 - val_loss: 1.2593e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0458e-04 - val_loss: 8.2475e-05\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6864e-05 - val_loss: 8.3455e-05\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1483e-05 - val_loss: 5.3653e-05\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5225e-05 - val_loss: 7.9249e-05\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2566e-04 - val_loss: 1.3419e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8997e-05 - val_loss: 6.2342e-05\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6148e-05 - val_loss: 1.9125e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6134e-05 - val_loss: 5.6642e-05\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3327e-05 - val_loss: 6.4227e-05\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0813e-05 - val_loss: 4.4258e-05\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0503e-05 - val_loss: 7.1024e-05\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.4871e-05 - val_loss: 8.6492e-05\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.0718e-05 - val_loss: 5.4723e-05\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.8303e-05 - val_loss: 7.9298e-05\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.8306e-05 - val_loss: 5.7159e-05\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0567e-04 - val_loss: 6.9901e-05\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.0842e-05 - val_loss: 7.7879e-05\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.6043e-05 - val_loss: 1.0988e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1280e-04 - val_loss: 6.5939e-05\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.4968e-05 - val_loss: 5.7706e-05\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.0186e-05 - val_loss: 9.5363e-05\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.6929e-05 - val_loss: 9.4585e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3534e-05 - val_loss: 4.8129e-05\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7759e-05 - val_loss: 4.8115e-05\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1214e-05 - val_loss: 4.1286e-05\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.7421e-05 - val_loss: 5.0365e-05\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8433e-05 - val_loss: 8.2381e-05\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8693e-05 - val_loss: 6.4602e-05\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.3351e-05 - val_loss: 2.1154e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.5232e-05 - val_loss: 7.1928e-05\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1488e-05 - val_loss: 6.1847e-05\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2093e-05 - val_loss: 5.7374e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0659e-04 - val_loss: 5.7209e-05\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4568e-05 - val_loss: 7.5540e-05\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0785e-05 - val_loss: 1.7332e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7870e-05 - val_loss: 1.4621e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0777e-04 - val_loss: 1.0721e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3366e-05 - val_loss: 1.0764e-04\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9757e-05 - val_loss: 7.2804e-05\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7357e-05 - val_loss: 6.9149e-05\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5816e-05 - val_loss: 1.4608e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2791e-05 - val_loss: 2.4999e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3310e-05 - val_loss: 9.2472e-05\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3016e-05 - val_loss: 5.1403e-05\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7419e-05 - val_loss: 1.0399e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0766e-04 - val_loss: 6.8898e-05\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6729e-05 - val_loss: 7.8793e-05\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5297e-05 - val_loss: 8.2653e-05\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4434e-05 - val_loss: 4.1301e-05\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4404e-05 - val_loss: 4.7515e-05\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5922e-05 - val_loss: 8.6397e-05\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6456e-05 - val_loss: 1.4743e-04\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5492e-05 - val_loss: 1.3045e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.9911e-05 - val_loss: 6.2636e-05\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1859e-05 - val_loss: 6.1281e-05\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7630e-05 - val_loss: 5.4757e-05\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7312e-05 - val_loss: 2.6570e-04\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7157e-05 - val_loss: 5.0606e-05\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.6946e-05 - val_loss: 4.3930e-05\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9464e-05 - val_loss: 7.4973e-05\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1056e-05 - val_loss: 7.6529e-05\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6086e-05 - val_loss: 1.4322e-04\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4264e-05 - val_loss: 4.9418e-05\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1626e-05 - val_loss: 9.5201e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0207e-05 - val_loss: 6.6910e-05\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0522e-05 - val_loss: 1.1988e-04\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1641e-05 - val_loss: 8.6454e-05\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9002e-05 - val_loss: 7.5037e-05\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6042e-05 - val_loss: 4.8455e-05\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1926e-05 - val_loss: 1.6888e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5936e-05 - val_loss: 7.7099e-05\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9476e-05 - val_loss: 6.7129e-05\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6317e-05 - val_loss: 6.7220e-05\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 6.7038e-05 - val_loss: 3.6347e-05\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0591e-04 - val_loss: 4.8036e-05\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.7970e-05 - val_loss: 4.4614e-05\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.4054e-05 - val_loss: 4.8230e-05\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.0592e-05 - val_loss: 4.7426e-05\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5059e-05 - val_loss: 6.7860e-05\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9473e-05 - val_loss: 6.1963e-05\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2871e-05 - val_loss: 6.0064e-05\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9231e-05 - val_loss: 9.1245e-05\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2044e-05 - val_loss: 8.9592e-05\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8909e-05 - val_loss: 9.3935e-05\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.1948e-05 - val_loss: 1.3760e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4682e-05 - val_loss: 4.4157e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5204e-05 - val_loss: 4.6648e-05\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2132e-05 - val_loss: 4.5217e-05\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.3205e-05 - val_loss: 5.7085e-05\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.6560e-05 - val_loss: 5.6463e-05\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/AAPL/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0544 - val_loss: 0.0020\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.2797e-04 - val_loss: 4.2703e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 3.7344e-04 - val_loss: 2.9372e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 2.9274e-04 - val_loss: 2.7322e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.6671e-04 - val_loss: 2.6701e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5689e-04 - val_loss: 2.3383e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.4717e-04 - val_loss: 2.3513e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 2.4611e-04 - val_loss: 2.2798e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.5762e-04 - val_loss: 3.4231e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5201e-04 - val_loss: 2.5524e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3774e-04 - val_loss: 2.0705e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3271e-04 - val_loss: 2.0153e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1188e-04 - val_loss: 2.3167e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1129e-04 - val_loss: 2.9724e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1005e-04 - val_loss: 1.6652e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1312e-04 - val_loss: 2.3870e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1116e-04 - val_loss: 1.9984e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5417e-04 - val_loss: 2.5504e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0471e-04 - val_loss: 1.5975e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0443e-04 - val_loss: 1.5743e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.2788e-04 - val_loss: 1.6048e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8892e-04 - val_loss: 2.4159e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0002e-04 - val_loss: 1.4877e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3306e-04 - val_loss: 2.0355e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3085e-04 - val_loss: 2.6035e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9495e-04 - val_loss: 2.0214e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8474e-04 - val_loss: 1.5274e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.2328e-04 - val_loss: 1.9992e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.8399e-04 - val_loss: 1.3952e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9448e-04 - val_loss: 2.7955e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.7812e-04 - val_loss: 2.0882e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7223e-04 - val_loss: 1.5278e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8268e-04 - val_loss: 2.1239e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8420e-04 - val_loss: 1.9063e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8331e-04 - val_loss: 1.7075e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5777e-04 - val_loss: 1.3519e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5844e-04 - val_loss: 1.2958e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3371e-04 - val_loss: 1.4395e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8904e-04 - val_loss: 1.3750e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6126e-04 - val_loss: 1.3809e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4711e-04 - val_loss: 1.4142e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6082e-04 - val_loss: 1.7118e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4438e-04 - val_loss: 1.4773e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6733e-04 - val_loss: 2.3495e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8593e-04 - val_loss: 1.6387e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7478e-04 - val_loss: 1.2862e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7588e-04 - val_loss: 1.4431e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6540e-04 - val_loss: 1.5269e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4202e-04 - val_loss: 1.4782e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5321e-04 - val_loss: 1.4329e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3695e-04 - val_loss: 2.4870e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3756e-04 - val_loss: 2.7741e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3432e-04 - val_loss: 1.8807e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4137e-04 - val_loss: 1.4108e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2659e-04 - val_loss: 1.5020e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2429e-04 - val_loss: 1.5217e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4483e-04 - val_loss: 1.7652e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4703e-04 - val_loss: 4.4069e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6127e-04 - val_loss: 1.1410e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5604e-04 - val_loss: 1.4891e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1832e-04 - val_loss: 1.3565e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1504e-04 - val_loss: 1.6941e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2568e-04 - val_loss: 1.3933e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2257e-04 - val_loss: 1.2362e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9558e-04 - val_loss: 1.0925e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1994e-04 - val_loss: 1.5314e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1984e-04 - val_loss: 1.1283e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1595e-04 - val_loss: 1.6190e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1191e-04 - val_loss: 1.3438e-04\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2307e-04 - val_loss: 1.8351e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1699e-04 - val_loss: 1.0881e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0697e-04 - val_loss: 1.2081e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2042e-04 - val_loss: 1.5366e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2151e-04 - val_loss: 1.1871e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0744e-04 - val_loss: 1.1167e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4409e-04 - val_loss: 2.9698e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1661e-04 - val_loss: 1.0478e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1598e-04 - val_loss: 2.5947e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4143e-04 - val_loss: 2.5005e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.4646e-04 - val_loss: 2.5044e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1277e-04 - val_loss: 1.1355e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0870e-04 - val_loss: 9.6543e-05\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3831e-04 - val_loss: 1.3776e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1180e-04 - val_loss: 1.4117e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4372e-04 - val_loss: 9.4977e-05\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0441e-04 - val_loss: 1.8327e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0170e-04 - val_loss: 2.3418e-04\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2103e-04 - val_loss: 1.5875e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3063e-04 - val_loss: 2.2067e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0424e-04 - val_loss: 1.0727e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1242e-04 - val_loss: 2.6024e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2465e-04 - val_loss: 1.4589e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3679e-04 - val_loss: 2.2275e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1096e-04 - val_loss: 1.2310e-04\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9804e-05 - val_loss: 9.7552e-05\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1239e-04 - val_loss: 1.3509e-04\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3079e-04 - val_loss: 1.4317e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0472e-04 - val_loss: 2.8624e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1124e-04 - val_loss: 9.9874e-05\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1273e-04 - val_loss: 3.2120e-04\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5450e-04 - val_loss: 1.6679e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2761e-04 - val_loss: 1.8549e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0367e-04 - val_loss: 1.1522e-04\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2299e-04 - val_loss: 1.3875e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8648e-05 - val_loss: 1.3622e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3094e-04 - val_loss: 3.6922e-04\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1078e-04 - val_loss: 1.5139e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0164e-04 - val_loss: 1.6435e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0841e-04 - val_loss: 1.6260e-04\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0247e-04 - val_loss: 9.8876e-05\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0971e-04 - val_loss: 9.3799e-05\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1384e-04 - val_loss: 1.0684e-04\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1078e-04 - val_loss: 1.2767e-04\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1511e-04 - val_loss: 1.0185e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0844e-04 - val_loss: 1.6064e-04\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2113e-04 - val_loss: 1.2761e-04\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0268e-04 - val_loss: 1.7602e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3140e-04 - val_loss: 1.1659e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1623e-04 - val_loss: 1.2712e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2821e-04 - val_loss: 1.3671e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6908e-04 - val_loss: 1.0373e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7497e-05 - val_loss: 1.2285e-04\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0813e-04 - val_loss: 2.6846e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9287e-05 - val_loss: 1.3657e-04\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0579e-04 - val_loss: 1.1066e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9870e-05 - val_loss: 1.0443e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5303e-04 - val_loss: 2.2015e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2172e-04 - val_loss: 9.4626e-05\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1077e-04 - val_loss: 1.5305e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1270e-04 - val_loss: 1.0561e-04\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0242e-04 - val_loss: 9.6603e-05\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1358e-04 - val_loss: 1.4221e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1236e-04 - val_loss: 1.0536e-04\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5353e-04 - val_loss: 1.1018e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1393e-04 - val_loss: 2.0233e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7743e-05 - val_loss: 1.1023e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0836e-04 - val_loss: 9.6287e-05\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8779e-05 - val_loss: 1.4298e-04\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2454e-04 - val_loss: 2.2474e-04\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0176e-04 - val_loss: 1.0547e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0259e-04 - val_loss: 1.1340e-04\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0742e-04 - val_loss: 9.5549e-05\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2930e-04 - val_loss: 1.7085e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3233e-04 - val_loss: 1.0012e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8934e-05 - val_loss: 1.0904e-04\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0732e-04 - val_loss: 1.8861e-04\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4392e-05 - val_loss: 1.0044e-04\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3793e-04 - val_loss: 9.1228e-05\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8784e-05 - val_loss: 9.0193e-05\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1139e-04 - val_loss: 1.8167e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9265e-05 - val_loss: 1.3100e-04\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9007e-05 - val_loss: 9.4984e-05\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1118e-04 - val_loss: 2.2205e-04\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0167e-04 - val_loss: 8.7601e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0264e-04 - val_loss: 1.0930e-04\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1853e-05 - val_loss: 1.0128e-04\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2665e-04 - val_loss: 1.1725e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1319e-04 - val_loss: 1.2221e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3493e-05 - val_loss: 9.4333e-05\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0444e-04 - val_loss: 1.0553e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2143e-05 - val_loss: 1.0155e-04\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0565e-04 - val_loss: 1.0279e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1261e-04 - val_loss: 9.8664e-05\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5796e-05 - val_loss: 1.5261e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2314e-04 - val_loss: 1.6912e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1947e-04 - val_loss: 9.2549e-05\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0096e-04 - val_loss: 1.3577e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6372e-05 - val_loss: 9.8115e-05\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2176e-04 - val_loss: 1.0117e-04\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0523e-04 - val_loss: 1.0687e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8095e-05 - val_loss: 2.5924e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0350e-04 - val_loss: 1.1756e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1785e-04 - val_loss: 1.0942e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0971e-05 - val_loss: 1.0535e-04\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3168e-05 - val_loss: 1.1859e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0582e-04 - val_loss: 1.4361e-04\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0277e-04 - val_loss: 1.1296e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6090e-05 - val_loss: 2.9840e-04\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0227e-04 - val_loss: 1.0515e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7222e-05 - val_loss: 1.1511e-04\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/AMZN/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0254 - val_loss: 7.8946e-04\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9757e-04 - val_loss: 5.1181e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 4.9650e-04 - val_loss: 2.5958e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 3.8895e-04 - val_loss: 2.2083e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.7840e-04 - val_loss: 5.3844e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.5054e-04 - val_loss: 3.2777e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 3.3533e-04 - val_loss: 2.2985e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 3.1141e-04 - val_loss: 2.4773e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.1170e-04 - val_loss: 4.1834e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.0788e-04 - val_loss: 3.0247e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.9175e-04 - val_loss: 1.7254e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.9750e-04 - val_loss: 1.6870e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.1364e-04 - val_loss: 2.3496e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 3.1561e-04 - val_loss: 2.1814e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.6515e-04 - val_loss: 1.9568e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 3.2955e-04 - val_loss: 1.8270e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.6881e-04 - val_loss: 2.1735e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.8189e-04 - val_loss: 1.7296e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.8768e-04 - val_loss: 3.1520e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.7611e-04 - val_loss: 1.4682e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2423e-04 - val_loss: 2.6704e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5212e-04 - val_loss: 1.9334e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.2902e-04 - val_loss: 1.6526e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3004e-04 - val_loss: 1.7544e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5309e-04 - val_loss: 1.5548e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3969e-04 - val_loss: 1.5831e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0584e-04 - val_loss: 1.5150e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5271e-04 - val_loss: 1.4349e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9623e-04 - val_loss: 4.2243e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3972e-04 - val_loss: 2.2890e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0562e-04 - val_loss: 1.2364e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1584e-04 - val_loss: 1.2118e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9233e-04 - val_loss: 1.6140e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0852e-04 - val_loss: 1.2967e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9669e-04 - val_loss: 1.4399e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1866e-04 - val_loss: 1.4887e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8372e-04 - val_loss: 2.0980e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6379e-04 - val_loss: 1.5674e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.4231e-04 - val_loss: 6.7193e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9428e-04 - val_loss: 1.7105e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7861e-04 - val_loss: 1.2137e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0437e-04 - val_loss: 1.7577e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6485e-04 - val_loss: 1.8195e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6743e-04 - val_loss: 1.9618e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6491e-04 - val_loss: 1.1152e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6554e-04 - val_loss: 1.1637e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5174e-04 - val_loss: 1.1982e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7461e-04 - val_loss: 2.2561e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4346e-04 - val_loss: 1.1846e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4094e-04 - val_loss: 1.2775e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5394e-04 - val_loss: 3.2803e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4390e-04 - val_loss: 1.1380e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2245e-04 - val_loss: 1.3393e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1039e-04 - val_loss: 5.1381e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8045e-04 - val_loss: 1.6073e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4478e-04 - val_loss: 1.1763e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1709e-04 - val_loss: 1.7876e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3328e-04 - val_loss: 1.5401e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2214e-04 - val_loss: 1.2505e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2739e-04 - val_loss: 1.7903e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3591e-04 - val_loss: 1.1025e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3317e-04 - val_loss: 1.5098e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2540e-04 - val_loss: 1.2211e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4074e-04 - val_loss: 1.2768e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2710e-04 - val_loss: 1.1619e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3009e-04 - val_loss: 1.2274e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3595e-04 - val_loss: 1.8483e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1539e-04 - val_loss: 1.5380e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2831e-04 - val_loss: 1.2983e-04\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1592e-04 - val_loss: 1.2131e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4068e-04 - val_loss: 1.1197e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1129e-04 - val_loss: 9.9433e-05\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1448e-04 - val_loss: 9.3042e-05\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3626e-04 - val_loss: 9.8370e-05\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1951e-04 - val_loss: 9.6222e-05\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2439e-04 - val_loss: 1.1465e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2086e-04 - val_loss: 1.1524e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1542e-04 - val_loss: 2.9512e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2798e-04 - val_loss: 1.2850e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1781e-04 - val_loss: 9.2537e-05\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1854e-04 - val_loss: 1.1127e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4233e-04 - val_loss: 1.0958e-04\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0030e-04 - val_loss: 2.2365e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.3508e-04 - val_loss: 1.3970e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1435e-04 - val_loss: 1.3972e-04\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2054e-04 - val_loss: 1.0962e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.1507e-04 - val_loss: 1.1001e-04\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0797e-04 - val_loss: 1.1805e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5724e-04 - val_loss: 1.5791e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2912e-04 - val_loss: 1.5268e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6648e-04 - val_loss: 8.8791e-05\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1464e-04 - val_loss: 1.5159e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1805e-04 - val_loss: 1.0370e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0049e-04 - val_loss: 9.9212e-05\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3649e-04 - val_loss: 1.4542e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4067e-04 - val_loss: 1.3768e-04\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9206e-05 - val_loss: 9.5733e-05\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0402e-04 - val_loss: 1.0260e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2321e-04 - val_loss: 1.6497e-04\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2282e-04 - val_loss: 8.7661e-05\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0898e-04 - val_loss: 1.6988e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2562e-04 - val_loss: 1.0378e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0712e-04 - val_loss: 9.6103e-05\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1310e-04 - val_loss: 1.0606e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0502e-04 - val_loss: 9.5744e-05\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1181e-04 - val_loss: 8.6975e-05\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2521e-04 - val_loss: 1.6630e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2961e-04 - val_loss: 8.7896e-05\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4938e-04 - val_loss: 1.3946e-04\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1762e-04 - val_loss: 1.8414e-04\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3161e-04 - val_loss: 1.1151e-04\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1304e-04 - val_loss: 9.6155e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6736e-05 - val_loss: 1.7863e-04\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1636e-04 - val_loss: 8.8282e-05\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1273e-04 - val_loss: 8.9042e-05\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.2476e-04 - val_loss: 2.5681e-04\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.4103e-04 - val_loss: 1.4463e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1176e-04 - val_loss: 1.2863e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0837e-04 - val_loss: 1.1084e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1049e-04 - val_loss: 1.0374e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1321e-04 - val_loss: 1.8240e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0749e-04 - val_loss: 9.9296e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8126e-05 - val_loss: 9.9917e-05\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2672e-04 - val_loss: 9.6340e-05\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0960e-04 - val_loss: 2.0869e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0578e-04 - val_loss: 8.5367e-05\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0167e-04 - val_loss: 1.6633e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3053e-04 - val_loss: 1.7683e-04\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4366e-04 - val_loss: 1.0714e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0332e-04 - val_loss: 8.5889e-05\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.2322e-04 - val_loss: 2.8223e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0468e-04 - val_loss: 1.0572e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1222e-04 - val_loss: 8.4410e-05\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0214e-04 - val_loss: 1.1794e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4735e-05 - val_loss: 9.7393e-05\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2038e-04 - val_loss: 1.0336e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0727e-04 - val_loss: 1.0878e-04\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0321e-04 - val_loss: 9.0268e-05\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.6326e-05 - val_loss: 9.5057e-05\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2306e-04 - val_loss: 1.7311e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1063e-04 - val_loss: 1.4302e-04\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1230e-04 - val_loss: 9.2521e-05\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1304e-04 - val_loss: 1.0488e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2981e-04 - val_loss: 1.1914e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0975e-04 - val_loss: 1.7077e-04\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0890e-04 - val_loss: 9.4916e-05\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1749e-05 - val_loss: 9.9529e-05\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1553e-04 - val_loss: 1.0416e-04\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0873e-04 - val_loss: 1.7071e-04\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5188e-05 - val_loss: 1.6959e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0264e-04 - val_loss: 1.8521e-04\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0223e-04 - val_loss: 1.0742e-04\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3047e-04 - val_loss: 1.9801e-04\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1911e-04 - val_loss: 9.6343e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.0092e-04 - val_loss: 9.7452e-05\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.1767e-05 - val_loss: 1.1285e-04\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9825e-05 - val_loss: 1.6093e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0532e-04 - val_loss: 1.0380e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1933e-04 - val_loss: 9.4065e-05\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0846e-04 - val_loss: 1.3948e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1342e-04 - val_loss: 8.8482e-05\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1185e-04 - val_loss: 1.0558e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1723e-04 - val_loss: 1.7435e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4835e-05 - val_loss: 9.1228e-05\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.9872e-05 - val_loss: 8.9273e-05\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0272e-04 - val_loss: 1.1012e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.3332e-04 - val_loss: 1.5029e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.2069e-04 - val_loss: 4.5077e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1580e-04 - val_loss: 2.1490e-04\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.2492e-04 - val_loss: 1.3457e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.5985e-05 - val_loss: 1.8591e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.2027e-04 - val_loss: 1.3830e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.7461e-05 - val_loss: 2.3200e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1544e-04 - val_loss: 9.5757e-05\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0361e-04 - val_loss: 1.0801e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7654e-05 - val_loss: 9.0459e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0616e-04 - val_loss: 1.0387e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7869e-05 - val_loss: 8.8284e-05\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1902e-05 - val_loss: 1.0280e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1290e-04 - val_loss: 9.1092e-05\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/AVGO/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0497 - val_loss: 0.0017\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 8.4610e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0010 - val_loss: 7.5672e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7538e-04 - val_loss: 9.3914e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 6.9589e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1185e-04 - val_loss: 6.8196e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5960e-04 - val_loss: 0.0012\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8098e-04 - val_loss: 6.4522e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.1585e-04 - val_loss: 6.4346e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.0373e-04 - val_loss: 6.4480e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.8679e-04 - val_loss: 7.8261e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.8200e-04 - val_loss: 6.1898e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.3705e-04 - val_loss: 7.2149e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.4525e-04 - val_loss: 6.3886e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.2767e-04 - val_loss: 6.2351e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.9330e-04 - val_loss: 0.0010\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.9663e-04 - val_loss: 6.0070e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.9712e-04 - val_loss: 6.2640e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9944e-04 - val_loss: 5.9919e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5086e-04 - val_loss: 5.7274e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9202e-04 - val_loss: 5.9193e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6599e-04 - val_loss: 7.4484e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6713e-04 - val_loss: 6.0364e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7201e-04 - val_loss: 5.7415e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4088e-04 - val_loss: 6.5696e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9290e-04 - val_loss: 7.2216e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 7.8384e-04 - val_loss: 0.0013\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1511e-04 - val_loss: 5.8215e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.7547e-04 - val_loss: 5.6449e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7995e-04 - val_loss: 5.4097e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.3215e-04 - val_loss: 5.4263e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.2152e-04 - val_loss: 6.2747e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.2159e-04 - val_loss: 5.4545e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7275e-04 - val_loss: 7.6526e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5193e-04 - val_loss: 6.0696e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3425e-04 - val_loss: 6.8295e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4232e-04 - val_loss: 7.9317e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8005e-04 - val_loss: 5.2676e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9480e-04 - val_loss: 0.0010\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0029e-04 - val_loss: 5.4448e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1280e-04 - val_loss: 6.0907e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0379e-04 - val_loss: 5.5810e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1312e-04 - val_loss: 5.0832e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0840e-04 - val_loss: 7.0679e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5209e-04 - val_loss: 4.9503e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 7.4424e-04 - val_loss: 5.9683e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9309e-04 - val_loss: 8.0515e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6385e-04 - val_loss: 8.3236e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0296e-04 - val_loss: 5.2913e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1012e-04 - val_loss: 6.8750e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9566e-04 - val_loss: 5.1698e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8489e-04 - val_loss: 5.3766e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3582e-04 - val_loss: 5.0242e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4489e-04 - val_loss: 5.2838e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3681e-04 - val_loss: 5.3977e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2577e-04 - val_loss: 4.9420e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1268e-04 - val_loss: 7.6628e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.4795e-04 - val_loss: 4.8035e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.8223e-04 - val_loss: 5.5388e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.6377e-04 - val_loss: 6.8530e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.4154e-04 - val_loss: 4.7157e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.6946e-04 - val_loss: 4.9696e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.4406e-04 - val_loss: 5.2961e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.0082e-04 - val_loss: 4.6550e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.2109e-04 - val_loss: 4.7476e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.7383e-04 - val_loss: 4.7885e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.4089e-04 - val_loss: 4.6207e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.3673e-04 - val_loss: 5.4771e-04\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4317e-04 - val_loss: 4.7472e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6259e-04 - val_loss: 7.4089e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6019e-04 - val_loss: 5.5427e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2799e-04 - val_loss: 5.1404e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3551e-04 - val_loss: 4.9775e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9145e-04 - val_loss: 5.1131e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7459e-04 - val_loss: 5.1706e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2984e-04 - val_loss: 6.9087e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 7.3358e-04 - val_loss: 4.6783e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0911e-04 - val_loss: 5.3423e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.0150e-04 - val_loss: 4.6309e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.2734e-04 - val_loss: 4.5514e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2528e-04 - val_loss: 7.0607e-04\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9938e-04 - val_loss: 4.8778e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0727e-04 - val_loss: 6.1706e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8008e-04 - val_loss: 4.6781e-04\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.4553e-04 - val_loss: 6.6908e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4920e-04 - val_loss: 4.7201e-04\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6150e-04 - val_loss: 4.6281e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0265e-04 - val_loss: 5.9643e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1775e-04 - val_loss: 4.5661e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2259e-04 - val_loss: 6.7977e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5192e-04 - val_loss: 4.6332e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3067e-04 - val_loss: 5.6900e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4095e-04 - val_loss: 4.5246e-04\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4963e-04 - val_loss: 4.8480e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6277e-04 - val_loss: 4.6534e-04\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4587e-04 - val_loss: 4.5208e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4695e-04 - val_loss: 4.6195e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8610e-04 - val_loss: 6.3700e-04\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.4716e-04 - val_loss: 5.8551e-04\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8614e-04 - val_loss: 4.4567e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.2831e-04 - val_loss: 4.8412e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.4260e-04 - val_loss: 4.4558e-04\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.1255e-04 - val_loss: 4.8725e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.1577e-04 - val_loss: 4.5905e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.1737e-04 - val_loss: 4.9049e-04\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.4506e-04 - val_loss: 5.0953e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.1825e-04 - val_loss: 4.3300e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.5392e-04 - val_loss: 4.6275e-04\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.2883e-04 - val_loss: 5.0756e-04\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4953e-04 - val_loss: 4.5236e-04\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0753e-04 - val_loss: 4.5029e-04\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2260e-04 - val_loss: 5.2291e-04\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8117e-04 - val_loss: 5.3345e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6514e-04 - val_loss: 4.9899e-04\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2579e-04 - val_loss: 0.0012\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3281e-04 - val_loss: 4.9201e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7218e-04 - val_loss: 5.3002e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3348e-04 - val_loss: 8.2933e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4190e-04 - val_loss: 5.5832e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3782e-04 - val_loss: 5.3223e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4443e-04 - val_loss: 4.5037e-04\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.3475e-04 - val_loss: 4.5053e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1382e-04 - val_loss: 9.8928e-04\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6018e-04 - val_loss: 4.6229e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4986e-04 - val_loss: 4.4442e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3103e-04 - val_loss: 5.2027e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2768e-04 - val_loss: 4.3643e-04\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2984e-04 - val_loss: 4.7712e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0345e-04 - val_loss: 4.4953e-04\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0066e-04 - val_loss: 4.6767e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2883e-04 - val_loss: 4.6338e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5348e-04 - val_loss: 4.7375e-04\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3882e-04 - val_loss: 5.6943e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0156e-04 - val_loss: 4.3749e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1384e-04 - val_loss: 7.5005e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4386e-04 - val_loss: 4.6215e-04\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0322e-04 - val_loss: 8.4358e-04\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5564e-04 - val_loss: 6.3180e-04\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3041e-04 - val_loss: 5.2516e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2537e-04 - val_loss: 4.6315e-04\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7684e-04 - val_loss: 5.3397e-04\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8505e-04 - val_loss: 4.4931e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0131e-04 - val_loss: 4.6591e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4482e-04 - val_loss: 4.9326e-04\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1497e-04 - val_loss: 5.9000e-04\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.1840e-04 - val_loss: 4.3934e-04\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2154e-04 - val_loss: 4.5225e-04\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9790e-04 - val_loss: 4.4702e-04\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0873e-04 - val_loss: 4.8811e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2220e-04 - val_loss: 5.5747e-04\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6159e-04 - val_loss: 5.0035e-04\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5174e-04 - val_loss: 0.0011\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.3803e-04 - val_loss: 4.6183e-04\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1579e-04 - val_loss: 4.4589e-04\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0808e-04 - val_loss: 4.5194e-04\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0948e-04 - val_loss: 4.5503e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2794e-04 - val_loss: 5.6274e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1893e-04 - val_loss: 6.5964e-04\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3735e-04 - val_loss: 4.4461e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0353e-04 - val_loss: 4.7912e-04\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9257e-04 - val_loss: 4.8364e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0367e-04 - val_loss: 4.6537e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0177e-04 - val_loss: 5.3576e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4809e-04 - val_loss: 4.4096e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.3082e-04 - val_loss: 4.4061e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4397e-04 - val_loss: 4.6458e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8795e-04 - val_loss: 4.4065e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1738e-04 - val_loss: 4.4489e-04\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9214e-04 - val_loss: 4.3021e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2972e-04 - val_loss: 5.0517e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0998e-04 - val_loss: 4.3708e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4052e-04 - val_loss: 5.8924e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7691e-04 - val_loss: 4.8218e-04\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6154e-04 - val_loss: 4.5407e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0670e-04 - val_loss: 4.5038e-04\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9834e-04 - val_loss: 4.5372e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3038e-04 - val_loss: 5.4934e-04\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9887e-04 - val_loss: 4.2065e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.1039e-04 - val_loss: 4.5041e-04\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/CSCO/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0436 - val_loss: 0.0018\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0011 - val_loss: 5.9133e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 4.3914e-04 - val_loss: 3.0381e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.0250e-04 - val_loss: 2.0190e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.7095e-04 - val_loss: 1.7121e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.4709e-04 - val_loss: 1.9852e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5393e-04 - val_loss: 1.7266e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5588e-04 - val_loss: 2.3287e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.7057e-04 - val_loss: 1.7677e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3490e-04 - val_loss: 3.3001e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.4725e-04 - val_loss: 1.9052e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.4418e-04 - val_loss: 1.6146e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9990e-04 - val_loss: 1.6531e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2920e-04 - val_loss: 1.6652e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1699e-04 - val_loss: 3.4211e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 2.6069e-04 - val_loss: 1.8886e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 2.1326e-04 - val_loss: 1.5066e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.7988e-04 - val_loss: 1.6997e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.9828e-04 - val_loss: 1.6105e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.9555e-04 - val_loss: 1.5623e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.8267e-04 - val_loss: 1.6666e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.6001e-04 - val_loss: 1.5002e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8428e-04 - val_loss: 1.4892e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7540e-04 - val_loss: 1.7368e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0536e-04 - val_loss: 2.8764e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0870e-04 - val_loss: 1.9782e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7956e-04 - val_loss: 2.4067e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1660e-04 - val_loss: 2.9646e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8712e-04 - val_loss: 1.7995e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6466e-04 - val_loss: 1.2839e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5938e-04 - val_loss: 1.4699e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5410e-04 - val_loss: 1.1188e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5576e-04 - val_loss: 2.1501e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6445e-04 - val_loss: 1.3428e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6236e-04 - val_loss: 2.0367e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3082e-04 - val_loss: 1.7701e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7000e-04 - val_loss: 1.4498e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5992e-04 - val_loss: 1.9230e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.4510e-04 - val_loss: 1.9675e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4856e-04 - val_loss: 1.0887e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5036e-04 - val_loss: 1.4743e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5683e-04 - val_loss: 1.9266e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6949e-04 - val_loss: 1.0562e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4551e-04 - val_loss: 1.0978e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4051e-04 - val_loss: 1.2345e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5937e-04 - val_loss: 1.1348e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2639e-04 - val_loss: 1.7339e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4440e-04 - val_loss: 1.1983e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5603e-04 - val_loss: 1.1596e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3598e-04 - val_loss: 1.2105e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5099e-04 - val_loss: 1.4025e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3856e-04 - val_loss: 1.0453e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3886e-04 - val_loss: 1.1965e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3681e-04 - val_loss: 1.2907e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3043e-04 - val_loss: 2.3366e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3103e-04 - val_loss: 1.7745e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4731e-04 - val_loss: 1.0617e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2570e-04 - val_loss: 1.0665e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4694e-04 - val_loss: 3.5880e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6018e-04 - val_loss: 1.4848e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4161e-04 - val_loss: 1.3388e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5082e-04 - val_loss: 1.5963e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2800e-04 - val_loss: 1.1405e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4004e-04 - val_loss: 1.4235e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5196e-04 - val_loss: 1.2157e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2403e-04 - val_loss: 1.5829e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2987e-04 - val_loss: 1.3120e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2804e-04 - val_loss: 1.5571e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.3067e-04 - val_loss: 1.5501e-04\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3243e-04 - val_loss: 1.4875e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3386e-04 - val_loss: 1.1109e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3532e-04 - val_loss: 2.2353e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2025e-04 - val_loss: 1.5505e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1413e-04 - val_loss: 1.5178e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4363e-04 - val_loss: 1.9726e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2323e-04 - val_loss: 1.2305e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3468e-04 - val_loss: 1.7936e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2129e-04 - val_loss: 1.5485e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4086e-04 - val_loss: 1.9426e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3660e-04 - val_loss: 1.5877e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1871e-04 - val_loss: 1.3797e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2557e-04 - val_loss: 1.4529e-04\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4439e-04 - val_loss: 1.3958e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3840e-04 - val_loss: 1.1193e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1962e-04 - val_loss: 1.0694e-04\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2603e-04 - val_loss: 1.2484e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3364e-04 - val_loss: 1.1163e-04\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2654e-04 - val_loss: 1.0128e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3013e-04 - val_loss: 1.1926e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3480e-04 - val_loss: 1.2650e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4141e-04 - val_loss: 1.1369e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2961e-04 - val_loss: 1.1923e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1816e-04 - val_loss: 1.0899e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3329e-04 - val_loss: 3.2095e-04\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5478e-04 - val_loss: 2.7133e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0740e-04 - val_loss: 1.3421e-04\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2578e-04 - val_loss: 1.1578e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4462e-04 - val_loss: 1.2914e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3691e-04 - val_loss: 1.3166e-04\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3207e-04 - val_loss: 1.3467e-04\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5540e-04 - val_loss: 1.4753e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1376e-04 - val_loss: 1.1629e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1396e-04 - val_loss: 1.2576e-04\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1341e-04 - val_loss: 1.4083e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0981e-04 - val_loss: 1.0879e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1801e-04 - val_loss: 1.0580e-04\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0721e-04 - val_loss: 1.1520e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2014e-04 - val_loss: 2.4398e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1959e-04 - val_loss: 1.1150e-04\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2972e-04 - val_loss: 1.6160e-04\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2599e-04 - val_loss: 1.1231e-04\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3145e-04 - val_loss: 1.3913e-04\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4566e-04 - val_loss: 1.1226e-04\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2102e-04 - val_loss: 2.6216e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2486e-04 - val_loss: 1.2687e-04\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2763e-04 - val_loss: 2.7475e-04\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3136e-04 - val_loss: 1.3855e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3653e-04 - val_loss: 2.3140e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3224e-04 - val_loss: 1.1223e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3281e-04 - val_loss: 1.0418e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1149e-04 - val_loss: 1.8903e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1292e-04 - val_loss: 1.0155e-04\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1681e-04 - val_loss: 1.2187e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2313e-04 - val_loss: 1.5235e-04\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3008e-04 - val_loss: 9.8559e-05\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2633e-04 - val_loss: 1.1840e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0341e-04 - val_loss: 1.2283e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2616e-04 - val_loss: 1.6615e-04\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3590e-04 - val_loss: 2.0929e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1080e-04 - val_loss: 1.0646e-04\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3556e-04 - val_loss: 1.4043e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1684e-04 - val_loss: 1.1908e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2834e-04 - val_loss: 1.0068e-04\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2413e-04 - val_loss: 1.4299e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3683e-04 - val_loss: 1.2689e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2400e-04 - val_loss: 1.2460e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2182e-04 - val_loss: 1.0103e-04\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1268e-04 - val_loss: 1.7056e-04\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3403e-04 - val_loss: 2.1683e-04\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4095e-04 - val_loss: 1.2243e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0711e-04 - val_loss: 1.9598e-04\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3040e-04 - val_loss: 1.0084e-04\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1295e-04 - val_loss: 1.6930e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1964e-04 - val_loss: 1.1548e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2340e-04 - val_loss: 1.1531e-04\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3153e-04 - val_loss: 1.5238e-04\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2859e-04 - val_loss: 1.3832e-04\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1682e-04 - val_loss: 1.8319e-04\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5158e-04 - val_loss: 1.2143e-04\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1971e-04 - val_loss: 1.0674e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2225e-04 - val_loss: 1.0840e-04\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3765e-04 - val_loss: 1.4291e-04\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3272e-04 - val_loss: 1.0305e-04\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0293e-04 - val_loss: 1.1731e-04\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1426e-04 - val_loss: 1.0971e-04\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0321e-04 - val_loss: 1.6996e-04\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1361e-04 - val_loss: 1.1330e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1489e-04 - val_loss: 1.6729e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2424e-04 - val_loss: 1.2137e-04\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1597e-04 - val_loss: 1.0216e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0818e-04 - val_loss: 1.7336e-04\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1084e-04 - val_loss: 1.1757e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1615e-04 - val_loss: 1.5208e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1940e-04 - val_loss: 3.0453e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1468e-04 - val_loss: 1.3068e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3182e-04 - val_loss: 2.0524e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6740e-04 - val_loss: 2.0683e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2738e-04 - val_loss: 1.7514e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0471e-04 - val_loss: 1.0805e-04\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3134e-04 - val_loss: 2.0818e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2933e-04 - val_loss: 1.1649e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2779e-04 - val_loss: 1.3452e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1361e-04 - val_loss: 1.1988e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1389e-04 - val_loss: 1.0323e-04\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1379e-04 - val_loss: 1.2650e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4286e-04 - val_loss: 1.1306e-04\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0301e-04 - val_loss: 1.2436e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1571e-04 - val_loss: 1.2700e-04\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1867e-04 - val_loss: 1.4656e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0663e-04 - val_loss: 1.2247e-04\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/FB/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0335 - val_loss: 0.0019\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.5621e-04 - val_loss: 3.0617e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4035e-04 - val_loss: 1.3388e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6776e-04 - val_loss: 1.1692e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5154e-04 - val_loss: 1.0221e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4498e-04 - val_loss: 1.0808e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4031e-04 - val_loss: 1.6845e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4087e-04 - val_loss: 1.6279e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3073e-04 - val_loss: 9.5788e-05\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3080e-04 - val_loss: 1.7504e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3278e-04 - val_loss: 1.1519e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2406e-04 - val_loss: 7.8455e-05\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2379e-04 - val_loss: 7.9697e-05\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2524e-04 - val_loss: 7.8300e-05\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1860e-04 - val_loss: 8.5488e-05\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3256e-04 - val_loss: 1.4559e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1468e-04 - val_loss: 9.0716e-05\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2766e-04 - val_loss: 6.3751e-05\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0802e-04 - val_loss: 7.2616e-05\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0936e-04 - val_loss: 1.0162e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0122e-04 - val_loss: 1.3285e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1304e-04 - val_loss: 1.1344e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4289e-05 - val_loss: 8.6476e-05\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1823e-04 - val_loss: 6.7614e-05\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2021e-04 - val_loss: 6.8958e-05\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1888e-04 - val_loss: 1.5629e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1191e-04 - val_loss: 4.1739e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2818e-04 - val_loss: 1.0651e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1279e-04 - val_loss: 6.7392e-05\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.9409e-05 - val_loss: 6.0726e-05\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1951e-05 - val_loss: 9.3427e-05\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1552e-05 - val_loss: 2.0668e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3574e-04 - val_loss: 2.3156e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0104e-04 - val_loss: 7.6397e-05\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1693e-04 - val_loss: 8.2450e-05\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0629e-04 - val_loss: 5.5378e-05\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4019e-05 - val_loss: 5.7651e-05\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8888e-05 - val_loss: 1.3610e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6274e-05 - val_loss: 1.1822e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5303e-05 - val_loss: 4.8809e-05\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0050e-04 - val_loss: 4.7531e-05\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3228e-05 - val_loss: 1.2451e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0084e-04 - val_loss: 7.2971e-05\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0130e-05 - val_loss: 5.7496e-05\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6935e-05 - val_loss: 4.9290e-05\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.9375e-05 - val_loss: 3.3718e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0751e-04 - val_loss: 6.8652e-05\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0105e-04 - val_loss: 5.7445e-05\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9495e-05 - val_loss: 4.6002e-05\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6889e-05 - val_loss: 5.8617e-05\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0005e-04 - val_loss: 6.9977e-05\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5383e-05 - val_loss: 5.4694e-05\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6328e-05 - val_loss: 1.1128e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3176e-05 - val_loss: 5.1938e-05\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9048e-05 - val_loss: 4.7945e-05\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6165e-05 - val_loss: 9.1257e-05\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6645e-05 - val_loss: 6.7340e-05\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6095e-05 - val_loss: 1.3345e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5352e-05 - val_loss: 4.0888e-05\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9712e-05 - val_loss: 8.3062e-05\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7808e-05 - val_loss: 4.3363e-05\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6514e-05 - val_loss: 5.1832e-05\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4869e-05 - val_loss: 1.2314e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1009e-05 - val_loss: 3.6354e-05\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8512e-05 - val_loss: 6.4009e-05\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7159e-05 - val_loss: 4.3710e-05\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5219e-05 - val_loss: 5.5300e-05\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1778e-05 - val_loss: 3.8567e-05\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8777e-05 - val_loss: 9.8518e-05\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8750e-05 - val_loss: 1.1746e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8202e-05 - val_loss: 7.8774e-05\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3186e-05 - val_loss: 1.2654e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2469e-05 - val_loss: 1.3856e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3473e-05 - val_loss: 9.4817e-05\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0880e-05 - val_loss: 6.1199e-05\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9304e-05 - val_loss: 4.1227e-05\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3110e-05 - val_loss: 5.5179e-05\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9020e-05 - val_loss: 3.1679e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5773e-05 - val_loss: 1.4663e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0964e-04 - val_loss: 5.4564e-05\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.2503e-05 - val_loss: 3.6221e-05\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2909e-05 - val_loss: 4.6915e-05\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7477e-05 - val_loss: 8.0322e-05\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9840e-05 - val_loss: 4.7643e-05\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7034e-05 - val_loss: 7.4953e-05\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2170e-05 - val_loss: 4.3112e-05\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4348e-05 - val_loss: 5.1362e-05\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6313e-05 - val_loss: 1.3100e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2205e-05 - val_loss: 5.8401e-05\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2349e-05 - val_loss: 3.8197e-05\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8330e-05 - val_loss: 1.1286e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0148e-04 - val_loss: 7.3774e-05\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6306e-05 - val_loss: 3.5096e-05\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7471e-05 - val_loss: 4.3816e-05\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1482e-05 - val_loss: 8.2661e-05\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0305e-05 - val_loss: 7.1598e-05\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9291e-05 - val_loss: 3.5859e-05\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1583e-05 - val_loss: 5.6836e-05\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2014e-05 - val_loss: 5.5422e-05\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7499e-05 - val_loss: 3.4700e-05\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9274e-05 - val_loss: 3.4558e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0618e-05 - val_loss: 5.8446e-05\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1485e-05 - val_loss: 6.8283e-05\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8773e-05 - val_loss: 8.1613e-05\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4567e-05 - val_loss: 3.6242e-05\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8629e-05 - val_loss: 5.8684e-05\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8925e-05 - val_loss: 4.8514e-05\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0953e-05 - val_loss: 4.4469e-05\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1869e-05 - val_loss: 6.5864e-05\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8547e-05 - val_loss: 3.3757e-05\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8603e-05 - val_loss: 3.5513e-05\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5868e-05 - val_loss: 9.9674e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5250e-05 - val_loss: 5.4011e-05\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4557e-05 - val_loss: 5.1719e-05\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6240e-05 - val_loss: 1.0598e-04\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5281e-05 - val_loss: 7.9074e-05\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5195e-05 - val_loss: 3.9443e-05\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2995e-05 - val_loss: 1.0890e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6088e-05 - val_loss: 4.4128e-05\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0768e-05 - val_loss: 3.6928e-05\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3971e-05 - val_loss: 1.3798e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2139e-05 - val_loss: 4.5590e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9112e-05 - val_loss: 1.0627e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9372e-05 - val_loss: 1.5642e-04\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9290e-05 - val_loss: 2.6430e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6174e-05 - val_loss: 5.3137e-05\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1272e-05 - val_loss: 4.5333e-05\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1153e-05 - val_loss: 4.4735e-05\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5042e-05 - val_loss: 1.0165e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8291e-05 - val_loss: 1.5443e-04\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7182e-05 - val_loss: 1.5137e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1644e-05 - val_loss: 6.7641e-05\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2422e-05 - val_loss: 8.2396e-05\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3739e-05 - val_loss: 4.0801e-05\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1642e-05 - val_loss: 4.0479e-05\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6665e-05 - val_loss: 3.9411e-05\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8315e-05 - val_loss: 5.7447e-05\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4815e-05 - val_loss: 4.9942e-05\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4720e-05 - val_loss: 4.4976e-05\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2831e-05 - val_loss: 9.7746e-05\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8754e-05 - val_loss: 3.5971e-05\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6130e-05 - val_loss: 4.0566e-05\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6537e-05 - val_loss: 9.5381e-05\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0740e-05 - val_loss: 5.8030e-05\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1538e-05 - val_loss: 3.3397e-05\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9721e-05 - val_loss: 4.0054e-05\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.2462e-05 - val_loss: 3.8067e-05\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8192e-05 - val_loss: 4.6717e-05\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3313e-05 - val_loss: 3.8061e-05\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1003e-05 - val_loss: 7.2731e-05\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0773e-05 - val_loss: 4.1361e-05\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4501e-05 - val_loss: 3.7990e-05\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4714e-05 - val_loss: 3.9758e-05\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8232e-05 - val_loss: 4.9339e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2692e-05 - val_loss: 7.1450e-05\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6884e-05 - val_loss: 5.8989e-05\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6756e-05 - val_loss: 3.8396e-05\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0881e-05 - val_loss: 8.1381e-05\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9970e-05 - val_loss: 3.8272e-05\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3199e-05 - val_loss: 3.5179e-05\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1945e-05 - val_loss: 9.6775e-05\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.2671e-05 - val_loss: 5.3113e-05\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7006e-05 - val_loss: 4.1531e-05\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7248e-05 - val_loss: 1.2335e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0925e-05 - val_loss: 5.1866e-05\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9387e-05 - val_loss: 1.5413e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1440e-05 - val_loss: 7.7891e-05\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0671e-05 - val_loss: 7.0964e-05\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8889e-05 - val_loss: 4.4086e-05\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7857e-05 - val_loss: 4.9379e-05\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7927e-05 - val_loss: 6.8748e-05\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1809e-05 - val_loss: 5.4159e-05\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6664e-05 - val_loss: 1.1331e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2432e-05 - val_loss: 4.6664e-05\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7076e-05 - val_loss: 7.6247e-05\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3731e-05 - val_loss: 4.4425e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7910e-05 - val_loss: 5.4910e-05\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3220e-05 - val_loss: 3.7392e-05\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2409e-05 - val_loss: 3.6335e-05\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6623e-05 - val_loss: 1.2120e-04\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/GOOG/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0290 - val_loss: 0.0013\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.3160e-04 - val_loss: 3.1219e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.6006e-04 - val_loss: 1.6811e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7869e-04 - val_loss: 1.3948e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7142e-04 - val_loss: 1.2300e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5179e-04 - val_loss: 1.1750e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5013e-04 - val_loss: 1.0045e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4133e-04 - val_loss: 9.1664e-05\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3238e-04 - val_loss: 9.0279e-05\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5766e-04 - val_loss: 1.0145e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4852e-04 - val_loss: 1.1823e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4546e-04 - val_loss: 1.1664e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3351e-04 - val_loss: 7.5901e-05\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2854e-04 - val_loss: 7.8293e-05\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3763e-04 - val_loss: 1.3363e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3337e-04 - val_loss: 1.1074e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3002e-04 - val_loss: 1.0216e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3595e-04 - val_loss: 8.1711e-05\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1393e-04 - val_loss: 1.0915e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1680e-04 - val_loss: 9.8873e-05\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2949e-04 - val_loss: 8.4191e-05\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2949e-04 - val_loss: 6.7833e-05\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2724e-04 - val_loss: 8.9929e-05\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2667e-04 - val_loss: 5.4784e-05\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0586e-04 - val_loss: 6.9282e-05\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2428e-04 - val_loss: 6.6631e-05\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8680e-05 - val_loss: 7.1406e-05\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0128e-04 - val_loss: 1.5376e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1973e-04 - val_loss: 5.1332e-05\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4903e-05 - val_loss: 1.8558e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1135e-04 - val_loss: 1.3014e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0804e-04 - val_loss: 7.1880e-05\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0305e-05 - val_loss: 9.2337e-05\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0807e-04 - val_loss: 8.2368e-05\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0166e-04 - val_loss: 5.4425e-05\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1217e-04 - val_loss: 7.7090e-05\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4002e-05 - val_loss: 1.4053e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7768e-05 - val_loss: 5.3420e-05\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2703e-05 - val_loss: 4.8717e-05\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0774e-04 - val_loss: 4.2565e-05\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.0027e-04 - val_loss: 2.1350e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0096e-04 - val_loss: 5.5257e-05\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3383e-04 - val_loss: 7.1268e-05\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8019e-05 - val_loss: 6.6560e-05\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0302e-04 - val_loss: 8.0453e-05\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8662e-05 - val_loss: 4.2484e-05\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.2811e-05 - val_loss: 7.6579e-05\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.3270e-05 - val_loss: 8.2831e-05\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0245e-04 - val_loss: 4.6369e-05\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.9516e-05 - val_loss: 7.5724e-05\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2581e-04 - val_loss: 1.8407e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1197e-04 - val_loss: 1.6564e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 9.9965e-05 - val_loss: 1.2488e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5646e-05 - val_loss: 5.6182e-05\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.0710e-05 - val_loss: 4.8802e-05\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3124e-05 - val_loss: 5.2024e-05\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6045e-05 - val_loss: 4.2630e-05\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0889e-05 - val_loss: 8.0840e-05\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4394e-05 - val_loss: 4.3095e-05\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8285e-05 - val_loss: 4.8952e-05\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7151e-05 - val_loss: 5.0280e-05\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1895e-05 - val_loss: 5.3898e-05\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8098e-05 - val_loss: 5.5749e-05\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4857e-05 - val_loss: 1.3268e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.2666e-05 - val_loss: 3.8121e-05\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7518e-05 - val_loss: 1.2041e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4595e-05 - val_loss: 1.6746e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2276e-05 - val_loss: 5.1710e-05\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3813e-05 - val_loss: 4.8113e-05\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3315e-05 - val_loss: 4.2937e-05\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0589e-04 - val_loss: 1.1122e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2833e-05 - val_loss: 5.1373e-05\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2108e-05 - val_loss: 7.0373e-05\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5214e-05 - val_loss: 5.0335e-05\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7704e-05 - val_loss: 1.4422e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1160e-04 - val_loss: 1.3563e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8722e-05 - val_loss: 5.6855e-05\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6261e-05 - val_loss: 9.2441e-05\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7717e-05 - val_loss: 6.6962e-05\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0781e-05 - val_loss: 5.2979e-05\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2225e-05 - val_loss: 4.7404e-05\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8305e-05 - val_loss: 4.4515e-05\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.7723e-05 - val_loss: 4.2949e-05\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.0239e-05 - val_loss: 4.4930e-05\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.3372e-05 - val_loss: 5.3197e-05\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.9345e-05 - val_loss: 4.1974e-05\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.8028e-05 - val_loss: 4.2552e-05\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.0852e-05 - val_loss: 4.0635e-05\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 6.8548e-05 - val_loss: 5.6683e-05\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.7127e-05 - val_loss: 5.1371e-05\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.8589e-05 - val_loss: 8.8351e-05\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.8342e-05 - val_loss: 4.5425e-05\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 6.1715e-05 - val_loss: 4.2841e-05\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.9009e-05 - val_loss: 5.3188e-05\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0228e-05 - val_loss: 9.5374e-05\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7643e-05 - val_loss: 9.1086e-05\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.0715e-05 - val_loss: 4.4316e-05\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.1385e-05 - val_loss: 6.5550e-05\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.8389e-05 - val_loss: 9.6910e-05\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.2325e-05 - val_loss: 6.1367e-05\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.1548e-05 - val_loss: 1.7421e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0084e-05 - val_loss: 4.7460e-05\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5791e-05 - val_loss: 6.0257e-05\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4753e-05 - val_loss: 1.4164e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.6954e-05 - val_loss: 4.8326e-05\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.6976e-05 - val_loss: 5.2435e-05\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.5056e-05 - val_loss: 5.7118e-05\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.1447e-05 - val_loss: 7.7675e-05\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 6.3791e-05 - val_loss: 9.3390e-05\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0185e-05 - val_loss: 4.8990e-05\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4677e-05 - val_loss: 5.9233e-05\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.7477e-05 - val_loss: 6.0057e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3833e-05 - val_loss: 4.5547e-05\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4518e-05 - val_loss: 1.3816e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.7608e-05 - val_loss: 5.5068e-05\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.6728e-05 - val_loss: 7.7755e-05\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.8218e-05 - val_loss: 4.2094e-05\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5227e-05 - val_loss: 4.1425e-05\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1956e-05 - val_loss: 9.5836e-05\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.4132e-05 - val_loss: 4.6172e-05\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.0284e-05 - val_loss: 5.3404e-05\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3040e-05 - val_loss: 7.6145e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.5760e-05 - val_loss: 1.7320e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.7800e-05 - val_loss: 4.0193e-05\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.7625e-05 - val_loss: 8.9218e-05\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1517e-05 - val_loss: 5.7831e-05\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.2119e-05 - val_loss: 6.9349e-05\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.4570e-05 - val_loss: 5.8157e-05\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.0312e-05 - val_loss: 4.4094e-05\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.0063e-05 - val_loss: 4.8237e-05\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.8931e-05 - val_loss: 6.1024e-05\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.6791e-05 - val_loss: 5.5530e-05\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.1920e-05 - val_loss: 6.3182e-05\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3271e-05 - val_loss: 5.2956e-05\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5341e-05 - val_loss: 5.9065e-05\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.0185e-05 - val_loss: 1.2139e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.0237e-05 - val_loss: 5.2548e-05\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.0319e-05 - val_loss: 4.2892e-05\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.2115e-05 - val_loss: 4.8111e-05\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 7.9049e-05 - val_loss: 4.7179e-05\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2104e-05 - val_loss: 5.9281e-05\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0482e-05 - val_loss: 6.6008e-05\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.7969e-05 - val_loss: 4.4078e-05\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.8637e-05 - val_loss: 5.2366e-05\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0144e-05 - val_loss: 5.1111e-05\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.0234e-05 - val_loss: 4.5824e-05\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.1956e-05 - val_loss: 5.3917e-05\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.3859e-05 - val_loss: 7.3899e-05\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2864e-05 - val_loss: 3.7793e-05\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2801e-05 - val_loss: 4.8123e-05\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.7631e-05 - val_loss: 4.0102e-05\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.4608e-05 - val_loss: 6.0638e-05\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0409e-05 - val_loss: 7.7558e-05\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 7.2041e-05 - val_loss: 5.4553e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5774e-05 - val_loss: 4.6672e-05\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3930e-05 - val_loss: 5.5242e-05\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.4646e-05 - val_loss: 4.9032e-05\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.3522e-05 - val_loss: 4.9329e-05\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.8460e-05 - val_loss: 4.8078e-05\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.4096e-05 - val_loss: 6.0258e-05\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4947e-05 - val_loss: 6.3114e-05\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4241e-05 - val_loss: 4.8337e-05\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1329e-05 - val_loss: 1.1706e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.6538e-05 - val_loss: 2.0537e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.1829e-05 - val_loss: 1.1452e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7113e-05 - val_loss: 8.5104e-05\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7956e-05 - val_loss: 6.5061e-05\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5929e-05 - val_loss: 1.4674e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9666e-05 - val_loss: 1.4014e-04\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0035e-04 - val_loss: 6.4697e-05\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1441e-05 - val_loss: 4.2484e-05\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1808e-05 - val_loss: 5.2811e-05\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5933e-05 - val_loss: 5.1091e-05\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9253e-05 - val_loss: 4.9669e-05\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7354e-05 - val_loss: 4.8100e-05\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9570e-05 - val_loss: 7.5507e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4708e-05 - val_loss: 6.3410e-05\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 6.4791e-05 - val_loss: 4.5700e-05\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1900e-05 - val_loss: 1.1206e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3907e-05 - val_loss: 7.1863e-05\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/GOOGL/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0325 - val_loss: 0.0013\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.6019e-04 - val_loss: 3.6543e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 2.7767e-04 - val_loss: 2.1959e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9401e-04 - val_loss: 2.1070e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.7730e-04 - val_loss: 1.3877e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4497e-04 - val_loss: 1.1804e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4793e-04 - val_loss: 2.0350e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5052e-04 - val_loss: 2.4693e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4086e-04 - val_loss: 1.0152e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3495e-04 - val_loss: 1.0659e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3130e-04 - val_loss: 1.7652e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0597e-04 - val_loss: 7.9609e-05\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1415e-04 - val_loss: 8.2742e-05\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4753e-04 - val_loss: 7.0736e-05\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3535e-04 - val_loss: 6.6820e-05\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1168e-04 - val_loss: 1.6337e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3936e-04 - val_loss: 7.4316e-05\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1413e-04 - val_loss: 6.8715e-05\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1985e-04 - val_loss: 1.2764e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1399e-04 - val_loss: 6.3274e-05\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1121e-04 - val_loss: 1.9631e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0931e-04 - val_loss: 5.7145e-05\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0830e-04 - val_loss: 1.3169e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0440e-04 - val_loss: 7.6092e-05\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3844e-04 - val_loss: 7.6381e-05\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1133e-04 - val_loss: 7.4305e-05\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4129e-05 - val_loss: 5.9011e-05\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8687e-05 - val_loss: 9.7839e-05\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3009e-04 - val_loss: 1.1187e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0954e-04 - val_loss: 5.7352e-05\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0564e-04 - val_loss: 7.2530e-05\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6727e-05 - val_loss: 1.8677e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7518e-05 - val_loss: 2.2607e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0751e-04 - val_loss: 5.4579e-05\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0334e-04 - val_loss: 7.5810e-05\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5633e-05 - val_loss: 5.6873e-05\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0426e-05 - val_loss: 1.0482e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0100e-04 - val_loss: 8.6194e-05\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0200e-04 - val_loss: 1.6146e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3333e-04 - val_loss: 7.6196e-05\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0209e-04 - val_loss: 2.1952e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5538e-05 - val_loss: 2.2013e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7382e-05 - val_loss: 1.1839e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2482e-04 - val_loss: 5.7526e-05\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0252e-05 - val_loss: 6.8337e-05\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0765e-05 - val_loss: 1.0105e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2225e-05 - val_loss: 3.5130e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0485e-05 - val_loss: 4.8633e-05\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4053e-05 - val_loss: 1.8340e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1335e-04 - val_loss: 4.8224e-05\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0813e-05 - val_loss: 5.6781e-05\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7422e-05 - val_loss: 7.6917e-05\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0482e-05 - val_loss: 1.1571e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5482e-05 - val_loss: 7.6138e-05\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5396e-05 - val_loss: 9.6370e-05\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0584e-05 - val_loss: 4.8287e-05\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2645e-05 - val_loss: 9.9197e-05\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5083e-05 - val_loss: 5.2891e-05\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2699e-05 - val_loss: 4.5995e-05\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7008e-05 - val_loss: 8.7842e-05\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8988e-05 - val_loss: 6.1084e-05\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6267e-05 - val_loss: 7.7553e-05\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6691e-05 - val_loss: 1.1247e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8830e-05 - val_loss: 3.9414e-05\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2139e-05 - val_loss: 9.8564e-05\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.9562e-05 - val_loss: 4.9379e-05\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5356e-05 - val_loss: 6.2988e-05\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2444e-05 - val_loss: 1.1395e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4776e-05 - val_loss: 4.5813e-05\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5067e-05 - val_loss: 6.5565e-05\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0658e-05 - val_loss: 4.3529e-05\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9745e-05 - val_loss: 5.7849e-05\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1634e-04 - val_loss: 1.9611e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8173e-05 - val_loss: 5.4353e-05\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7619e-05 - val_loss: 4.1286e-05\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3006e-05 - val_loss: 7.9132e-05\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5779e-05 - val_loss: 1.2513e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7955e-05 - val_loss: 3.8250e-05\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0746e-05 - val_loss: 1.0643e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4029e-05 - val_loss: 2.2004e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9124e-05 - val_loss: 1.3474e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1151e-04 - val_loss: 8.6289e-05\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4884e-05 - val_loss: 5.5905e-05\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1159e-05 - val_loss: 1.1070e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8204e-05 - val_loss: 9.5230e-05\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3471e-05 - val_loss: 5.3562e-05\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0220e-05 - val_loss: 5.3385e-05\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3331e-05 - val_loss: 5.3447e-05\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.2532e-05 - val_loss: 4.5887e-05\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0218e-05 - val_loss: 6.4794e-05\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4787e-05 - val_loss: 5.0764e-05\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6955e-05 - val_loss: 6.7452e-05\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0553e-05 - val_loss: 5.3813e-05\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.9155e-05 - val_loss: 5.0344e-05\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3428e-05 - val_loss: 4.7502e-05\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2637e-05 - val_loss: 6.0429e-05\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8560e-05 - val_loss: 5.2310e-05\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.0720e-05 - val_loss: 5.3110e-05\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8985e-05 - val_loss: 5.6277e-05\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1452e-05 - val_loss: 4.6444e-05\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0433e-05 - val_loss: 6.3202e-05\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2096e-05 - val_loss: 4.7328e-05\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7782e-05 - val_loss: 4.4855e-05\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0471e-05 - val_loss: 4.8153e-05\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9485e-05 - val_loss: 5.1321e-05\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5934e-05 - val_loss: 4.6337e-05\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0768e-05 - val_loss: 5.4073e-05\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8851e-05 - val_loss: 6.4563e-05\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4858e-05 - val_loss: 3.5121e-05\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3748e-05 - val_loss: 3.6680e-05\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8229e-05 - val_loss: 5.4411e-05\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.2190e-05 - val_loss: 5.8397e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8328e-05 - val_loss: 4.4547e-05\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8093e-05 - val_loss: 4.2874e-05\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9200e-05 - val_loss: 5.4941e-05\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5940e-05 - val_loss: 5.6715e-05\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.6990e-05 - val_loss: 6.3070e-05\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.2905e-05 - val_loss: 4.2229e-05\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.6966e-05 - val_loss: 3.2365e-05\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.7140e-05 - val_loss: 5.5397e-05\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.1077e-05 - val_loss: 4.3858e-05\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.4499e-05 - val_loss: 7.1009e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.5727e-05 - val_loss: 7.8155e-05\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.6011e-05 - val_loss: 6.2981e-05\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.4002e-05 - val_loss: 3.4181e-05\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7211e-05 - val_loss: 5.7279e-05\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0389e-05 - val_loss: 1.1387e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4139e-05 - val_loss: 7.0634e-05\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.6915e-05 - val_loss: 5.7810e-05\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3477e-05 - val_loss: 7.6712e-05\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0063e-05 - val_loss: 6.3214e-05\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4365e-05 - val_loss: 7.2407e-05\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8794e-05 - val_loss: 5.3943e-05\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3025e-05 - val_loss: 4.8035e-05\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6338e-05 - val_loss: 1.2280e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6956e-05 - val_loss: 6.4854e-05\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4493e-05 - val_loss: 4.8992e-05\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3883e-05 - val_loss: 3.7679e-05\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5668e-05 - val_loss: 7.2866e-05\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5879e-05 - val_loss: 6.1659e-05\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1314e-05 - val_loss: 7.4480e-05\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0162e-05 - val_loss: 7.7390e-05\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6757e-05 - val_loss: 3.7426e-05\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5606e-05 - val_loss: 5.2583e-05\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.8146e-05 - val_loss: 4.5916e-05\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6223e-05 - val_loss: 7.0277e-05\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1834e-05 - val_loss: 4.3743e-05\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7280e-05 - val_loss: 4.9945e-05\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4660e-05 - val_loss: 3.6053e-05\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8517e-05 - val_loss: 5.2629e-05\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4789e-05 - val_loss: 7.1358e-05\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.2767e-05 - val_loss: 7.8656e-05\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6424e-05 - val_loss: 6.4135e-05\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4058e-05 - val_loss: 3.7138e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3058e-05 - val_loss: 5.6770e-05\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4597e-05 - val_loss: 3.9532e-05\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8285e-05 - val_loss: 4.9863e-05\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3391e-05 - val_loss: 1.2235e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3372e-05 - val_loss: 1.2893e-04\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7855e-05 - val_loss: 4.1962e-05\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.2501e-05 - val_loss: 4.6926e-05\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8792e-05 - val_loss: 1.4153e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5860e-05 - val_loss: 5.2330e-05\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3482e-05 - val_loss: 6.0444e-05\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5680e-05 - val_loss: 3.8427e-05\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3909e-05 - val_loss: 4.4904e-05\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7980e-05 - val_loss: 6.1023e-05\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4346e-05 - val_loss: 4.1685e-05\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7669e-05 - val_loss: 7.6280e-05\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0214e-05 - val_loss: 5.0382e-05\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8849e-05 - val_loss: 1.3168e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3038e-05 - val_loss: 4.5751e-05\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8175e-05 - val_loss: 4.6251e-05\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7035e-05 - val_loss: 5.0187e-05\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3390e-05 - val_loss: 8.0864e-05\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1476e-05 - val_loss: 5.6626e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5009e-05 - val_loss: 5.6968e-05\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8343e-05 - val_loss: 5.5754e-05\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8725e-05 - val_loss: 1.6148e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5379e-05 - val_loss: 5.1418e-05\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/MSFT/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0149 - val_loss: 6.4321e-04\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.2732e-04 - val_loss: 3.5702e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 3.1439e-04 - val_loss: 2.0050e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1330e-04 - val_loss: 2.0586e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8802e-04 - val_loss: 2.0380e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7279e-04 - val_loss: 5.0759e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5998e-04 - val_loss: 1.4000e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7712e-04 - val_loss: 1.6894e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6677e-04 - val_loss: 1.3798e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4069e-04 - val_loss: 1.7849e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5691e-04 - val_loss: 1.7150e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5216e-04 - val_loss: 1.0428e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5109e-04 - val_loss: 1.0434e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4164e-04 - val_loss: 1.0552e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3431e-04 - val_loss: 2.4393e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4951e-04 - val_loss: 9.2096e-05\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1857e-04 - val_loss: 9.2861e-05\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1226e-04 - val_loss: 9.7754e-05\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2129e-04 - val_loss: 1.0689e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2869e-04 - val_loss: 9.6522e-05\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2574e-04 - val_loss: 1.4812e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6137e-04 - val_loss: 1.1643e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5078e-04 - val_loss: 9.3751e-05\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1990e-04 - val_loss: 2.0530e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2098e-04 - val_loss: 1.0200e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2558e-04 - val_loss: 7.2876e-05\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0737e-04 - val_loss: 1.0465e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0223e-04 - val_loss: 8.5724e-05\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0815e-04 - val_loss: 1.1691e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8032e-05 - val_loss: 8.8108e-05\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0561e-04 - val_loss: 8.8992e-05\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0992e-04 - val_loss: 6.6422e-05\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2188e-04 - val_loss: 2.8891e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1907e-04 - val_loss: 7.7163e-05\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0121e-04 - val_loss: 1.3506e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1824e-04 - val_loss: 1.1344e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0526e-04 - val_loss: 2.4459e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1808e-04 - val_loss: 2.2340e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1222e-04 - val_loss: 9.1811e-05\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0837e-04 - val_loss: 2.2222e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1887e-04 - val_loss: 7.9794e-05\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0768e-04 - val_loss: 8.2976e-05\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1458e-05 - val_loss: 8.5980e-05\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6603e-05 - val_loss: 1.2951e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4515e-05 - val_loss: 6.8694e-05\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3747e-05 - val_loss: 1.4769e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2018e-05 - val_loss: 5.9843e-05\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3407e-05 - val_loss: 6.4138e-05\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8076e-05 - val_loss: 6.9074e-05\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0423e-04 - val_loss: 4.7214e-05\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0891e-04 - val_loss: 5.7327e-05\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.5806e-05 - val_loss: 4.8982e-05\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1221e-04 - val_loss: 5.6906e-05\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1695e-05 - val_loss: 9.0964e-05\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3751e-05 - val_loss: 5.4198e-05\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3536e-05 - val_loss: 9.5432e-05\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7743e-05 - val_loss: 7.2842e-05\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1177e-04 - val_loss: 7.5224e-05\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1821e-05 - val_loss: 6.8203e-05\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4567e-05 - val_loss: 6.0925e-05\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.8700e-05 - val_loss: 4.8948e-05\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2358e-05 - val_loss: 5.3318e-05\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7904e-05 - val_loss: 1.0014e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.8425e-05 - val_loss: 6.4025e-05\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2560e-05 - val_loss: 5.7508e-05\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3099e-05 - val_loss: 8.9469e-05\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4211e-05 - val_loss: 6.7726e-05\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9985e-05 - val_loss: 1.0570e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6452e-05 - val_loss: 5.4088e-05\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8933e-05 - val_loss: 9.4660e-05\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.8361e-05 - val_loss: 4.8653e-05\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2620e-05 - val_loss: 4.1919e-05\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.8233e-05 - val_loss: 1.3279e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.6133e-05 - val_loss: 7.1312e-05\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5780e-05 - val_loss: 8.3074e-05\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3070e-05 - val_loss: 7.0373e-05\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8368e-05 - val_loss: 5.5771e-05\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0047e-05 - val_loss: 4.6082e-05\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1181e-05 - val_loss: 6.7872e-05\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9070e-05 - val_loss: 7.6876e-05\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9788e-05 - val_loss: 1.1854e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1422e-04 - val_loss: 5.4032e-05\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1468e-05 - val_loss: 4.7809e-05\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5001e-05 - val_loss: 7.0266e-05\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7580e-05 - val_loss: 1.0921e-04\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5479e-05 - val_loss: 7.4853e-05\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0916e-05 - val_loss: 4.8383e-05\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4603e-05 - val_loss: 7.4542e-05\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1228e-05 - val_loss: 5.2421e-05\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9845e-05 - val_loss: 4.7595e-05\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9163e-05 - val_loss: 7.7918e-05\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6936e-05 - val_loss: 6.3864e-05\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3916e-05 - val_loss: 1.1863e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2112e-05 - val_loss: 6.2620e-05\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9446e-05 - val_loss: 5.5402e-05\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8450e-05 - val_loss: 7.5752e-05\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4256e-05 - val_loss: 6.7792e-05\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3434e-05 - val_loss: 4.7382e-05\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.2611e-05 - val_loss: 6.2678e-05\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.8611e-05 - val_loss: 6.1363e-05\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0021e-05 - val_loss: 8.3015e-05\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.3059e-05 - val_loss: 1.9024e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5289e-05 - val_loss: 9.0436e-05\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6851e-05 - val_loss: 7.6244e-05\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4234e-05 - val_loss: 7.4566e-05\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5483e-05 - val_loss: 3.0231e-04\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2416e-05 - val_loss: 6.4645e-05\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4234e-05 - val_loss: 4.4846e-05\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8849e-05 - val_loss: 5.1370e-05\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.6796e-05 - val_loss: 2.1168e-04\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6841e-05 - val_loss: 7.3864e-05\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5978e-05 - val_loss: 7.7553e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2779e-05 - val_loss: 5.4148e-05\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1221e-05 - val_loss: 7.3011e-05\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.0741e-05 - val_loss: 7.4953e-05\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9515e-05 - val_loss: 6.2691e-05\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0127e-05 - val_loss: 6.6352e-05\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5683e-05 - val_loss: 4.1571e-05\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1749e-05 - val_loss: 1.1412e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7040e-05 - val_loss: 4.0401e-05\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6332e-05 - val_loss: 4.3454e-05\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6631e-05 - val_loss: 5.4127e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9423e-05 - val_loss: 4.3392e-05\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0827e-05 - val_loss: 4.6339e-05\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7756e-05 - val_loss: 1.0728e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7076e-05 - val_loss: 4.3278e-05\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3492e-05 - val_loss: 5.9172e-05\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4133e-05 - val_loss: 3.8431e-05\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0379e-05 - val_loss: 4.7697e-05\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6191e-05 - val_loss: 4.1135e-05\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.9231e-05 - val_loss: 4.5371e-05\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6859e-05 - val_loss: 7.2273e-05\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1982e-05 - val_loss: 7.0516e-05\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5878e-05 - val_loss: 4.8605e-05\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3848e-05 - val_loss: 6.7642e-05\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.8730e-05 - val_loss: 7.3662e-05\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2763e-05 - val_loss: 5.8373e-05\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3278e-05 - val_loss: 5.4133e-05\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8641e-05 - val_loss: 1.0061e-04\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3691e-05 - val_loss: 1.3546e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6024e-05 - val_loss: 8.9516e-05\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3645e-05 - val_loss: 7.0953e-05\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7017e-05 - val_loss: 1.6428e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7160e-05 - val_loss: 1.4071e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3419e-05 - val_loss: 5.0055e-05\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7019e-05 - val_loss: 5.2831e-05\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0775e-05 - val_loss: 5.3226e-05\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5246e-05 - val_loss: 4.7074e-05\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4638e-05 - val_loss: 4.9160e-05\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4550e-05 - val_loss: 8.7182e-05\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2957e-05 - val_loss: 5.9257e-05\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8898e-05 - val_loss: 4.3698e-05\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5184e-05 - val_loss: 4.5495e-05\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5472e-05 - val_loss: 5.1294e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3585e-05 - val_loss: 5.4413e-05\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5350e-05 - val_loss: 6.7821e-05\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5208e-05 - val_loss: 6.8342e-05\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9288e-05 - val_loss: 4.0758e-05\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4935e-05 - val_loss: 8.5186e-05\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4486e-05 - val_loss: 6.1649e-05\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5635e-05 - val_loss: 8.9953e-05\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8637e-05 - val_loss: 7.6960e-05\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 6.3553e-05 - val_loss: 8.4775e-05\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6838e-05 - val_loss: 5.0341e-05\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4655e-05 - val_loss: 8.6403e-05\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4954e-05 - val_loss: 1.1205e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.4974e-05 - val_loss: 5.0574e-05\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3657e-05 - val_loss: 6.6137e-05\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1452e-05 - val_loss: 6.5224e-05\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2350e-05 - val_loss: 1.3183e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5703e-05 - val_loss: 5.7662e-05\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.9146e-05 - val_loss: 6.7655e-05\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9291e-05 - val_loss: 4.3008e-05\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0057e-05 - val_loss: 4.4798e-05\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.8100e-05 - val_loss: 4.3733e-05\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5073e-05 - val_loss: 7.2199e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1785e-05 - val_loss: 6.2893e-05\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9243e-05 - val_loss: 5.6505e-05\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6745e-05 - val_loss: 4.9687e-05\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6605e-05 - val_loss: 7.6750e-05\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/NVDA/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0172 - val_loss: 8.1909e-04\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.0904e-04 - val_loss: 5.7651e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.8284e-04 - val_loss: 4.4216e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 3.6945e-04 - val_loss: 6.5779e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 3.4316e-04 - val_loss: 3.6705e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.9431e-04 - val_loss: 5.6274e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.7342e-04 - val_loss: 2.6741e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.6522e-04 - val_loss: 2.7119e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3794e-04 - val_loss: 2.3158e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.6240e-04 - val_loss: 2.6459e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.6567e-04 - val_loss: 3.7108e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.4475e-04 - val_loss: 2.3029e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.2348e-04 - val_loss: 2.5276e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.4102e-04 - val_loss: 3.3296e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.2827e-04 - val_loss: 2.1802e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0762e-04 - val_loss: 1.9630e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1413e-04 - val_loss: 3.5545e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8956e-04 - val_loss: 1.7572e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8776e-04 - val_loss: 2.9285e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9827e-04 - val_loss: 1.6504e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1051e-04 - val_loss: 2.7526e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1239e-04 - val_loss: 3.5278e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8834e-04 - val_loss: 2.6258e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7807e-04 - val_loss: 5.8281e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0580e-04 - val_loss: 2.4950e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8121e-04 - val_loss: 1.8227e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6577e-04 - val_loss: 1.4515e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8191e-04 - val_loss: 1.9191e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8538e-04 - val_loss: 2.6086e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5812e-04 - val_loss: 1.2700e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6229e-04 - val_loss: 2.0096e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5206e-04 - val_loss: 1.6838e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5015e-04 - val_loss: 1.1207e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4049e-04 - val_loss: 1.8496e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4575e-04 - val_loss: 4.6090e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3174e-04 - val_loss: 1.2401e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6496e-04 - val_loss: 2.5779e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5816e-04 - val_loss: 1.4969e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5783e-04 - val_loss: 3.7219e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8448e-04 - val_loss: 9.0908e-05\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4989e-04 - val_loss: 1.7608e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2318e-04 - val_loss: 1.4127e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5981e-04 - val_loss: 1.4367e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4752e-04 - val_loss: 1.4584e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2417e-04 - val_loss: 1.0613e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2577e-04 - val_loss: 8.9113e-05\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5368e-04 - val_loss: 1.0252e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3129e-04 - val_loss: 1.6742e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5291e-04 - val_loss: 2.4665e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4851e-04 - val_loss: 1.1547e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4791e-04 - val_loss: 1.9668e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1094e-04 - val_loss: 8.9161e-05\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1733e-04 - val_loss: 1.6194e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2177e-04 - val_loss: 2.7842e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2856e-04 - val_loss: 9.7131e-05\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6960e-04 - val_loss: 7.9024e-05\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3072e-04 - val_loss: 1.4680e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1430e-04 - val_loss: 9.1487e-05\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2097e-04 - val_loss: 1.4074e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4504e-04 - val_loss: 1.3924e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4202e-04 - val_loss: 1.1193e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3651e-04 - val_loss: 1.0567e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2018e-04 - val_loss: 8.1221e-05\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4574e-04 - val_loss: 1.8257e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0064e-04 - val_loss: 1.0835e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3037e-04 - val_loss: 7.1482e-05\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1342e-04 - val_loss: 1.1647e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3791e-04 - val_loss: 1.0841e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2327e-04 - val_loss: 9.6853e-05\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2719e-04 - val_loss: 1.8593e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5868e-04 - val_loss: 1.1630e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1373e-04 - val_loss: 1.2159e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2698e-04 - val_loss: 1.7802e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1685e-04 - val_loss: 1.1334e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0630e-04 - val_loss: 8.5989e-05\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3109e-04 - val_loss: 2.2397e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1207e-04 - val_loss: 1.0158e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2934e-04 - val_loss: 6.9082e-05\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3187e-04 - val_loss: 1.2390e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1456e-04 - val_loss: 1.1212e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1016e-04 - val_loss: 7.3629e-05\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3297e-04 - val_loss: 7.3926e-05\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9213e-05 - val_loss: 1.1853e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2658e-04 - val_loss: 8.7653e-05\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8972e-05 - val_loss: 7.1397e-05\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0570e-04 - val_loss: 1.1247e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3747e-04 - val_loss: 6.8201e-05\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1475e-04 - val_loss: 7.6659e-05\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0316e-04 - val_loss: 1.0615e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1386e-04 - val_loss: 1.6843e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1795e-04 - val_loss: 1.1293e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1687e-04 - val_loss: 9.4214e-05\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0058e-04 - val_loss: 2.8585e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2096e-04 - val_loss: 7.5457e-05\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1863e-04 - val_loss: 1.1611e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1700e-04 - val_loss: 6.7305e-05\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1141e-04 - val_loss: 8.3579e-05\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1664e-04 - val_loss: 2.1130e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2441e-04 - val_loss: 2.9278e-04\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1621e-04 - val_loss: 1.5841e-04\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0226e-04 - val_loss: 7.6063e-05\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1009e-04 - val_loss: 8.1251e-05\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8313e-05 - val_loss: 1.8180e-04\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0629e-04 - val_loss: 1.1052e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0844e-04 - val_loss: 1.0320e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2936e-04 - val_loss: 9.3139e-05\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2688e-04 - val_loss: 6.8914e-05\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1828e-04 - val_loss: 1.1006e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8662e-05 - val_loss: 6.5313e-05\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1539e-04 - val_loss: 6.6499e-05\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0154e-04 - val_loss: 7.6059e-05\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3216e-04 - val_loss: 9.1403e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2923e-04 - val_loss: 6.3897e-05\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2282e-04 - val_loss: 1.8018e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0727e-04 - val_loss: 9.4609e-05\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0040e-04 - val_loss: 7.0510e-05\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0494e-04 - val_loss: 1.1899e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1063e-04 - val_loss: 1.1757e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1900e-04 - val_loss: 9.0568e-05\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0741e-04 - val_loss: 1.1504e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5931e-05 - val_loss: 8.4356e-05\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1177e-04 - val_loss: 8.7305e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7303e-05 - val_loss: 8.0166e-05\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1167e-05 - val_loss: 7.6938e-05\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1147e-04 - val_loss: 1.1338e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0476e-04 - val_loss: 6.4824e-05\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2865e-04 - val_loss: 6.8488e-05\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0147e-04 - val_loss: 5.9453e-05\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4221e-04 - val_loss: 6.9929e-05\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2194e-04 - val_loss: 7.5719e-05\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0400e-05 - val_loss: 1.1580e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1631e-04 - val_loss: 6.8627e-05\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1009e-04 - val_loss: 1.7124e-04\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3509e-04 - val_loss: 2.6021e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1227e-04 - val_loss: 7.4521e-05\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2059e-04 - val_loss: 1.4786e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0108e-04 - val_loss: 8.2994e-05\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0947e-04 - val_loss: 1.2080e-04\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0356e-04 - val_loss: 7.8500e-05\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5259e-05 - val_loss: 1.3047e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.1690e-04 - val_loss: 1.3498e-04\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1131e-04 - val_loss: 8.4554e-05\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.2782e-04 - val_loss: 2.2585e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1033e-04 - val_loss: 1.6340e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3202e-05 - val_loss: 1.2555e-04\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.0385e-04 - val_loss: 7.0006e-05\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1517e-04 - val_loss: 7.6117e-05\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1062e-05 - val_loss: 8.5773e-05\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1093e-04 - val_loss: 8.8831e-05\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3367e-04 - val_loss: 1.7538e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1435e-04 - val_loss: 7.1508e-05\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 8.6865e-05 - val_loss: 7.2092e-05\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.9756e-05 - val_loss: 6.9882e-05\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3347e-04 - val_loss: 1.0747e-04\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4150e-05 - val_loss: 1.2633e-04\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0947e-04 - val_loss: 8.2427e-05\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.5205e-05 - val_loss: 8.9189e-05\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.9140e-05 - val_loss: 7.5016e-05\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.6338e-05 - val_loss: 1.5132e-04\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.2392e-05 - val_loss: 6.4582e-05\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.8483e-05 - val_loss: 1.0531e-04\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.9143e-05 - val_loss: 9.2523e-05\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1239e-05 - val_loss: 1.0179e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4621e-05 - val_loss: 1.5354e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0965e-04 - val_loss: 1.3721e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1648e-05 - val_loss: 6.2163e-05\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 9.5296e-05 - val_loss: 9.8783e-05\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.5998e-05 - val_loss: 1.0668e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 9.1089e-05 - val_loss: 8.0938e-05\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0895e-04 - val_loss: 7.1130e-05\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.2687e-05 - val_loss: 1.5365e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.1118e-05 - val_loss: 1.1422e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0067e-04 - val_loss: 4.7860e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.3936e-04 - val_loss: 2.0186e-04\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0252e-04 - val_loss: 7.0493e-05\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.6481e-05 - val_loss: 7.1369e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0452e-04 - val_loss: 1.2658e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0394e-04 - val_loss: 1.7561e-04\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1114e-04 - val_loss: 8.2956e-05\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5832e-05 - val_loss: 7.1720e-05\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/TSLA/ann\\assets\n"
     ]
    }
   ],
   "source": [
    "histories = {}\n",
    "\n",
    "def get_all_file_paths(directory):\n",
    "  \n",
    "    # initializing empty file paths list\n",
    "    file_paths = []\n",
    "  \n",
    "    # crawling through directory and subdirectories\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for fileName_model in files:\n",
    "            # join the two strings in order to form the full filepath.\n",
    "            filepath = os.path.join(root, fileName_model)\n",
    "            file_paths.append(filepath)\n",
    "  \n",
    "    # returning all file paths\n",
    "    return file_paths\n",
    "    \n",
    "for tick in data.keys():\n",
    "    stock_data = data[tick]\n",
    "    df = pd.DataFrame(stock_data).T\n",
    "    df['H-L'] = df['High'] - df['Low']\n",
    "    df['O-C'] = df['Open'] - df['Close']\n",
    "    df['7MA'] = df['Adj Close'].rolling(window=7).mean()\n",
    "    df['14MA'] = df['Adj Close'].rolling(window=14).mean()\n",
    "    df['21MA'] = df['Adj Close'].rolling(window=21).mean()\n",
    "    df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "\n",
    "    features = ['H-L','O-C','7MA','14MA','21MA','7SD','Volume','Close']\n",
    "    df = df[features].apply(pd.to_numeric)\n",
    "    df_final = df[20:].copy()\n",
    "    df_final['Close'] = df_final['Close'].shift(1)\n",
    "\n",
    "    features = ['H-L','O-C','7MA','14MA','21MA','7SD','Volume']\n",
    "    #https://stackoverflow.com/questions/36926140/how-to-convert-numpy-arrays-to-standard-tensorflow-format\n",
    "    X = np.asarray(df_final[1:][features], np.float32)\n",
    "    Y = np.asarray(df_final[1:]['Close'], np.float32)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    scaler_x = MinMaxScaler().fit(X_train)\n",
    "    scaler_y = MinMaxScaler().fit(y_train)\n",
    "\n",
    "    X_train = scaler_x.transform(X_train)\n",
    "    y_train = scaler_y.transform(y_train)\n",
    "    # Defining the Input layer and FIRST hidden layer, both are same!\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=50, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "    # Defining the Second layer of the model\n",
    "    # after the first layer we don't have to specify input_dim as keras configure it automatically\n",
    "    model.add(Dense(units=25, kernel_initializer='normal', activation='tanh'))\n",
    "\n",
    "    model.add(Dense(units=10, kernel_initializer='normal', activation='tanh'))\n",
    "    \n",
    "    # The output neuron is a single fully connected node \n",
    "    # Since we will be predicting a single number\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    # Fitting the ANN to the Training set\n",
    "    history = model.fit(X_train, y_train ,batch_size = 10, validation_split = 0.1, epochs = 180, verbose=1)\n",
    "\n",
    "    histories[tick] = history\n",
    "\n",
    "    filepath_model = \"../../../data/models/\" + tick + \"/ann\"\n",
    "    model.save(filepath_model)\n",
    "    file_paths = get_all_file_paths(filepath_model)\n",
    "\n",
    "    \n",
    "    #took help from this: https://www.geeksforgeeks.org/working-zip-files-python/\n",
    "    with ZipFile(filepath_model + \".zip\",'w') as zip:\n",
    "            # writing each file one by one\n",
    "            for file in file_paths:\n",
    "                zip.write(file)\n",
    "    \n",
    "    \n",
    "    fileName_model = \"ann.zip\"\n",
    "    bucket = storage.bucket()\n",
    "    #upload models\n",
    "    blob = bucket.blob(\"models/\" + tick + \"/\" + fileName_model)\n",
    "    blob.upload_from_filename(filepath_model+\".zip\")\n",
    "    \n",
    "    #upload normalizer training data\n",
    "    filepath_normalizer = \"../../../data/normalizers/\" + tick + \"/ann_x.pkl\"\n",
    "    pickle.dump(scaler_x, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "    filename_normalizer = \"ann_x.pkl\"\n",
    "    blob = bucket.blob(\"normalizers/\" + tick + \"/ann_x.pkl\")\n",
    "    blob.upload_from_filename(filepath_normalizer)\n",
    "\n",
    "    #upload normalizer predicted value\n",
    "    filepath_normalizer = \"../../../data/normalizers/\" + tick + \"/ann_y.pkl\"\n",
    "    pickle.dump(scaler_y, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "    filename_normalizer = \"ann_y.pkl\"\n",
    "    blob = bucket.blob(\"normalizers/\" + tick + \"/ann_y.pkl\")\n",
    "    blob.upload_from_filename(filepath_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2816e62dd90>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD4CAYAAADo30HgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA0TklEQVR4nO3deXxU9b3/8ddnJitJCIGEPZCwCCJW0Ihapa1aK9pWrHXBti693qq/K7f20fZWbW+terW32qq3tlaLxYpUxbUVFRdkVZEl7EkgkA1IyE72fWa+vz/OmWEmmYRJWJI4n+fjwYPJmXPOfM/J5LzPdznniDEGpZRSysvR3wVQSik1sGgwKKWUCqDBoJRSKoAGg1JKqQAaDEoppQJE9HcBToTk5GSTlpbW38VQSqlBZevWrVXGmJTO078QwZCWlkZmZmZ/F0MppQYVETkQbLo2JSmllAqgwaCUUiqABoNSSqkAGgxKKaUCaDAopZQKoMGglFIqgAaDUkqpAGEdDKv2lPOXtXn9XQyllBpQwjoY1uZW8tz6gv4uhlJKDShhHQxOh+D26IOKlFLKX0jBICLzRCRXRPJE5N4g70eLyKv2+5tEJM3vvfvs6bkicrk9LVVE1ohIjohki8jdfvM/ICIlIrLD/nflCdjOoBwiaC4opVSgY94rSUScwNPAZUAxsEVElhtjcvxmuw2oMcZMEZEFwKPADSIyA1gAnAGMBT4WkdMAF/AzY8w2EUkAtorISr91PmmM+cOJ2sjuOB1ojUEppToJpcYwB8gzxhQYY9qBZcD8TvPMB5bYr98ALhURsacvM8a0GWMKgTxgjjGm1BizDcAY0wDsAcYd/+b0jlVj0GBQSil/oQTDOOCQ38/FdD2I++YxxriAOmBEKMvazU6zgU1+kxeKyC4ReV5EkoIVSkRuF5FMEcmsrKwMYTO6cjg0GJRSqrN+7XwWkXjgTeAnxph6e/IzwGRgFlAKPB5sWWPMImNMhjEmIyWly+3EQ+IU7XxWSqnOQgmGEiDV7+fx9rSg84hIBJAIVPe0rIhEYoXCS8aYt7wzGGPKjTFuY4wHeA6rKeuksGoMYLTWoJRSPqEEwxZgqoiki0gUVmfy8k7zLAdusV9fC6w21tF2ObDAHrWUDkwFNtv9D4uBPcaYJ/xXJCJj/H78DpDV240KlVMEQEcmKaWUn2OOSjLGuERkIfAh4ASeN8Zki8hDQKYxZjnWQX6piOQBR7DCA3u+14AcrJFIdxlj3CJyEXATsFtEdtgf9UtjzArgMRGZBRigCLjjhG1tJ047Ft0eg9MhJ+tjlFJqUAnp0Z72AXtFp2n3+71uBa7rZtlHgEc6TfsUCHokNsbcFEqZTgSHw1tj0CqDUkp5hfeVz3ZTknZAK6XUUeEdDHaNwa01BqWU8gnrYBC7xmA8/VwQpZQaQMI6GJx2L4fWGJRS6qjwDgaH9jEopVRnYR0MOipJKaW6Cutg0FFJSinVVVgHg0ObkpRSqouwDoajt8TQYFBKKa/wDgatMSilVBdhHQza+ayUUl2FdzDY1zFohUEppY4K62DQUUlKKdVVWAeDjkpSSqmuwjoYdFSSUkp1Fd7BoDUGpZTqIqyDQUclKaVUV2EdDEc7n/u5IEopNYCEdTA4/J75rJRSyhLWwaCdz0op1VVYB4P2MSilVFfhHQx6gZtSSnUR1sHg1BqDUkp1Ed7BoKOSlFKqi7AOBh2VpJRSXYV1MGhTklJKdRXewaCdz0op1UVYB4MOV1VKqa7COhi0xqCUUl2FdTA4fFc+93NBlFJqAAnvYLC33qPJoJRSPiEFg4jME5FcEckTkXuDvB8tIq/a728SkTS/9+6zp+eKyOX2tFQRWSMiOSKSLSJ3+80/XERWish++/+kE7CdQfmex6B9DEop5XPMYBARJ/A0cAUwA7hRRGZ0mu02oMYYMwV4EnjUXnYGsAA4A5gH/MVenwv4mTFmBnA+cJffOu8FVhljpgKr7J9PCu1jUEqprkKpMcwB8owxBcaYdmAZML/TPPOBJfbrN4BLRUTs6cuMMW3GmEIgD5hjjCk1xmwDMMY0AHuAcUHWtQS4uk9bFgIdlaSUUl2FEgzjgEN+Pxdz9CDeZR5jjAuoA0aEsqzd7DQb2GRPGmWMKbVflwGjghVKRG4XkUwRyaysrAxhM7rSGoNSSnXVr53PIhIPvAn8xBhT3/l9Y4wBgh61jTGLjDEZxpiMlJSUPn2+Q5/5rJRSXYQSDCVAqt/P4+1pQecRkQggEajuaVkRicQKhZeMMW/5zVMuImPsecYAFaFuTG/pLTGUUqqrUIJhCzBVRNJFJAqrM3l5p3mWA7fYr68FVttn+8uBBfaopXRgKrDZ7n9YDOwxxjzRw7puAd7u7UaFSu+uqpRSXUUcawZjjEtEFgIfAk7geWNMtog8BGQaY5ZjHeSXikgecAQrPLDnew3IwRqJdJcxxi0iFwE3AbtFZIf9Ub80xqwAfge8JiK3AQeA60/g9gawc0FrDEop5eeYwQBgH7BXdJp2v9/rVuC6bpZ9BHik07RPAelm/mrg0lDKdbx8TUnax6CUUj5hfeWzrylJawxKKeUT1sHg0BqDUkp1EdbBAFZzktYYlFLqKA0GER2VpJRSfsI+GBwOHZWklFL+wj4YrBqDBoNSSnmFfTA4HKI1BqWU8qPBIKKjkpRSyk/YB4OOSlJKqUBhHwwOHZWklFIBwj4YnA69wE0ppfxpMIg2JSmllL+wDwaHQzuflVLKX9gHg3Y+K6VUIA0GvcBNKaUChH0wOByCVhiUUuooDQZBawxKKeVHg0FHJSmlVICwDwanjkpSSqkAGgw6KkkppQKEfTA4dFSSUkoFCPtgcOptt5VSKoAGg9YYlFIqQNgHg8MBHr27qlJK+WgwiDYlKaWUv7APBh2VpJRSgcI+GPTRnkopFSjsg0FrDEopFSjsg0Ef7amUUoHCPhj00Z5KKRUopGAQkXkikisieSJyb5D3o0XkVfv9TSKS5vfeffb0XBG53G/68yJSISJZndb1gIiUiMgO+9+Vx7F9x6RNSUopFeiYwSAiTuBp4ApgBnCjiMzoNNttQI0xZgrwJPCovewMYAFwBjAP+Iu9PoAX7GnBPGmMmWX/W9G7Teod7XxWSqlAodQY5gB5xpgCY0w7sAyY32me+cAS+/UbwKUiIvb0ZcaYNmNMIZBnrw9jzHrgyAnYhuOiNQallAoUSjCMAw75/VxsTws6jzHGBdQBI0JcNpiFIrLLbm5KCjaDiNwuIpkikllZWRnCKoPTC9yUUirQQOx8fgaYDMwCSoHHg81kjFlkjMkwxmSkpKT0+cOspqQ+L66UUl84oQRDCZDq9/N4e1rQeUQkAkgEqkNcNoAxptwY4zbGeIDnsJueThanQx/tqZRS/kIJhi3AVBFJF5EorM7k5Z3mWQ7cYr++FlhtjDH29AX2qKV0YCqwuacPE5Exfj9+B8jqbt4TQfsYlFIqUMSxZjDGuERkIfAh4ASeN8Zki8hDQKYxZjmwGFgqInlYHcoL7GWzReQ1IAdwAXcZY9wAIvIK8DUgWUSKgd8YYxYDj4nILMAARcAdJ3B7u9BRSUopFeiYwQBgDxld0Wna/X6vW4Hruln2EeCRINNv7Gb+m0Ip04miNQallAo0EDufTyl9tKdSSgUK+2BwOrQpSSml/GkwaFOSUkoFCPtgEAGtMCil1FFhHwxOHZWklFIBNBi0KUkppQKEfTA4RDAGjIaDUkoBGgw4HQLobTGUUspLg8EbDFpjUEopQIMBh1jBoHdYVUopS9gHg9PeA1pjUEopS9gHg7fGoH0MSill0WCwg0FHJSmllCXsg0FHJSmlVKCwDwaHjkpSSqkAYR8MTh2VpJRSATQYdFSSUkoFCPtgOHodgwaDUkqBBoN2PiulVCcaDNr5rJRSAcI+GLQpSSmlAmkweINBc0EppQANhqOjkjQZlFIK0GDwqzFoMCilFGgw6KgkpZTqJOyDQW+JoZRSgcI+GJw6KkkppQJoMGhTklJKBQj7YPA9qEebkpRSCtBg8NUY9O6qSillCftgsHNBh6sqpZQtpGAQkXkikisieSJyb5D3o0XkVfv9TSKS5vfeffb0XBG53G/68yJSISJZndY1XERWish++/+k49i+Y9JRSUopFeiYwSAiTuBp4ApgBnCjiMzoNNttQI0xZgrwJPCovewMYAFwBjAP+Iu9PoAX7Gmd3QusMsZMBVbZP580OipJKaUChVJjmAPkGWMKjDHtwDJgfqd55gNL7NdvAJeKiNjTlxlj2owxhUCevT6MMeuBI0E+z39dS4CrQ9+c3tNRSUopFSiUYBgHHPL7udieFnQeY4wLqANGhLhsZ6OMMaX26zJgVLCZROR2EckUkczKysoQNiM4vSWGUkoFGtCdz8YYAwQ9YhtjFhljMowxGSkpKX3+jKM1hj6vQimlvlBCCYYSINXv5/H2tKDziEgEkAhUh7hsZ+UiMsZe1xigIoQy9pk+81kppQKFEgxbgKkiki4iUVidycs7zbMcuMV+fS2w2j7bXw4ssEctpQNTgc3H+Dz/dd0CvB1CGftMH9SjlFKBjhkMdp/BQuBDYA/wmjEmW0QeEpGr7NkWAyNEJA/4KfZIImNMNvAakAN8ANxljHEDiMgrwOfANBEpFpHb7HX9DrhMRPYDX7d/Pmm081kppQJFhDKTMWYFsKLTtPv9XrcC13Wz7CPAI0Gm39jN/NXApaGU60TQzmellAo0oDufTwXvBW4aDEopZQn7YPBe4KajkpRSyhL2weDQUUlKKRUg7INBb4mhlFKBNBh0VJJSSgUI+2DQzmellAoU9sFwtPNZg0EppUCD4WhTktYYlFIK0GDArjCguaCUUpawDwZtSlJKqUAaDDoqSSmlAoR9MIgIIjoqSSmlvMI+GMBqTtIag1JKWTQYsK5l0FFJSill0WDAqjHoLTGUUsqiwYDVAa13V1VKKYsGA2jns1JK+dFgwKoxaDAopZRFgwEdlaSUUv40GLBGJWmNQSmlLBoMaI1BKaX8aTAAsVFOWjp0WJJSSoEGAwDx0RE0tnb0dzGUUmpA0GDADoY2V38XQymlBgQNBiAuOoLGNnd/F0MppQYEDQYgISaCxjZtSlJKKdBgALx9DNqUpJRSoMEAQHyM9jEopZSXBgNWjaHDbWhzaT+DUkppMGAFA6DNSUopRYjBICLzRCRXRPJE5N4g70eLyKv2+5tEJM3vvfvs6bkicvmx1ikiL4hIoYjssP/NOr5NPDZfMGhzklJKHTsYRMQJPA1cAcwAbhSRGZ1muw2oMcZMAZ4EHrWXnQEsAM4A5gF/ERFnCOv8L2PMLPvfjuPZwFDEx1jB0NCHGkNRVRNrcitOdJGUUqrfhFJjmAPkGWMKjDHtwDJgfqd55gNL7NdvAJeKiNjTlxlj2owxhUCevb5Q1nnKeGsMTX2oMTz3SQE/WbbjBJdIKaX6TyjBMA445PdzsT0t6DzGGBdQB4zoYdljrfMREdklIk+KSHSwQonI7SKSKSKZlZWVIWxG946nKammuZ361g59NKhS6gtjIHY+3wdMB84FhgP3BJvJGLPIGJNhjMlISUk5rg/0NiX1JRjqWjowBhq0f0Ip9QURSjCUAKl+P4+3pwWdR0QigESguodlu12nMabUWNqAv2M1O51Ux1NjqG9x2f/rldNKqS+GUIJhCzBVRNJFJAqrM3l5p3mWA7fYr68FVhtjjD19gT1qKR2YCmzuaZ0iMsb+X4Crgazj2L6QHM9w1To7EOr17qxKqS+IiGPNYIxxichC4EPACTxvjMkWkYeATGPMcmAxsFRE8oAjWAd67PleA3IAF3CXMcYNEGyd9ke+JCIpgAA7gDtP2NZ2Y0iUE5G+NyXB0ZqDUkoNdscMBgBjzApgRadp9/u9bgWu62bZR4BHQlmnPf2SUMp0IokI8dERvR6u6vEYX01BawxKDW55FQ38Y+NB7v/WDBwO6e/i9KuB2PncL+KjI3o9XLWx3YX3UdHax6DU4LYyp4IXNhRR2djW30XpdxoMtr48rKeu+WgY1Ifh7TTW5Fbw2pZDx55RqUHAV/vXkzwNBq++3GG1zu8LFI5fpn98foA/r8nr72IodULU+waShN9JXmcaDLa+9DH4h0E49jHUtXQEhKNSg5mOMDxKg8HWlz4G/y9QX+6zNBCsya2gtK6lT8vWtXToVd/qC8NbUwjH2n9nGgy2PvUx2F+ghOiI4/oyHajunxvxeTyG21/MZPEnhX1avr5Vr/pWXxzev+fBepJ3Imkw2OJjev94T+8XafzwIcdV/XxmbT4LX9qGMaf2zLuupYMOt6GsvrXPy4OeYakvhgZtSvLRYLAlREfYw09DPzjXtXTgdAhjE2OO6wK3ktoWmtrdp7zTq7qpHYCKht4Pz2tzuWnt8ABoP4P6QtCLVY/SYLDFRUdgDDS3W4/33JBXRVZJXY/L1Le4GBoTQWJs5HGdZZTVtQb8f6rUNFvBUNmHYPAPAw0GNdgZc/Ri1QatMWgwePnfYXXp50V872+b+NW/er5NU11LB0NjIxkaG3lczSm+YOhjk05fVTfaNYY+fK7/WZUGgxrsWjs8dLit1gIdrhriLTHCgfdGei9sKOKZtfkMiXKyp7SeDreHSGfw/Kxr6SAxNpKhMRE0tLnweEyvL6VvaO3wdd6Wn+IawxG7Kamp3U1jm8u3D0KhNQb1RRLu1yR1pjUGm/eg+Oy6fM5LH85D82fS7vKQX9nY7TK+YIiNxBjrFhm9Ve53tl56yoPhaBNSb2sN9RoM6gskcOj58X+f1+yt4IOssuNeT3/RYLB5gyHS4eB/rzmTWanDAMgqqe92mXpvU1JMpO/n3jpce/SAfKqbko40HS1vbzugtcagvkgChp6fgKakJ1bu44mVuce9nv6iwWBLTrCeIPofF09mUko86clxDIly9tgBXd/awdCYSIbGWqHSl9EM3v6FoTERAbWHU+FIUxtit3z1Nhi8Z1gRDtFgUINevf/Q8+P8PhtjyK9spKSm5ZQPQT9RNBhsk1PiefuuC/nxJVMBcDqEGWOGkn04eDAYY/z6GOwaQx+qoN7mo7NSh53yUUnVTe2kjYgDet+U5L2B4NhhsRoMakArrmlmV3Ftj/N4/3bHJ8Ue9wVuZfWtNLe7aWp3D9q/DQ0GP2elDgvoPJ45LpHsw/VBb/nQ0uGmw218fQxgXTF56Ehzr84SyupbSI6PJnX4kH5oSmonPTmOqAhHr4es1rV0EBvpZER8lHbW9ZPuvmf7yhv4LK8q6HuvbjnI+n2VJ7NYA85v3s7m9he39jiP90RnfFIsLR1u2l2eHuc/dKS52/fyK5p8r4tr+na7mf6mwdCDM8YOpbndTWF1U5f3vGcC/jWGNbkVzH1sDa/at6L++es7+d37e3v8jMO1rYxJjGH00BiONLXT5nKf4K3oXk1TO8PjokiJj+5TH0NibCSJsZGD9qyov3yUXcalj6/t0xMDvd7eUULGwx/THGTAw2Mf5LLw5eBX0j/6QS7PfVLQ588dbNpcbjbkV1NW39pjp7K3X2HcsFig5w7orJI65j62ho0F1UHf9x+wUlLb92D4MLuMd3cd7vPyx0ODoQczxyUC8PaOw13+yLz9CVaNwepjeD3TCoQ/rtrPypxy3thazCubD+Lu4SZzZXWtjLaDAaCivm8PCVm9t5zfrtgTMG1DXhW3v5gZ9PONMVQ3tTMiLopRQ6N73b9R3/rFCIaNBdXM/M2H/Oy1neRVdD8C7UTakF9NfmUT7x3HH/2WoiNUN7WTfbjr4Ij8ykZqmjs43Klpsra5nSNN7Ryo7v5sNxhjDJsKqgdle/m2A7W0dFgnWz1td11LB3FRTobHRQE93y9pV7HVvLzzUG3Q9/MqGomyh7j3tcbQ2uHm3jd38fhH+/q0/PHSYOjBaaMSuGT6SJ5atZ9rn/2cy55Yxx1LM2lpd/uuARgaG+Eb0dThNpw9YRilda3c9fI2X8fs7pI6apraWbKhyFcjaO1wY4yhtK6FsYkxjEq0gqGvQ1YXrS9g0fqCgIcHvbW9hI9yyikIMuS2ud1Nm8tDUlwUIxNi+lRjGBobMeiD4ZP9lTS3u1ixu5TvPrOh1/08Ww/UBD1r70lhlVUDfT2zuFfL+fM2V3gPUl5tLjcH7WaOzgMnCuzPLa5pPmZTib81uRXcsGgjb+/on7PX4/Fp3tFmM+/2B9NlhGEPNYb9FQ0A5JY1BH0/v7KR08cOJTbSSUkfg+HdXaXUNHdwoLqJ1o5T14rgpcHQA6dD+NvNGdx3xXRqmtsZnRjDRznl/OjFTH7x5k6iIxxMGRlPhNPhC4fHr5/F+ZOG0+7y8OD8MxCBT/ZV8sTKffxmeTZ3vbTd1wyw8JXt1Le6GJ0Yyxg7GPrSz9Dc7mLrgRoAdvh1sm0/aE3bHWRklTfYhsdFMXJodO87n1tcvma0+pbe3Xp7U0F1r29xfrLsKW1gysh43v3xRbS53PzizV0hnxlX1Ldy3bMbeHZd75pmiqqbcDqEzAM1QUM7FN7mis4H/4PVzb4aYnbnYKi0DoweY4VDqFbmWHf+fWnTgT6VtT99sr+KM+2af2Hl0WD4IKuUO5du9f2u61q8Iwy9Q8+7/356a5a55d0Hw5SUeMYnxVJS27vamdfSz4sQsX5XRUGask82DYZjcDiEO746mdU/+xpLbzuPR64+k0/zqmhp9/DqHRcwJtFqk0xJiOarp6WQnhzH7689i4evnsn35kxg5thE3ttdyhtbi5mUEsfHe8q5e9kOEmIieG9XKQBjEmMYZTcl9eXq502FR3yX8+84WAtYnWn59h9CsGDw3kBvRFwUIxOiqW918et/ZXHtMxtwuY99Nuk9w0qMjcTTi4v7SutaWPDcRl7YUBTS/F7N7S62FB3p1TKh2FNaz+ljhjI5JZ5fXXk66/dV+vqIgvkou4yr/vwprR1uth2sxWNgXS9umd7h9lBc08I1s8fhEHh9a+9rDQ2tHb4aXufRNt6DVlSEg6xOzUyFVUdDKNSDjTGG1XvLiY5wsKWohn3dHAxDWc+WoiOntDmqpqmd3SV1XDZjFOOGxQZs88ubD/FBdpmvD8DbNJpg3xqnpz6G/eXWftxf0djlb6WhtYPy+jYmj4xjXFJsn/oYdh6qZWdxHQvOTQ34vFNJg6GXvnfeBF7+9/N478cX+S6CA3j+1nP5vxtmAZA6fAg/OH8iIsLcqcnsLWugpcPNX75/No9990v8+JIprP2vr/Gd2eMAayTE0JgIYiOd/HV9AWc9+BEf55QDVluy98y/M+8f2Sf7qoiKcJCeHMf2Q4E1h5hIB7uLuwZDjX+NIcEKpaUbD5B5oIZ3grR9uz0m4I+g3q/zGY6O6uhwe3psWtp2oBZjILOXB/n7387mumc/5++f9f7ZES63h79/VsjhTn+ktc3tlNa1cvqYoQD84PyJnDU+kUXrC/B4DH9evZ8r/vhJQG1o8aeF7CquY+uBGt/vZVdJHdUhPkD+0BHrjP68SSO4/IzRLP60sNu26u54z/xnjhtKQVVTQCe2tyZxybSRXU4ICquaSBpi/b6KqkI7k80+XE95fRs//8Y0opwOXt50sFdl9fpkfxXXPfs5H+85dc8dWbW3AmPgoqnJpCfH+ZqS2l0ethRa3z9vU1x9i4uhsRFHawydgqGqsY2CykbqWzsoq29lckoc7S4PBzqNTvL+bianxDNuWGxAU1Jzu4uqEL4nH2aXEeEQfvaNaTjECqBTTYOhD748Jdl3hu+VnhxHkt1x5W/u1BT7/2Smjx7K9eem8tNvTCM6wsnvvnsmz92cwdkTkhARLj19JCkJ0QwbEsl/vbGTt7YV873nNnLDXzeyak95wHqrGtv4+hPruO+tXazfX8l56cM5L3042w/WYoxhx8FaROBbXxpL9uH6Lh3Q1X7BMD7JqvXcdP5EThsVzzNr88k5XM+PXsz0tYfftmQL33tuEx1uD26PoaHNFVD19obBYx/s5Wu/XxPQme1ye3xnTtvsg+n2Q7Uhnz3mVTTy1rZikoZE8uA7OSzf2bu27vX7K3nwnRyue/ZzDvidNeaUWmfUM+xgEBFuvTCNgqom/rm9hD+vyWNPaT1Z9rUsh2tb2GQfUNbvr2T7wVqGxlh35f20m+GhnXnPWtOTh/DId85kZEI0dyzdSkVD6DVF78H/6lnjMCawySi/somxiTHMSR9OZUNbQBNhQWUTsyckkRAdEbAferJqTwUicM3Z45g3czRvbisOeeTc+n2VvLWt2PcaYPXeUxcML206wKSUOGanDiMteQiFlY0YY9hZfLRD2hvK3htiemsMnZuSfv2vLL77zAZy7FrYN780FoB9nfoZvM1LU0bGMy4plprmDl8f1P1vZ/Otpz49Zv/O5sIjnDk+keT4aCYMH0JeRd9qacdDg+Eky0hL4prZ4/jF5dO7vBcd4eSyGaN81078+Xtn8/7dc1l8y7m0dLj56Ws7mZQcz/QxCdz5j61c8L+rOO+3H/P2jhJ+smwHB6qbeWXzIfIqGrloSjKzUodR19JBYVUT2w/VcNrIBM6fNIKWDneXtmzvfZKGx0Vx/qQRvPn/LuDBq87gzq9OZl95I/Of/pSVOeUs2VBERUMr6/ZVsrnoCE+t2u+7bsG/xlDf0oExxtdpdt9bu30H/t+u2MvFf1hLRUOr7yy7trmjx85Af//38T5iIp289+O5zEodxsPv5tBh115q7VuH92RlTjlDopw0t7u4/q+f+9rXvX/k3hoDwJVnjmFEXBT3vLkLl9sgYh0cAV8gTRg+hHW5lewqqeWas8eTNCSSdblHOzk/zC7r9sLIQvtMPW1EHMPjolh0Uwa1Le3c88bRvg1jDL/+VxYrc8qDriO/spEIh/DNL40BrKbCoqomXG7r3l6TR8b7RtR5Q83jMRRWNTEpOY605DgKjzEy6Zf/3M2Zv/mQv31awOzUYYyIj+bq2WNpaHWxIS/4ME1/a3Mr+LcXtnDPm7uobW7ns3xrmfX7KkM6IciraOCW5zcHBFhdSwe//3BvSIMdskrq2H6wlh+cZ9Xc05PjqW91UdPcwYa8akRgUkocO+xg8N7FID4qApHApqQ2l5v1+yqpae7g2XX5AFwxczQisNcvGDwew5INRaQOjyVtRJxv6GtJTQsut4ePsssoq2/t9vcK1qCUXcV1zEkfDlgB4z9abk9pPW9sDT2c+0qD4SSLdDp44oZZnDk+MeRlpoyM53fXfIlz05J48bY5LL3tPK49J5UvT05mdGIsdy/bwad5VTzynZn87pozGZ8Uy+VnjGb2hCTAGimz41AtsycM83W8eZsV2l0eKhvaqG5qJ8ruNHc4hHMmDsfhEL591limjIzn9DFDuWDSCN7bXcoHWWUYA+dPGs6f1+Sxyj7rC2hKaukgq6Se0rpWzk1LYvXeCpZsKCKvooElnxfR7vLwemYxWYfruXiaVYvabveH9GTdvkre3VXKDy9MY+ywWBZePIWKhjY+ziln8aeFnPvIxz1e1erxGD7eU8HF00byyu3n09Lu5pbnN1Pb3M6e0gaS46NJsW+HAlZYL5iTistjuOHcVGanDvM9dvVf20uYlTqMa88Zz96yBlo7PGSkJTF3agrr91fh8Rg+z6/mjqVbuerPn/HoB3t9NbXVe8vZVVxLUVUTCTERvmGRM8YO5Z5501mTW+nrb9hVXMfSjQd4YHk27S4P7S5PQA2soLKJCSOGMMYetPDoB3v52h/W8uA7OeRXNDI5JZ4ZY62w89ZwSutbaXN5mJQSz8QRQ3wH3GCDALYUHeHlTQeZPiaBSclx3PLlNAAunJJMfHTEMW8Ot6e0njv/sZUxw2LocBte2nSQPaX1TBwxhJLaFvIrG3l7Rwl3Lt3K1U9/5hs44W/JhgOs21fJHUu3+s64f/veHp5ek8/Sz4t8ZfeeIBTXNPP7D/fyxMp9vLblEM+szScm0sF3zxkPwKRk6wr/wqpGNuRXccbYocydkkxWSR0dbg8NrS6GxkbicEiX+yVlFtXQ1O7GIbA2t5KoCAenjUpg4vAhAX0u72eVkX24np9edhpOh/hq4sU1LWw9UEN9qwunQwI68XccquWJj3J9zbQ7DtXS7vYwJ80bDAkUVjXR4fbg8RjuXradn7++k0v+sI5P94dWS+0Lve32AHX17HFcbfdBAPzvNWcCVrPMok8K6HAZrs9IRURYMGcCYPUDxEdH8F9v7AJg9oRhTE6JIzbSyfOfFfLChiL7VuKGIVFOkuIiEZGAz410OnjvxxcR5XSwYncZd728jadW7WdSchx/u+VcLn9yPf/zbg5gB8OQo8GQU1qGQ+CZH5zDz1/fyQPv5DBqaDRDopyMTYzl2bX5tLs8XHtOKpkHath2sIZr7T9c77ZtLjrCuWnDiXQ62F/ewMKXtjF9dAL/8bUpAFw8fSRjE2P4y9p88isb6XAb/vjxfhbfei6f51czIj6K00Yl+Na5q6SOyoY2vj5jJNNHD2XRzRncvHgzNz63iaY2F6ePSaCzH16YTlldGz/5+mm8uuUgf/hoH3//rJC9ZQ088O0ZnJU6jCdW7rP3cRJuj2H5zsPcvzyLDXnVpA6P5fz0ETyzNp+xiTFcNmM0dy7dxqjEaMYNiyU9OS5gv99yQRofZJXxP+/kcNGUZF7NPIRDrIujXtp0gA+yyth2sIZfXD6d2y5KJ7+ykUnJ8QB8/7wJbCo8QnSEk6UbrQPO5JQ44qMjuPyMUTz/aSHfPHOM7yw7PTmOw7UtvJ9Vxm9X7OEfGw+wfOGFTBmZQHVjG1WN7fz6X1mMSYxhyb/NYUjU0UNEdISTS6aP5KOcMh5xzyTC73b02w7WYAycPWEYD7+XQ2ykk7f+34XcsOhz/rR6PwA//8Y0/vOV7fzu/VxW7S1nbKJ1lfHPX9/J+3fPJSbS6fsevJ9VytSR8eSWN3DH0q1cMXMMr2YeIirCwUubDnLTBWl860+fkBgbyZIfzuHfl2SSW96Af2VkwbmpvhOXNDsYthTVsP1gLbdemMb00Qks+fyAr9bgnTchJvD5Kmv2VhDldPCD8yfy/GeFTEqOw+kQpo1O8DUddbg9PLEyl9NGxXPVWdbf7bhhQwDrme6lda1EOoV/nzuJZ+zv7sThQ/jpazsoqGyivtXFA1edwebCI4hAxkQrGKaOjKfDbThQ3cz+8gb2lTdy+1cmsWpPOf/x0lZW3D2X8UlDunyHj5cGwyAT4XT4DpKdOR3CH647iz2l9QwbEsm3zxpLhNNBRloSG/KrOWdCErddNIn4aCfv7ipl2uiuB0WwDgAAl0wfyZAoJ1WN7VyfkUp8dAS/mDeNu5ftAPCNSgIrGFbmlJORNpzk+GieuzmDh9/NYcnnB7j/WzOIjHDwa/vBR+dMTGJW6jC2H6ylobWDxjYXcdERLHx5O+v3VXL6mKF8/fSRLN14gOhIJ4tvPZc4eziw0yHcOGcCj6/cR3SEgxvnpPLK5kP88p+7fR2jZ45LJD46gqS4SN8yF08bCcD5k0aw6OZz+OlrOznS1M4VM0d32f7k+Ggev/4swAqiP3y0jwffySFjYhIL5kwg0ulgaEwEMZFOxibG8G27H2fRemvY6ov/Noe5U5M5UN3Mn1bnsbukjna3h0NHWjh0pIWrzhob8HkOh/D7a89i3h/X87PXdpJVUsf8WeMoqGriwXesEM6YmMQjK/bwflYpRVXNXDzd2p6Fl0xlIdDS7ubKpz6hsKqJySlWaPzumi/xzac+4a6XtzFzrFVznJwSR0ltC26P8ZX3v/+VxZ1fncztS7f62r+f/t7ZAaHgdcXM0SzfeZiH39vD5sIjzD0tmfHDYnngnRycIvzoK+l8llfN/d+aQUpCNFfPGscTK/eREB3BFTNHM9kemXfaqHjevusith6o4QeLN/HHVfu5Z57V3Lqp8AhVje08NH8m1Y1t/HbFXj7ZX8XEEUP46WWncfeyHdy4aCPFNS0crm3lksfXUdfSwd9vPZevnpZCQVUTWSV1fOW0FF+5xyfFEuEQ350ILp0+0nfjzIffsy4MHTU02ve99q8xrMmt4LxJw7npAisYptonHmdPSOLD7HI+yCplV3Ed+ZVN/O3mDJx20/DIhGgmpcTx9Np8oiMcnD9pBD+8MI3n1hfw8Ls5fG3aSAoqm5iTPpwXNhSRHB/FxoJqpo1K8J1wTRlp/S63Hazhhc+KSE+O45550/n+eRP45lOf8p+vbOe1Oy7o9pkxfaXB8AUzb+Zo5nU62C26KYMOj8d38Q5YB5RjiY2y+kDe3nGYK2Za7dlXnTWWv39WxI5DtSTGRhIX5SQqwsGfVufR2Obiv795OmDVPB6cP5N/nzuJ8Umx1Le4+J93ckiOj2J0YgxnT0jij6v2M+uhlbg9BqdDEOD2r0ziX9tL+NPqPC6dPpJ7r5jua6v1umFOKs+uy+c/Lp7CzRdMZMXuMl7edJBvnjmGWanDWJlTjttj2FRgXR18waQRDBtydGDA16aN5IOfzOWv6wq4wR4S2J0ZY4YyccQQIhzC327J8J3V/mjuJBwOQUQQgV9eeTpTUuKpbWn3HZB+fvk0rv/r57yWWcyCc1PJq2gk80CN7+zV34QRQ/jllafz33Z4Xp+Risvj4Yd/38Kvvnk6t345jdczi3ny4320uz1MGxUY6rFRTp68YRaPf5TLl+zRcklxUfzpe7P50YtbeT+rjFFDrWaztBHWGeaE4UO46fyJPLJiD1uKapg2KoG7Lp7CqKHRZNhNGZ19dVoKMZEOXthgHaT+al/DMXdqMpUNbTy9Jp/xSbF8/3yrFjt/1lieWLmP8yYNJ8Lp4LIZozm8oYinv3c2sVFOLpqazLXnjOeZtfmU1bWy8JIpvLvrMEOinFw8bSSxUU6uOmscb+8sYU76cKakxPPo+3vJKa3n1i+nMWPMUH7x5i5u/8okX1hOGRnvO6B6RTod/OaqM6htamfuaSnMSh2Gx2NIiI5g56FabpyTyrwzrL+boTERbCk6wgPLs0mMjSS/sonvnzeR9OQ4fnXl6cyeYO3fWy9MY0VWGT99bSctHW5unJPK12eM8n2mwyE88/1zuPrpz6jscHPbRemMTIjhN9+ewf3Ls1mTW8nsCcN45Ufn8+Nl2/mDfZXzLRdM9K1jysh4IhzCL+xWgN9f+yWcDmHiiDh+990zWfjydj7MLuNbXwo82ThuxphB/++cc84x6uTYX15v/rRqn/F4PL5pWSW15s6lmaal3WWMMWbVnjJz75u7zA/+ttGU17d0u67/W7nPPLs2z15vg7l58Sbz+w/2mhc3FJqH3sk2G/OrjDHGNLZ2mENHmnosV0Nrh69M7+06bB7/KNe43J6AeVo7XObdnYfN/vKG3m+4n8qGVtPc5urTsjcv3mQm3/eeOVjdZDbmV5mJ97xr3t99OOi8Ho/H3Lx4k/n642t929bY2hEwT2uHy6zLrTDtLnfIZfB4PKa2qd002OtqauswP/z7ZrP9YI1xuz3m+mc3mG88sc5UNbSGtL5/bS82b249ZNxuj9l1qNYs/qTAtLvcpry+xdy8eJNZs7c8YP5F6/JNZtERX/k7f0ea21zmsQ/2mKm/WmEm3vOumXjPu+Y/X97W7ef/Y2OR+dZTn/i259CRpoDvZ2/8Y2OR+cfGooDl1+VWmJsXbzLT/tsqz2m/WmEOVgf/PhbXNJtZD35ovv742m6/I+/tOmwyHl5pimuafdOW7ygxF/z2Y5NZVG2MsX5Hr2ceMl95bLXZVFAdsPyOgzXmlU0HzNLPi0xHp9/71gNH+rTdXkCmCXJMFTMI73/SWUZGhsnMzOzvYijVRVVjG4eONPsGBhw60sy4YbHdPgK23WUNB46Ncp6yMna4PThEfE0g/eVwbQur91aQfbiOmy9ICxgt1h88HkO724PI0ebVYCoaWomNdJLgVyMfLERkqzEmo8t0DQallApP3QVDSD0WIjJPRHJFJE9E7g3yfrSIvGq/v0lE0vzeu8+enisilx9rnSKSbq8jz15n16vGlFJKnTTHDAYRcQJPA1cAM4AbRWRGp9luA2qMMVOAJ4FH7WVnAAuAM4B5wF9ExHmMdT4KPGmvq8Zet1JKqVMklBrDHCDPGFNgjGkHlgHzO80zH1hiv34DuFSsgdrzgWXGmDZjTCGQZ68v6DrtZS6x14G9zqv7vHVKKaV6LZRgGAf4326y2J4WdB5jjAuoA0b0sGx300cAtfY6uvssAETkdhHJFJHMysrwelShUkqdTIP2lhjGmEXGmAxjTEZKSsqxF1BKKRWSUIKhBPC/Cmi8PS3oPCISASQC1T0s2930amCYvY7uPksppdRJFEowbAGm2qOForA6k5d3mmc5cIv9+lpgtX3xxHJggT1qKR2YCmzubp32MmvsdWCv8+2+b55SSqneOuYtMYwxLhFZCHwIOIHnjTHZIvIQ1lVzy4HFwFIRyQOOYB3osed7DcgBXMBdxhg3QLB12h95D7BMRB4GttvrVkopdYp8IS5wE5FKoK8PpE0GTt79a0+swVLWwVJOGDxlHSzlhMFT1sFSTjh5ZZ1ojOnSSfuFCIbjISKZwa78G4gGS1kHSzlh8JR1sJQTBk9ZB0s54dSXddCOSlJKKXVyaDAopZQKoMEAi/q7AL0wWMo6WMoJg6esg6WcMHjKOljKCae4rGHfx6CUUiqQ1hiUUkoF0GBQSikVIKyD4VjPmegvIpIqImtEJEdEskXkbnv6AyJSIiI77H9X9ndZAUSkSER222XKtKcNF5GVIrLf/j+pn8s4zW+/7RCRehH5yUDZpyLyvIhUiEiW37Sg+1AsT9nf210icnY/l/P3IrLXLss/RWSYPT1NRFr89u2zp6qcPZS12993d8+O6adyvupXxiIR2WFPPzX7NNjzPsPhH9YV1/nAJCAK2AnM6O9y2WUbA5xtv04A9mE9t+IB4Of9Xb4g5S0CkjtNewy41359L/Bof5ez0+++DJg4UPYp8BXgbCDrWPsQuBJ4HxDgfGBTP5fzG0CE/fpRv3Km+c83QPZp0N+3/fe1E4gG0u1jg7O/ytnp/ceB+0/lPg3nGkMoz5noF8aYUmPMNvt1A7CHbm4/PoD5P6NjoD1X41Ig3xjT16vlTzhjzHqs28n4624fzgdeNJaNWDeeHNNf5TTGfGSO3ip/I9bNL/tdN/u0O909O+ak66mc9jNqrgdeORVl8QrnYAjlORP9TqzHpM4GNtmTFtpV9uf7u3nGjwE+EpGtInK7PW2UMabUfl0GjOqfogW1gMA/tIG4T6H7fTiQv7v/hlWb8UoXke0isk5E5vZXoToJ9vseqPt0LlBujNnvN+2k79NwDoYBT0TigTeBnxhj6oFngMnALKAUq4o5EFxkjDkb61Gtd4nIV/zfNFYdeECMixbrbr5XAa/bkwbqPg0wkPZhd0TkV1g3y3zJnlQKTDDGzAZ+CrwsIkP7q3y2QfH79nMjgScxp2SfhnMwhPKciX4jIpFYofCSMeYtAGNMuTHGbYzxAM9xiqq6x2KMKbH/rwD+iVWucm/zhv1/Rf+VMMAVwDZjTDkM3H1q624fDrjvrojcCnwL+L4dYtjNMtX2661Y7fan9Vsh6fH3PRD3aQRwDfCqd9qp2qfhHAyhPGeiX9jtiouBPcaYJ/ym+7cjfwfI6rzsqSYicSKS4H2N1RGZReAzOgbSczUCzsAG4j71090+XA7cbI9OOh+o82tyOuVEZB7wC+AqY0yz3/QUEXHarydhPY+loH9K6StTd7/v7p4d05++Duw1xhR7J5yyfXoqet0H6j+s0R37sFL3V/1dHr9yXYTVbLAL2GH/uxJYCuy2py8HxgyAsk7CGs2xE8j27kes53evAvYDHwPDB0BZ47CeEpjoN21A7FOssCoFOrDat2/rbh9ijUZ62v7e7gYy+rmceVjt897v6rP2vN+1vxM7gG3AtwfAPu329w38yt6nucAV/VlOe/oLwJ2d5j0l+1RviaGUUipAODclKaWUCkKDQSmlVAANBqWUUgE0GJRSSgXQYFBKKRVAg0EppVQADQallFIB/j983rBmfiIwXgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(histories.values())[0].history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000102086204"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_test = y_test.reshape(-1,1)\n",
    "y_test_transform = scaler_y.transform(y_test)\n",
    "X_test = scaler_x.transform(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "mean_squared_error(y_test_transform, y_pred)\n",
    "scaler_y.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick = 'NVDA'\n",
    "import pandas_datareader as pdr\n",
    "def getTestData(ticker, start):\n",
    "    data = pdr.get_data_yahoo(ticker, start=start, end=today)\n",
    "    # dataname= ticker+\"_\"+str(today)\n",
    "    return data[-100:]\n",
    "    \n",
    "from datetime import date  \n",
    "today = date.today()\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "start = d = today - timedelta(days=190)\n",
    "\n",
    "df = getTestData(tick,start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['H-L'] = df['High'] - df['Low']\n",
    "df['O-C'] = df['Open'] - df['Close']\n",
    "df['7MA'] = df['Adj Close'].rolling(window=7).mean()\n",
    "df['14MA'] = df['Adj Close'].rolling(window=14).mean()\n",
    "df['21MA'] = df['Adj Close'].rolling(window=21).mean()\n",
    "df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "\n",
    "test_data = np.asarray(df[-1:][features], np.float32)\n",
    "test = scaler_x.transform(test_data)\n",
    "pred = model.predict(test)  \n",
    "pred_price = scaler_y.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate LSTM with more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import os\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from firebase_admin import storage\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas_ta as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histories = {}\n",
    "for tick in data.keys():\n",
    "    stock_data = data[tick]\n",
    "    stock_data = files[1][0]\n",
    "    df = pd.DataFrame(stock_data).T\n",
    "    df['H-L'] = df['High'] - df['Low']\n",
    "    df['O-C'] = df['Open'] - df['Close']\n",
    "    df['5MA'] = df['Adj Close'].rolling(window=5).mean()\n",
    "    df['10MA'] = df['Adj Close'].rolling(window=10).mean()\n",
    "    df['20MA'] = df['Adj Close'].rolling(window=20).mean()\n",
    "    df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "    df[\"EMA8\"] = df['Adj Close'].ewm(span=8).mean()\n",
    "    df[\"EMA21\"] = df['Adj Close'].ewm(span=21).mean()\n",
    "    df['EMA34'] = df['Adj Close'].ewm(span=34).mean()\n",
    "    df['EMA55'] = df['Adj Close'].ewm(span=55).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    df['Returns'] = df['Close'] / df['Close'].shift(1)\n",
    "    df['Returns'] -= 1\n",
    "    df.dropna(inplace=True)\n",
    "    df.ta.rsi(close='Close', length=14, append=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    features = ['Adj Close','H-L','O-C','5MA','10MA','20MA','7SD','RSI_14', 'EMA8','EMA21','EMA34','EMA55','Returns','Volume']\n",
    "    df = df[features].apply(pd.to_numeric)\n",
    "    train_data = int(0.9*len(df))\n",
    "    val_data = int(0.05*len(df))\n",
    "    train_df,val_df, test_df = df[1:train_data], df[train_data:-val_data], df[-val_data:]\n",
    "    sc = MinMaxScaler()\n",
    "\n",
    "    train = sc.fit_transform(train_df)\n",
    "    val = sc.transform(val_df)\n",
    "    test = sc.transform(test_df)\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_val = []\n",
    "    Y_val = []\n",
    "\n",
    "    timesteps = 50\n",
    "    hl = [40,35,50]\n",
    "    batch = 32\n",
    "    epochs = 8\n",
    "\n",
    "    # Loop for training data\n",
    "    for i in range(timesteps,train.shape[0]):\n",
    "        X_train.append(train[i-timesteps:i])\n",
    "        Y_train.append(train[i][0])\n",
    "    X_train,Y_train = np.array(X_train),np.array(Y_train)\n",
    "\n",
    "    # Loop for val data\n",
    "    for i in range(timesteps,val.shape[0]):\n",
    "        X_val.append(val[i-timesteps:i])\n",
    "        Y_val.append(val[i][0])\n",
    "    X_val,Y_val = np.array(X_val),np.array(Y_val)\n",
    "\n",
    "    # Adding Layers to the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(X_train.shape[2],input_shape = (X_train.shape[1],X_train.shape[2]),return_sequences = True,\n",
    "                    activation = 'relu'))\n",
    "    for i in range(len(hl)-1):        \n",
    "        model.add(LSTM(hl[i],activation = 'relu',return_sequences = True))\n",
    "    model.add(LSTM(hl[-1],activation = 'relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "    #print(model.summary())\n",
    "\n",
    "    # Training the data\n",
    "    history = model.fit(X_train,Y_train,epochs = epochs,batch_size = batch,validation_data = (X_val, Y_val),\n",
    "                        shuffle = False)\n",
    "    histories[tick] = history\n",
    "    model.reset_states()\n",
    "\n",
    "    filepath_model = \"../../../data/models/\" + tick + \"/multi_lstm\"\n",
    "    model.save(filepath_model)\n",
    "    file_paths = get_all_file_paths(filepath_model)\n",
    "\n",
    "    \n",
    "    #took help from this: https://www.geeksforgeeks.org/working-zip-files-python/\n",
    "    with ZipFile(filepath_model + \".zip\",'w') as zip:\n",
    "            # writing each file one by one\n",
    "            for file in file_paths:\n",
    "                zip.write(file)\n",
    "    \n",
    "    \n",
    "    fileName_model = \"multi_lstm.zip\"\n",
    "    bucket = storage.bucket()\n",
    "    #upload models\n",
    "    blob = bucket.blob(\"models/\" + tick + \"/\" + fileName_model)\n",
    "    blob.upload_from_filename(filepath_model+\".zip\")\n",
    "    \n",
    "    #upload normalizer training data\n",
    "    filepath_normalizer = \"../../../data/normalizers/\" + tick + \"/multi_lstm.pkl\"\n",
    "    pickle.dump(sc, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "    filename_normalizer = \"multi_lstm.pkl\"\n",
    "    blob = bucket.blob(\"normalizers/\" + tick + \"/multi_lstm.pkl\")\n",
    "    blob.upload_from_filename(filepath_normalizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "63/63 [==============================] - 80s 1s/step - loss: 0.0190 - val_loss: 0.0189\n",
      "Epoch 2/8\n",
      "63/63 [==============================] - 74s 1s/step - loss: 0.0352 - val_loss: 0.0195\n",
      "Epoch 3/8\n",
      "63/63 [==============================] - 74s 1s/step - loss: 3443707648.0000 - val_loss: 2202838.7500\n",
      "Epoch 4/8\n",
      "63/63 [==============================] - 74s 1s/step - loss: 0.0313 - val_loss: 0.4286\n",
      "Epoch 5/8\n",
      "33/63 [==============>...............] - ETA: 34s - loss: 0.0067"
     ]
    }
   ],
   "source": [
    "# histories = {}\n",
    "# for tick in data.keys():\n",
    "tick = 'AAPL'\n",
    "stock_data = data[tick]\n",
    "# df = files[4][0]\n",
    "df = pd.DataFrame(stock_data).T\n",
    "df['H-L'] = df['High'] - df['Low']\n",
    "df['O-C'] = df['Open'] - df['Close']\n",
    "df['5MA'] = df['Adj Close'].rolling(window=5).mean()\n",
    "df['10MA'] = df['Adj Close'].rolling(window=10).mean()\n",
    "df['20MA'] = df['Adj Close'].rolling(window=20).mean()\n",
    "df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "df[\"EMA8\"] = df['Adj Close'].ewm(span=8).mean()\n",
    "df[\"EMA21\"] = df['Adj Close'].ewm(span=21).mean()\n",
    "df['EMA34'] = df['Adj Close'].ewm(span=34).mean()\n",
    "df['EMA55'] = df['Adj Close'].ewm(span=55).mean()\n",
    "df.dropna(inplace=True)\n",
    "df['Returns'] = df['Close'] / df['Close'].shift(1)\n",
    "df['Returns'] -= 1\n",
    "df.dropna(inplace=True)\n",
    "df.ta.rsi(close='Close', length=14, append=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "features = ['Adj Close','H-L','O-C','5MA','10MA','20MA','7SD','RSI_14', 'EMA8','EMA21','EMA34','EMA55','Returns','Volume']\n",
    "df = df[features].apply(pd.to_numeric)\n",
    "train_data = int(0.9*len(df))\n",
    "val_data = int(0.05*len(df))\n",
    "train_df,val_df, test_df = df[1:train_data], df[train_data:-val_data], df[-val_data:]\n",
    "sc = MinMaxScaler()\n",
    "\n",
    "train = sc.fit_transform(train_df)\n",
    "val = sc.transform(val_df)\n",
    "test = sc.transform(test_df)\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_val = []\n",
    "Y_val = []\n",
    "\n",
    "timesteps = 50\n",
    "hl = [40,35,50]\n",
    "batch = 32\n",
    "epochs = 8\n",
    "\n",
    "# Loop for training data\n",
    "for i in range(timesteps,train.shape[0]):\n",
    "    X_train.append(train[i-timesteps:i])\n",
    "    Y_train.append(train[i][0])\n",
    "X_train,Y_train = np.array(X_train),np.array(Y_train)\n",
    "\n",
    "# Loop for val data\n",
    "for i in range(timesteps,val.shape[0]):\n",
    "    X_val.append(val[i-timesteps:i])\n",
    "    Y_val.append(val[i][0])\n",
    "X_val,Y_val = np.array(X_val),np.array(Y_val)\n",
    "\n",
    "# Adding Layers to the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(X_train.shape[2],input_shape = (X_train.shape[1],X_train.shape[2]),return_sequences = True,\n",
    "                activation = 'relu'))\n",
    "for i in range(len(hl)-1):        \n",
    "    model.add(LSTM(hl[i],activation = 'relu',return_sequences = True))\n",
    "model.add(LSTM(hl[-1],activation = 'relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "#print(model.summary())\n",
    "# Training the data\n",
    "history = model.fit(X_train,Y_train,epochs = epochs,batch_size = batch,validation_data = (X_val, Y_val),\n",
    "                    shuffle = False)\n",
    "histories['TSLA'] = history\n",
    "model.reset_states()\n",
    "\n",
    "filepath_model = \"../../../data/models/TSLA/multi_lstm\"\n",
    "model.save(filepath_model)\n",
    "file_paths = get_all_file_paths(filepath_model)\n",
    "\n",
    "\n",
    "#took help from this: https://www.geeksforgeeks.org/working-zip-files-python/\n",
    "with ZipFile(filepath_model + \".zip\",'w') as zip:\n",
    "        # writing each file one by one\n",
    "        for file in file_paths:\n",
    "            zip.write(file)\n",
    "\n",
    "\n",
    "fileName_model = \"multi_lstm.zip\"\n",
    "bucket = storage.bucket()\n",
    "#upload models\n",
    "blob = bucket.blob(\"models/TSLA/\" + fileName_model)\n",
    "blob.upload_from_filename(filepath_model+\".zip\")\n",
    "\n",
    "#upload normalizer training data\n",
    "filepath_normalizer = \"../../../data/normalizers/TSLA/multi_lstm.pkl\"\n",
    "pickle.dump(sc, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "filename_normalizer = \"multi_lstm.pkl\"\n",
    "blob = bucket.blob(\"normalizers/TSLA/multi_lstm.pkl\")\n",
    "blob.upload_from_filename(filepath_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TSLA'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "71/71 [==============================] - 21s 97ms/step - loss: 2.0676e-04 - val_loss: 1.0693\n",
      "Epoch 2/4\n",
      "71/71 [==============================] - 6s 79ms/step - loss: 1.6010e-04 - val_loss: 1.0449\n",
      "Epoch 3/4\n",
      "71/71 [==============================] - 6s 80ms/step - loss: 1.0193e-04 - val_loss: 0.1253\n",
      "Epoch 4/4\n",
      "71/71 [==============================] - 5s 75ms/step - loss: 2.4640e-05 - val_loss: 0.0335\n"
     ]
    }
   ],
   "source": [
    "#the model which worked better for multi lstm\n",
    "# histories = {}\n",
    "# for tick in data.keys():\n",
    "tick = 'TSLA'\n",
    "# stock_data = data[tick]\n",
    "df = files[3][0]\n",
    "# df = pd.DataFrame(stock_data).T\n",
    "df['H-L'] = df['High'] - df['Low']\n",
    "df['O-C'] = df['Open'] - df['Close']\n",
    "df['5MA'] = df['Adj Close'].rolling(window=5).mean()\n",
    "df['10MA'] = df['Adj Close'].rolling(window=10).mean()\n",
    "df['20MA'] = df['Adj Close'].rolling(window=20).mean()\n",
    "df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "df[\"EMA8\"] = df['Adj Close'].ewm(span=8).mean()\n",
    "df[\"EMA21\"] = df['Adj Close'].ewm(span=21).mean()\n",
    "df['EMA34'] = df['Adj Close'].ewm(span=34).mean()\n",
    "df['EMA55'] = df['Adj Close'].ewm(span=55).mean()\n",
    "df.dropna(inplace=True)\n",
    "df['Returns'] = df['Close'] / df['Close'].shift(1)\n",
    "df['Returns'] -= 1\n",
    "df.dropna(inplace=True)\n",
    "df.ta.rsi(close='Close', length=14, append=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "features = ['Adj Close','H-L','O-C','5MA','10MA','20MA','7SD','RSI_14', 'EMA8','EMA21','EMA34','EMA55','Returns','Volume']\n",
    "df = df[features].apply(pd.to_numeric)\n",
    "train_data = int(0.9*len(df))\n",
    "\n",
    "train_df,val_df = df[1:train_data], df[train_data:]\n",
    "sc_x = MinMaxScaler()\n",
    "sc_y = MinMaxScaler()\n",
    "\n",
    "\n",
    "features_x = ['H-L','O-C','5MA','10MA','20MA','7SD','RSI_14', 'EMA8','EMA21','EMA34','EMA55','Returns','Volume']\n",
    "X_tr = train_df[features_x]\n",
    "Y_tr = train_df['Adj Close']\n",
    "x_train_scaled = sc_x.fit_transform(X_tr)\n",
    "y_train_scaled = sc_y.fit_transform(np.array(Y_tr).reshape(-1, 1))\n",
    "\n",
    "X_ts = val_df[features_x]\n",
    "Y_ts = val_df['Adj Close']\n",
    "x_val_scaled = sc_x.transform(X_ts)\n",
    "y_val_scaled = sc_y.transform(np.array(Y_ts).reshape(-1, 1))\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_val = []\n",
    "Y_val = []\n",
    "\n",
    "timesteps = 7\n",
    "        # Loop for training data\n",
    "for i in range(timesteps,train.shape[0]):\n",
    "        \n",
    "    X_train.append(x_train_scaled[i-timesteps:i])\n",
    "    Y_train.append(y_train_scaled[i][0])\n",
    "        \n",
    "X_train,Y_train = np.array(X_train),np.array(Y_train)\n",
    "\n",
    "        # Loop for val data\n",
    "for i in range(timesteps,val.shape[0]):\n",
    "\n",
    "    X_val.append(x_val_scaled[i-timesteps:i])\n",
    "    Y_val.append(y_val_scaled[i][0])\n",
    "\n",
    "X_val,Y_val = np.array(X_val),np.array(Y_val)\n",
    "\n",
    "#start here again\n",
    "# val_data = int(0.05*len(df))\n",
    "# train_df,val_df, test_df = df[1:train_data], df[train_data:-val_data], df[-val_data:]\n",
    "# sc = MinMaxScaler()\n",
    "\n",
    "# train = sc.fit_transform(train_df)\n",
    "# val = sc.transform(val_df)\n",
    "# test = sc.transform(test_df)\n",
    "# X_train = []\n",
    "# Y_train = []\n",
    "# X_val = []\n",
    "# Y_val = []\n",
    "\n",
    "# Loop for training data\n",
    "# for i in range(timesteps,train.shape[0]):\n",
    "#     X_train.append(train[i-timesteps:i])\n",
    "#     Y_train.append(train[i][0])\n",
    "# X_train,Y_train = np.array(X_train),np.array(Y_train)\n",
    "\n",
    "# # Loop for val data\n",
    "# for i in range(timesteps,val.shape[0]):\n",
    "#     X_val.append(val[i-timesteps:i])\n",
    "#     Y_val.append(val[i][0])\n",
    "\n",
    "hl = [40,35,40,50]\n",
    "batch = 16\n",
    "epochs = 4\n",
    "\n",
    "\n",
    "# Adding Layers to the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(40,input_shape = (X_train.shape[1],X_train.shape[2]),return_sequences = True,\n",
    "                        activation = 'relu'))\n",
    "for i in range(len(hl)-1):        \n",
    "    model.add(LSTM(hl[i],activation = 'relu',return_sequences = True))\n",
    "model.add(LSTM(hl[-1],activation = 'relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "        #print(model.summary())\n",
    "\n",
    "        # Training the data\n",
    "history = model.fit(X_train,Y_train,epochs = epochs,batch_size = batch,validation_data = (X_val, Y_val))\n",
    "histories[tick] = history\n",
    "model.reset_states()\n",
    "\n",
    "# filepath_model = \"../../../data/models/MSFT/multi_lstm\"\n",
    "# model.save(filepath_model)\n",
    "# file_paths = get_all_file_paths(filepath_model)\n",
    "\n",
    "\n",
    "# #took help from this: https://www.geeksforgeeks.org/working-zip-files-python/\n",
    "# with ZipFile(filepath_model + \".zip\",'w') as zip:\n",
    "#         # writing each file one by one\n",
    "#         for file in file_paths:\n",
    "#             zip.write(file)\n",
    "\n",
    "\n",
    "# fileName_model = \"multi_lstm.zip\"\n",
    "# bucket = storage.bucket()\n",
    "# #upload models\n",
    "# blob = bucket.blob(\"models/MSFT/\" + fileName_model)\n",
    "# blob.upload_from_filename(filepath_model+\".zip\")\n",
    "\n",
    "# #upload normalizer training data\n",
    "# filepath_normalizer = \"../../../data/normalizers/MSFT/multi_lstm.pkl\"\n",
    "# pickle.dump(sc, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "# filename_normalizer = \"multi_lstm.pkl\"\n",
    "# blob = bucket.blob(\"normalizers/MSFT/multi_lstm.pkl\")\n",
    "# blob.upload_from_filename(filepath_normalizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tick = 'AAPL'\n",
    "filepath_model = f\"../../../data/models/{tick}/multi_lstm\"\n",
    "model.save(filepath_model)\n",
    "file_paths = get_all_file_paths(filepath_model)\n",
    "\n",
    "\n",
    "#took help from this: https://www.geeksforgeeks.org/working-zip-files-python/\n",
    "with ZipFile(filepath_model + \".zip\",'w') as zip:\n",
    "        # writing each file one by one\n",
    "        for file in file_paths:\n",
    "            zip.write(file)\n",
    "\n",
    "\n",
    "fileName_model = \"multi_lstm.zip\"\n",
    "bucket = storage.bucket()\n",
    "#upload models\n",
    "blob = bucket.blob(f\"models/{tick}/\" + fileName_model)\n",
    "blob.upload_from_filename(filepath_model+\".zip\")\n",
    "\n",
    "#upload normalizer training data\n",
    "filepath_normalizer = f\"../../../data/normalizers/{tick}/multi_lstm_x.pkl\"\n",
    "pickle.dump(sc_x, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "filename_normalizer = \"multi_lstm_x.pkl\"\n",
    "blob = bucket.blob(f\"normalizers/{tick}/multi_lstm_x.pkl\")\n",
    "blob.upload_from_filename(filepath_normalizer)\n",
    "\n",
    "#upload normalizer training data\n",
    "filepath_normalizer = f\"../../../data/normalizers/{tick}/multi_lstm_y.pkl\"\n",
    "pickle.dump(sc_y, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "filename_normalizer = \"multi_lstm_y.pkl\"\n",
    "blob = bucket.blob(f\"normalizers/{tick}/multi_lstm_y.pkl\")\n",
    "blob.upload_from_filename(filepath_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving plot images for the frontend\n",
    "def plot_error(train_loss,val_loss,file_path,tick):\n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.title(tick + ' loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(file_path)\n",
    "    plt.show()\n",
    "folder_path = '../../frontend/public/assets/images/multi_lstm/'\n",
    "for tick in histories.keys():\n",
    "    plot_error(histories[tick].history['loss'],histories[tick].history['val_loss'],folder_path + tick + '_v2_loss.jpg',tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data predictions\n",
    "tick = 'AMZN'\n",
    "import pandas_datareader as pdr\n",
    "def getTestData(ticker, start): \n",
    "    data = pdr.get_data_yahoo(ticker, start=start, end=today)\n",
    "    # dataname= ticker+\"_\"+str(today)\n",
    "    return data[-350:]\n",
    "    \n",
    "from datetime import date  \n",
    "today = date.today()\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "start = d = today - timedelta(days=500)\n",
    "\n",
    "df = getTestData(tick,start) \n",
    "\n",
    "df['H-L'] = df['High'] - df['Low']\n",
    "df['O-C'] = df['Open'] - df['Close']\n",
    "df['5MA'] = df['Adj Close'].rolling(window=5).mean()\n",
    "df['10MA'] = df['Adj Close'].rolling(window=10).mean()\n",
    "df['20MA'] = df['Adj Close'].rolling(window=20).mean()\n",
    "df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "df[\"EMA8\"] = df['Adj Close'].ewm(span=8).mean()\n",
    "df[\"EMA21\"] = df['Adj Close'].ewm(span=21).mean()\n",
    "df['EMA34'] = df['Adj Close'].ewm(span=34).mean()\n",
    "df['EMA55'] = df['Adj Close'].ewm(span=55).mean()\n",
    "df.dropna(inplace=True)\n",
    "df['Returns'] = df['Close'] / df['Close'].shift(1)\n",
    "df['Returns'] -= 1\n",
    "df.dropna(inplace=True)\n",
    "df.ta.rsi(close='Close', length=14, append=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "features = ['Adj Close','H-L','O-C','5MA','10MA','20MA','7SD','RSI_14', 'EMA8','EMA21','EMA34','EMA55','Returns','Volume']\n",
    "df = df[features].apply(pd.to_numeric)\n",
    "df = df[-50:]\n",
    "scaler_path = \"../../../data/normalizers/AMZN/multi_lstm.pkl\"\n",
    "with open(scaler_path, \"rb\") as input_file:\n",
    "    scaler = pickle.load(input_file)\n",
    "scaled_data = scaler.transform(df)\n",
    "scaled_data.shape\n",
    "scaled_data = scaled_data.reshape((1,50,14))\n",
    "sc_output = MinMaxScaler()\n",
    "\n",
    "#https://stackoverflow.com/questions/49330195/how-to-use-inverse-transform-in-minmaxscaler-for-a-column-in-a-matrix\n",
    "sc_output.min_ , sc_output.scale_ = scaler.min_[0], scaler.scale_[0]\n",
    "# test_data = np.asarray(scaled_data, np.float32)\n",
    "# pred = model.predict(test_data)  \n",
    "# pred_price = sc_output.inverse_transform(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df, title=\"Pandas Profiling Report\", explorative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [00:14<00:00, 15.69it/s, Completed]                    \n",
      "Generate report structure: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.05s/it]\n",
      "Render HTML: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.33s/it]\n",
      "Export report to file: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 91.43it/s]\n"
     ]
    }
   ],
   "source": [
    "profile.to_file(output_file='get_info.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "318it [00:23, 13.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVDA Profit using Custom ANN  64291.57669067383 316 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import pandas_datareader as pdr\n",
    "from datetime import date\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "tick = 'NVDA'\n",
    "scaler_path = \"../../../data/normalizers/\" + tick + \"/ann_x.pkl\"\n",
    "model_path = \"../../../data/models/\" + tick + \"/ann\"\n",
    "if(os.path.exists(model_path)):\n",
    "    model = load_model(model_path)\n",
    "if(os.path.exists(scaler_path)):\n",
    "    with open(scaler_path, \"rb\") as input_file:\n",
    "        scaler_x = pickle.load(input_file)\n",
    "\n",
    "scaler_path = \"../../../data/normalizers/\" + tick + \"/ann_y.pkl\"\n",
    "if(os.path.exists(scaler_path)):\n",
    "    with open(scaler_path, \"rb\") as input_file:\n",
    "        scaler_y = pickle.load(input_file)\n",
    "\n",
    "\n",
    "def getTestData(ticker, start = \"2020-11-30\"):\n",
    "    data = pdr.get_data_yahoo(ticker, start=start, end=today)\n",
    "    # dataname= ticker+\"_\"+str(today)\n",
    "    return data\n",
    "\n",
    "df = getTestData(tick)\n",
    "df['H-L'] = df['High'] - df['Low']\n",
    "df['O-C'] = df['Open'] - df['Close']\n",
    "df['7MA'] = df['Adj Close'].rolling(window=7).mean()\n",
    "df['14MA'] = df['Adj Close'].rolling(window=14).mean()\n",
    "df['21MA'] = df['Adj Close'].rolling(window=21).mean()\n",
    "df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "features = ['H-L','O-C','7MA','14MA','21MA','7SD','Volume']\n",
    "\n",
    "index = 20\n",
    "profit = 0\n",
    "rate = 0.05\n",
    "wins = 0\n",
    "loss = 0\n",
    "money = 50000\n",
    "\n",
    "for row in tqdm(df[20:-1][features].iterrows()):\n",
    "\n",
    "    present_close_price = df['Close'][index]\n",
    "    row = np.asarray(df[-1:][features], np.float32)\n",
    "    scaled_row = scaler_x.transform(row)\n",
    "    prediction = model.predict(scaled_row)  \n",
    "    prediction_value = scaler_y.inverse_transform(prediction)[0][0]\n",
    "\n",
    "    next_open_price = df['Open'][index + 1]\n",
    "    next_high_price = df['High'][index + 1]\n",
    "    next_close_price = df['Close'][index + 1]\n",
    "    next_low_price = df['Low'][index + 1]\n",
    "\n",
    "    if(prediction_value > present_close_price):\n",
    "    #if price is more, then long on stock\n",
    "\n",
    "        if next_open_price != next_high_price:\n",
    "        #if high price of the day is not same as open price, it means, the stock has gone up some time\n",
    "            difference = (next_high_price - next_open_price)*10\n",
    "            if money - next_open_price*10 < 0:\n",
    "                print('its lower')\n",
    "                continue\n",
    "            money += difference\n",
    "            profit += difference\n",
    "            profit -= rate*present_close_price\n",
    "            wins += 1\n",
    "        else:\n",
    "        #if high price of the day is same as open price, it means, the stock has gone less, so close position at the closing time of the day\n",
    "            difference = (next_close_price - next_open_price)*10\n",
    "            if money - next_open_price*10 < 0:\n",
    "                print('its lower')\n",
    "                continue\n",
    "            money += difference\n",
    "            profit += difference\n",
    "            profit -= rate*present_close_price\n",
    "            if(difference > 0):\n",
    "                wins += 1\n",
    "            else:\n",
    "                loss += 1\n",
    "    \n",
    "    else:\n",
    "    #if price is less, then short on stock\n",
    "\n",
    "        if next_open_price != next_low_price:\n",
    "        #if low price of the day is not same as open price, it means, the stock has gone less some time            \n",
    "            difference = (next_open_price - next_low_price)*10\n",
    "            if money - next_open_price*10 < 0:\n",
    "                print('its lower')\n",
    "                continue\n",
    "            money += difference\n",
    "            profit += difference\n",
    "            profit -= rate*present_close_price\n",
    "            wins += 1\n",
    "\n",
    "        else:\n",
    "        #if low price of the day is same as open price, it means, the stock has gone high, so close position at the closing time of the day\n",
    "            if money - next_open_price*10 < 0:\n",
    "                print('its lower')\n",
    "                continue\n",
    "            money += difference\n",
    "            profit += difference\n",
    "            profit -= rate*present_close_price\n",
    "            if(difference > 0):\n",
    "                wins += 1\n",
    "            else:\n",
    "                loss += 1\n",
    "            \n",
    "\n",
    "    index += 1\n",
    "\n",
    "print(f'{tick} Profit using Custom ANN ', money, wins, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVDA Profit using LSTM  10668.55964012146 315 3\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import pandas_datareader as pdr\n",
    "from datetime import date\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "tick = 'NVDA'\n",
    "scaler_path = \"../../../data/normalizers/\" + tick + \"/lstm.pkl\"\n",
    "model_path = \"../../../data/models/\" + tick + \"/lstm\"\n",
    "if(os.path.exists(model_path)):\n",
    "    model = load_model(model_path)\n",
    "if(os.path.exists(scaler_path)):\n",
    "    with open(scaler_path, \"rb\") as input_file:\n",
    "        scaler = pickle.load(input_file)\n",
    "\n",
    "def getTestData(ticker, start = \"2020-8-7\"):\n",
    "    data = pdr.get_data_yahoo(ticker, start=start, end=today)\n",
    "    # dataname= ticker+\"_\"+str(today)\n",
    "    return data\n",
    "\n",
    "df = getTestData(tick)\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "complete_data = df[['Close', 'Open', 'High', 'Low']]\n",
    "dataset = complete_data['Close']\n",
    "dataset = scaler.transform(np.array(dataset).reshape(-1,1))\n",
    "time_step = 100\n",
    "profit = 0\n",
    "rate = 0.05\n",
    "wins = 0\n",
    "loss = 0\n",
    "\n",
    "for i in range(len(dataset)-time_step-1):\n",
    "    present_close_price = complete_data.iloc[i+time_step-1]['Close']\n",
    "    next_open_price = complete_data.iloc[i+time_step]['Open']\n",
    "    next_close_price = complete_data.iloc[i+time_step]['Close']\n",
    "    next_high_price = complete_data.iloc[i+time_step]['High']\n",
    "    next_low_price = complete_data.iloc[i+time_step]['Low']\n",
    "    \n",
    "    data = dataset[i:(i+time_step), 0]\n",
    "    test_data = data.reshape(-1,1)\n",
    "    test_data = np.array([test_data,])\n",
    "    prediction = model.predict(test_data)\n",
    "    prediction_value = scaler.inverse_transform(prediction)[0][0]\n",
    "\n",
    "    if(prediction_value > present_close_price):\n",
    "    #if price is more, then long on stock\n",
    "\n",
    "        if next_open_price != next_high_price:\n",
    "        #if high price of the day is not same as open price, it means, the stock has gone up some time\n",
    "            difference = (next_high_price - next_open_price)*10\n",
    "            profit += difference\n",
    "            profit -= rate*present_close_price\n",
    "            wins += 1\n",
    "        else:\n",
    "\n",
    "        #if high price of the day is same as open price, it means, the stock has gone less, so close position at the closing time of the day\n",
    "            difference = (next_close_price - next_open_price)*10\n",
    "            profit += difference\n",
    "            profit -= rate*present_close_price\n",
    "            if(difference > 0):\n",
    "                wins += 1\n",
    "            else:\n",
    "                loss += 1\n",
    "    \n",
    "    else:\n",
    "    #if price is less, then short on stock\n",
    "\n",
    "        if next_open_price != next_low_price:\n",
    "        #if low price of the day is not same as open price, it means, the stock has gone less some time            \n",
    "            difference = (next_open_price - next_low_price)*10\n",
    "            profit += difference\n",
    "            profit -= rate*present_close_price\n",
    "            wins += 1\n",
    "\n",
    "        else:\n",
    "        #if low price of the day is same as open price, it means, the stock has gone high, so close position at the closing time of the day\n",
    "            difference = (next_open_price - next_close_price)*10\n",
    "            profit += difference\n",
    "            profit -= rate*present_close_price\n",
    "            if(difference > 0):\n",
    "                wins += 1\n",
    "            else:\n",
    "                loss += 1\n",
    "        \n",
    "\n",
    "print(f'{tick} Profit using LSTM ', profit, wins, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining NLP and Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import os\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from firebase_admin import storage\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas_ta as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = {}\n",
    "\n",
    "for tick in data.keys():\n",
    "    stock_data = data[tick]\n",
    "    # stock_data = files[1][0]\n",
    "    df = pd.DataFrame(stock_data).T\n",
    "    df['H-L'] = df['High'] - df['Low']\n",
    "    df['O-C'] = df['Open'] - df['Close']\n",
    "    df['5MA'] = df['Adj Close'].rolling(window=5).mean()\n",
    "    df['10MA'] = df['Adj Close'].rolling(window=10).mean()\n",
    "    df['20MA'] = df['Adj Close'].rolling(window=20).mean()\n",
    "    df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "    df[\"EMA8\"] = df['Adj Close'].ewm(span=8).mean()\n",
    "    df[\"EMA21\"] = df['Adj Close'].ewm(span=21).mean()\n",
    "    df['EMA34'] = df['Adj Close'].ewm(span=34).mean()\n",
    "    df['EMA55'] = df['Adj Close'].ewm(span=55).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    df['Returns'] = df['Close'] / df['Close'].shift(1)\n",
    "    df['Returns'] -= 1\n",
    "    df.dropna(inplace=True)\n",
    "    df.ta.rsi(close='Close', length=14, append=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    features = ['Adj Close','H-L','O-C','5MA','10MA','20MA','7SD','RSI_14', 'EMA8','EMA21','EMA34','EMA55','Returns','Volume']\n",
    "    df = df[features].apply(pd.to_numeric)\n",
    "\n",
    "    #make the index as a column and rename it to 'Date'\n",
    "    df.reset_index(inplace = True)\n",
    "    df.rename(columns = {'index': 'Date'}, inplace = True)\n",
    "\n",
    "    #read the news sentiment data given by Varun\n",
    "    news_sentiment = pd.read_csv(f'../../../data/sentiment data/{tick}_data.csv')\n",
    "    news_sentiment.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "    #merge on date\n",
    "    df_comb = pd.merge(news_sentiment, df, how='inner', on='Date')\n",
    "\n",
    "    #replace index to date again\n",
    "    df_comb.set_index('Date', inplace = True)\n",
    "\n",
    "    #drop the buy sell columns\n",
    "    df_comb.drop(['Buy', 'Sell'], inplace = True, axis = 1)\n",
    "\n",
    "    train_data = int(0.9*len(df_comb))\n",
    "\n",
    "    train_df,val_df = df_comb[1:train_data], df_comb[train_data:]\n",
    "    sc_x = MinMaxScaler()\n",
    "    sc_y = MinMaxScaler()\n",
    "\n",
    "\n",
    "    features_x = ['H-L','O-C','5MA','10MA','20MA','7SD','RSI_14', 'EMA8','EMA21','EMA34','EMA55','Returns','Volume', 'Vander_Score']\n",
    "    X_tr = train_df[features_x]\n",
    "    Y_tr = train_df['Adj Close']\n",
    "    x_train_scaled = sc_x.fit_transform(X_tr)\n",
    "    y_train_scaled = sc_y.fit_transform(np.array(Y_tr).reshape(-1, 1))\n",
    "\n",
    "    X_ts = val_df[features_x]\n",
    "    Y_ts = val_df['Adj Close']\n",
    "    x_val_scaled = sc_x.transform(X_ts)\n",
    "    y_val_scaled = sc_y.transform(np.array(Y_ts).reshape(-1, 1))\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_val = []\n",
    "    Y_val = []\n",
    "\n",
    "    timesteps = 7\n",
    "        # Loop for training data\n",
    "    for i in range(timesteps,train.shape[0]):\n",
    "        \n",
    "        X_train.append(x_train_scaled[i-timesteps:i])\n",
    "        Y_train.append(y_train_scaled[i][0])\n",
    "        \n",
    "    X_train,Y_train = np.array(X_train),np.array(Y_train)\n",
    "\n",
    "        # Loop for val data\n",
    "    for i in range(timesteps,val.shape[0]):\n",
    "\n",
    "        X_val.append(x_val_scaled[i-timesteps:i])\n",
    "        Y_val.append(y_val_scaled[i][0])\n",
    "\n",
    "    X_val,Y_val = np.array(X_val),np.array(Y_val)\n",
    "\n",
    "    hl = [40,35,50]\n",
    "    batch = 32\n",
    "    epochs = 15\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(X_train.shape[2],input_shape = (X_train.shape[1],X_train.shape[2]),return_sequences = True,\n",
    "                        activation = 'relu'))\n",
    "    for i in range(len(hl)-1):        \n",
    "        model.add(LSTM(hl[i],activation = 'relu',return_sequences = True))\n",
    "    model.add(LSTM(hl[-1],activation = 'relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "        #print(model.summary())\n",
    "\n",
    "        # Training the data\n",
    "    history = model.fit(X_train,Y_train,epochs = epochs,batch_size = batch,validation_data = (X_val, Y_val),\n",
    "                            shuffle = False)\n",
    "    \n",
    "    histories[tick] = history\n",
    "    model.reset_states()\n",
    "\n",
    "    filepath_model = \"../../../data/models/\" + tick + \"/combination_model1\"\n",
    "    model.save(filepath_model)\n",
    "    file_paths = get_all_file_paths(filepath_model)\n",
    "\n",
    "    \n",
    "    #took help from this: https://www.geeksforgeeks.org/working-zip-files-python/\n",
    "    with ZipFile(filepath_model + \".zip\",'w') as zip:\n",
    "            # writing each file one by one\n",
    "            for file in file_paths:\n",
    "                zip.write(file)\n",
    "    \n",
    "    \n",
    "    fileName_model = \"combination_model1.zip\"\n",
    "    bucket = storage.bucket()\n",
    "    #upload models\n",
    "    blob = bucket.blob(\"models/\" + tick + \"/\" + fileName_model)\n",
    "    blob.upload_from_filename(filepath_model+\".zip\")\n",
    "    \n",
    "    #upload normalizer features data\n",
    "    filepath_normalizer = \"../../../data/normalizers/\" + tick + \"/combination_model1_X.pkl\"\n",
    "    pickle.dump(sc_x, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "    filename_normalizer = \"combination_model1.pkl\"\n",
    "    blob = bucket.blob(\"normalizers/\" + tick + \"/combination_model1_X.pkl\")\n",
    "    blob.upload_from_filename(filepath_normalizer)\n",
    "\n",
    "    #upload normalizer labels data\n",
    "    filepath_normalizer = \"../../../data/normalizers/\" + tick + \"/combination_model1_Y.pkl\"\n",
    "    pickle.dump(sc_y, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "    filename_normalizer = \"combination_model1.pkl\"\n",
    "    blob = bucket.blob(\"normalizers/\" + tick + \"/combination_model1_Y.pkl\")\n",
    "    blob.upload_from_filename(filepath_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving plot images for the frontend\n",
    "def plot_error(train_loss,val_loss,file_path,tick):\n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.title(tick + ' loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(file_path)\n",
    "    plt.show()\n",
    "folder_path = '../../frontend/public/assets/images/combined_model1/'\n",
    "for tick in histories.keys():\n",
    "    plot_error(histories[tick].history['loss'],histories[tick].history['val_loss'],folder_path + tick + '_loss.jpg',tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca1edcbf756503ef84b6dec62a8a514ccc7037ff00e4b32f050febc64cc6ba90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('FYP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
