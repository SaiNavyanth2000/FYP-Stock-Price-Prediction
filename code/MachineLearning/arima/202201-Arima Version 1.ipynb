{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "from datetime import date\n",
    "import yfinance as yf\n",
    "yf.pdr_override()\n",
    "import pandas as pd\n",
    "\n",
    "ticker_list=[\"AAPL\", \"MSFT\", \"AMZN\", \"FB\", \"TSLA\", \"NVDA\", \"GOOG\", \"GOOGL\", \"CSCO\", \"AVGO\"]\n",
    "today = date.today()\n",
    "# We can get data by our choice by giving days bracket\n",
    "start_date= \"2013-01-01\"\n",
    "end_date=\"2020-11-30\"\n",
    "\n",
    "files=[]\n",
    "def getData(ticker):\n",
    "    data = pdr.get_data_yahoo(ticker, start=start_date, end=today)\n",
    "    # dataname= ticker+\"_\"+str(today)\n",
    "    files.append((data,ticker))\n",
    "    \n",
    "for tik in ticker_list:\n",
    "    getData(tik)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup firebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "\n",
    "cred_obj = firebase_admin.credentials.Certificate('../../../fyp2022-stockpriceprediction-firebase-adminsdk-ku62m-f9ed330292.json')\n",
    "fyp_app = firebase_admin.initialize_app(cred_obj, {\n",
    "\t'databaseURL':\"https://fyp2022-stockpriceprediction-default-rtdb.asia-southeast1.firebasedatabase.app/\",\n",
    "\t'storageBucket': 'fyp2022-stockpriceprediction.appspot.com'\n",
    "\t})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data into firebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firebase_admin import db\n",
    "from datetime import datetime as dt\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    tick = file[1]\n",
    "    data = file[0]\n",
    "    data['Ticker'] = tick\n",
    "    \n",
    "    #convert date index to string, as firebsae cant have datetime as an index\n",
    "    data.index = data.index.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    #convert the dataframe to json/dictionary\n",
    "    data_dict = data.to_dict(orient=\"index\")\n",
    "    \n",
    "    #upload it to the database\n",
    "    ref = db.reference(\"/data/\"+tick)\n",
    "    ref.set(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from firebase database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from firebase_admin import db\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "ref = db.reference(\"/data\")\n",
    "data = ref.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import math\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import os\n",
    "from firebase_admin import storage\n",
    "from sklearn.metrics import mean_squared_error\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def get_all_file_paths(directory):\n",
    "  \n",
    "    # initializing empty file paths list\n",
    "    file_paths = []\n",
    "  \n",
    "    # crawling through directory and subdirectories\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for fileName_model in files:\n",
    "            # join the two strings in order to form the full filepath.\n",
    "            filepath = os.path.join(root, fileName_model)\n",
    "            file_paths.append(filepath)\n",
    "  \n",
    "    # returning all file paths\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "  # Specify an invalid GPU device\n",
    "  #with tf.device('/device:GPU:2'):\n",
    "  \n",
    "  with tf.device('/device:GPU:2'):\n",
    "    for tick, stock_data in data.items():\n",
    "      print(tick)\n",
    "      df = pd.DataFrame(stock_data).T\n",
    "      df = df.reset_index()['Close']\n",
    "      scaler = MinMaxScaler(feature_range=(0,1))\n",
    "      df=scaler.fit_transform(np.array(df).reshape(-1,1))\n",
    "      train_data = df[:int(0.9*len(df))]\n",
    "      test_data = df[int(0.9*len(df)):]\n",
    "      train_data = train_data.reshape(-1,1)\n",
    "      test_data = test_data.reshape(-1,1)\n",
    "      \n",
    "      def create_dataset(dataset, time_step=1):\n",
    "          dataX, dataY = [], []\n",
    "          for i in range(len(dataset)-time_step-1):\n",
    "              a = dataset[i:(i+time_step), 0]\n",
    "              dataX.append(a)\n",
    "              dataY.append(dataset[i + time_step, 0])\n",
    "          return np.array(dataX), np.array(dataY)\n",
    "      time_step = 100\n",
    "      X_train, y_train = create_dataset(train_data, time_step)\n",
    "      X_test, ytest = create_dataset(test_data, time_step)\n",
    "      \n",
    "      X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "      X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "      \n",
    "      model=Sequential()\n",
    "      model.add(LSTM(50,return_sequences=True,input_shape=(100,1)))\n",
    "      model.add(LSTM(50,return_sequences=True))\n",
    "      model.add(LSTM(50))\n",
    "      model.add(Dense(1))\n",
    "      model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "      history = model.fit(X_train, y_train,epochs=10)\n",
    "      \n",
    "      \n",
    "      # calling function to get all file paths in the directory\n",
    "      filepath_model = \"../../../data/models/\" + tick + \"/lstm\"\n",
    "      model.save(filepath_model)\n",
    "      file_paths = get_all_file_paths(filepath_model)\n",
    "\n",
    "      \n",
    "      #took help from this: https://www.geeksforgeeks.org/working-zip-files-python/\n",
    "      with ZipFile(filepath_model + \".zip\",'w') as zip:\n",
    "              # writing each file one by one\n",
    "              for file in file_paths:\n",
    "                  zip.write(file)\n",
    "      \n",
    "      \n",
    "      fileName_model = \"lstm\"\n",
    "      bucket = storage.bucket()\n",
    "      #upload models\n",
    "      blob = bucket.blob(\"models/\" + tick + \"/\" + fileName_model)\n",
    "      blob.upload_from_filename(fileName_model+\".zip\")\n",
    "      \n",
    "      #upload normalizer\n",
    "      filepath_normalizer = \"../../../data/normalizers/\" + tick + \"/lstm.pkl\"\n",
    "      pickle.dump(scaler, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "      filename_normalizer = \"lstm.pkl\"\n",
    "      blob = bucket.blob(\"normalizers/\" + tick + \"/\" + filename_normalizer)\n",
    "      blob.upload_from_filename(filename_normalizer)\n",
    "      \n",
    "      # Opt : if you want to make public access from the URL\n",
    "      #blob.make_public()\n",
    "    # checking whether folder exists or not\n",
    "    folder_path = filepath_model\n",
    "    if os.path.exists(folder_path):\n",
    "\n",
    "        # checking whether the folder is empty or not\n",
    "        if len(os.listdir(folder_path)) == 0:\n",
    "            # removing the file using the os.remove() method\n",
    "            os.rmdir(folder_path)\n",
    "        else:\n",
    "            # messaging saying folder not empty\n",
    "            print(\"Folder is not empty\")\n",
    "    else:\n",
    "        # file not found message\n",
    "        print(\"File not found in the directory\")\n",
    "      \n",
    "except RuntimeError as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op OptionsDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_predict_function_291545 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_predict_function_291545 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_predict_function_291545 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_predict_function_291545 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ConcatV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.64311682215732"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict=model.predict(X_test)\n",
    "test_predict=scaler.inverse_transform(test_predict)\n",
    "\n",
    "ytest = scaler.inverse_transform(ytest.reshape(-1,1))\n",
    "error = math.sqrt(mean_squared_error(ytest,test_predict))\n",
    "error    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 100, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0307830607276087"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predict=model.predict(X_train)\n",
    "train_predict=scaler.inverse_transform(train_predict)\n",
    "y_train = scaler.inverse_transform(y_train.reshape(-1,1))\n",
    "math.sqrt(mean_squared_error(y_train,train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_predict)\n",
    "plt.plot(ytest)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                             Modified             Size\n",
      "AAPL_model/keras_metadata.pb                   2022-01-12 19:16:08        17068\n",
      "AAPL_model/saved_model.pb                      2022-01-12 19:16:08      2382689\n",
      "AAPL_model/variables/variables.data-00000-of-00001 2022-01-12 19:16:08       618372\n",
      "AAPL_model/variables/variables.index           2022-01-12 19:16:08         2731\n",
      "Extracting all the files now...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# specifying the zip file name\n",
    "file_name = \"AAPL_model.zip\"\n",
    "  \n",
    "# opening the zip file in READ mode\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "    # printing all the contents of the zip file\n",
    "    zip.printdir()\n",
    "  \n",
    "    # extracting all the files\n",
    "    print('Extracting all the files now...')\n",
    "    zip.extractall()\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Qr in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DiagPart in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sign in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Transpose in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ConcatV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Qr in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DiagPart in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sign in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Transpose in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ConcatV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Qr in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DiagPart in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sign in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Transpose in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ConcatV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op OptionsDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_predict_function_332981 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[174.37128]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the last 100 days and make a prediction\n",
    "tick = 'AAPL'\n",
    "def getTestData(ticker, start):\n",
    "    data = pdr.get_data_yahoo(ticker, start=start, end=today)\n",
    "    # dataname= ticker+\"_\"+str(today)\n",
    "    return data[-100:]\n",
    "    \n",
    "   \n",
    "   \n",
    "from datetime import datetime, timedelta\n",
    "start = d = today - timedelta(days=190)\n",
    "\n",
    "df = getTestData(tick,start) \n",
    "\n",
    "#df = pd.DataFrame(stock_data).T\n",
    "df = df.reset_index()['Close']\n",
    "df=scaler.transform(np.array(df).reshape(-1,1))\n",
    "test_data = df.reshape(-1,1)\n",
    "\n",
    "import keras.models\n",
    "model = keras.models.load_model(tick + '_model')\n",
    "prediction = model.predict( np.array( [test_data,] )  )\n",
    "scaler.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(1,1,1)(0,1,1)[12]             : AIC=inf, Time=1.47 sec\n",
      " ARIMA(0,1,0)(0,1,0)[12]             : AIC=8903.869, Time=0.05 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12]             : AIC=8357.856, Time=0.23 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12]             : AIC=inf, Time=0.92 sec\n",
      " ARIMA(1,1,0)(0,1,0)[12]             : AIC=8891.064, Time=0.08 sec\n",
      " ARIMA(1,1,0)(2,1,0)[12]             : AIC=8009.664, Time=0.47 sec\n",
      " ARIMA(1,1,0)(2,1,1)[12]             : AIC=inf, Time=2.58 sec\n",
      " ARIMA(1,1,0)(1,1,1)[12]             : AIC=inf, Time=1.20 sec\n",
      " ARIMA(0,1,0)(2,1,0)[12]             : AIC=8023.083, Time=0.39 sec\n",
      " ARIMA(2,1,0)(2,1,0)[12]             : AIC=8011.656, Time=0.56 sec\n",
      " ARIMA(1,1,1)(2,1,0)[12]             : AIC=8011.619, Time=1.13 sec\n",
      " ARIMA(0,1,1)(2,1,0)[12]             : AIC=8009.624, Time=0.50 sec\n",
      " ARIMA(0,1,1)(1,1,0)[12]             : AIC=8358.974, Time=0.24 sec\n",
      " ARIMA(0,1,1)(2,1,1)[12]             : AIC=inf, Time=2.62 sec\n",
      " ARIMA(0,1,1)(1,1,1)[12]             : AIC=inf, Time=1.39 sec\n",
      " ARIMA(0,1,2)(2,1,0)[12]             : AIC=8011.623, Time=0.53 sec\n",
      " ARIMA(1,1,2)(2,1,0)[12]             : AIC=8013.286, Time=1.57 sec\n",
      " ARIMA(0,1,1)(2,1,0)[12] intercept   : AIC=8011.601, Time=1.44 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,1)(2,1,0)[12]          \n",
      "Total fit time: 17.367 seconds\n"
     ]
    }
   ],
   "source": [
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "stock_data = data['AAPL']\n",
    "df = pd.DataFrame(stock_data).T\n",
    "\n",
    "data = df.sort_index(ascending=True, axis=0)\n",
    "\n",
    "train = data[:-2]\n",
    "valid = data[-2:]\n",
    "\n",
    "training = train['Close']\n",
    "validation = valid['Close']\n",
    "\n",
    "model = auto_arima(training, start_p=1, start_q=1,max_p=3, max_q=3, m=12,start_P=0, seasonal=True,d=1, D=1, trace=True,error_action='ignore',suppress_warnings=True)\n",
    "model.fit(training)\n",
    "\n",
    "forecast = model.predict(n_periods=len(valid))\n",
    "forecast = pd.DataFrame(forecast,index = valid.index,columns=['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(forecast['Prediction'],label='Prediction')\n",
    "plt.plot(valid['Close'], label='True value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SVOBBI~1\\AppData\\Local\\Temp/ipykernel_13036/2386101552.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#importing prophet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfbprophet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProphet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstock_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AAPL'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fbprophet'"
     ]
    }
   ],
   "source": [
    "#importing prophet\n",
    "from fbprophet import Prophet\n",
    "\n",
    "stock_data = data['AAPL']\n",
    "df = pd.DataFrame(stock_data).T\n",
    "\n",
    "#creating dataframe\n",
    "new_data = pd.DataFrame(index=range(0,len(df)),columns=['Date', 'Close'])\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    new_data['Date'][i] = data['Date'][i]\n",
    "    new_data['Close'][i] = data['Close'][i]\n",
    "\n",
    "new_data['Date'] = pd.to_datetime(new_data.Date,format='%Y-%m-%d')\n",
    "new_data.index = new_data['Date']\n",
    "\n",
    "#preparing data\n",
    "new_data.rename(columns={'Close': 'y', 'Date': 'ds'}, inplace=True)\n",
    "\n",
    "#train and validation\n",
    "train = new_data[:-10]\n",
    "valid = new_data[-10:]\n",
    "\n",
    "#fit the model\n",
    "model = Prophet()\n",
    "model.fit(train)\n",
    "\n",
    "#predictions\n",
    "close_prices = model.make_future_dataframe(periods=len(valid))\n",
    "forecast = model.predict(close_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom ANN (wih MA, H-L, O-C, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import os\n",
    "from firebase_admin import storage\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['AAPL', 'AMZN', 'AVGO', 'CSCO', 'FB', 'GOOG', 'GOOGL', 'MSFT', 'NVDA', 'TSLA'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0451 - val_loss: 0.0021\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0012 - val_loss: 5.7774e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 4.0813e-04 - val_loss: 3.1403e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.8470e-04 - val_loss: 3.3182e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.5519e-04 - val_loss: 3.3123e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3791e-04 - val_loss: 3.7756e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3366e-04 - val_loss: 2.8529e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3683e-04 - val_loss: 3.0682e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.2459e-04 - val_loss: 2.1216e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3477e-04 - val_loss: 2.4510e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2185e-04 - val_loss: 3.1171e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1951e-04 - val_loss: 2.0703e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2525e-04 - val_loss: 3.3924e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1597e-04 - val_loss: 3.0537e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.8945e-04 - val_loss: 2.9481e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3716e-04 - val_loss: 1.8793e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0067e-04 - val_loss: 1.9929e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7474e-04 - val_loss: 1.9666e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9410e-04 - val_loss: 3.1562e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1130e-04 - val_loss: 3.7873e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9882e-04 - val_loss: 1.8543e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7420e-04 - val_loss: 1.8414e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7006e-04 - val_loss: 1.6543e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9831e-04 - val_loss: 2.0185e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6846e-04 - val_loss: 1.7479e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7344e-04 - val_loss: 1.7386e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6735e-04 - val_loss: 1.7176e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8327e-04 - val_loss: 2.8062e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6419e-04 - val_loss: 1.5220e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0330e-04 - val_loss: 1.4388e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4823e-04 - val_loss: 1.5000e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4676e-04 - val_loss: 1.4104e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6322e-04 - val_loss: 1.8920e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6352e-04 - val_loss: 2.0523e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5811e-04 - val_loss: 1.5789e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6622e-04 - val_loss: 2.0116e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3568e-04 - val_loss: 1.5667e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7991e-04 - val_loss: 1.3142e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3526e-04 - val_loss: 2.0846e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0894e-04 - val_loss: 1.5597e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5088e-04 - val_loss: 2.1142e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6313e-04 - val_loss: 1.9425e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4378e-04 - val_loss: 1.5513e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6004e-04 - val_loss: 1.7909e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4959e-04 - val_loss: 1.2527e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3422e-04 - val_loss: 1.5726e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8785e-04 - val_loss: 2.4311e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6010e-04 - val_loss: 1.2029e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5729e-04 - val_loss: 1.8479e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5466e-04 - val_loss: 2.0713e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4950e-04 - val_loss: 1.6552e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4505e-04 - val_loss: 1.3223e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3129e-04 - val_loss: 1.1367e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4469e-04 - val_loss: 1.2529e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3729e-04 - val_loss: 1.6826e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3269e-04 - val_loss: 1.1731e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3890e-04 - val_loss: 1.1106e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6884e-04 - val_loss: 2.1047e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4324e-04 - val_loss: 1.2083e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3701e-04 - val_loss: 1.6853e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3114e-04 - val_loss: 1.6776e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2272e-04 - val_loss: 1.7909e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3335e-04 - val_loss: 1.5822e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3156e-04 - val_loss: 1.2463e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2752e-04 - val_loss: 1.7754e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4153e-04 - val_loss: 1.3223e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2055e-04 - val_loss: 1.4774e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2737e-04 - val_loss: 1.2037e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4550e-04 - val_loss: 1.5882e-04\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3847e-04 - val_loss: 1.0817e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5986e-04 - val_loss: 1.6070e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1711e-04 - val_loss: 1.3166e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3790e-04 - val_loss: 1.4092e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2656e-04 - val_loss: 1.2039e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4006e-04 - val_loss: 3.0946e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3165e-04 - val_loss: 2.2019e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2178e-04 - val_loss: 1.1632e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1914e-04 - val_loss: 1.1794e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1846e-04 - val_loss: 1.2982e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1531e-04 - val_loss: 1.2253e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5691e-04 - val_loss: 1.6075e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4778e-04 - val_loss: 2.3354e-04\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2865e-04 - val_loss: 1.2010e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3364e-04 - val_loss: 1.4019e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.1808e-04 - val_loss: 1.0845e-04\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2366e-04 - val_loss: 1.3873e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2457e-04 - val_loss: 1.0790e-04\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1711e-04 - val_loss: 1.1475e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5092e-04 - val_loss: 1.2963e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1627e-04 - val_loss: 4.0000e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7773e-04 - val_loss: 2.0804e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.1651e-04 - val_loss: 1.4102e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6746e-04 - val_loss: 1.9131e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3095e-04 - val_loss: 1.0959e-04\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2406e-04 - val_loss: 1.5747e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3333e-04 - val_loss: 1.1858e-04\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3780e-04 - val_loss: 1.1252e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2450e-04 - val_loss: 1.4652e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1467e-04 - val_loss: 1.0294e-04\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.1233e-04 - val_loss: 1.1205e-04\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1521e-04 - val_loss: 1.2692e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1730e-04 - val_loss: 1.2474e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1568e-04 - val_loss: 2.2614e-04\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2137e-04 - val_loss: 1.0435e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2822e-04 - val_loss: 1.2634e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3132e-04 - val_loss: 1.4323e-04\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4189e-04 - val_loss: 1.1476e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9500e-04 - val_loss: 1.3481e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1434e-04 - val_loss: 1.1974e-04\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4467e-04 - val_loss: 1.6041e-04\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3055e-04 - val_loss: 1.8726e-04\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1677e-04 - val_loss: 1.0367e-04\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2072e-04 - val_loss: 1.0760e-04\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2110e-04 - val_loss: 1.1283e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1115e-04 - val_loss: 1.5102e-04\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2313e-04 - val_loss: 2.8334e-04\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2912e-04 - val_loss: 1.6003e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2149e-04 - val_loss: 1.1853e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3844e-04 - val_loss: 1.3191e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3430e-04 - val_loss: 1.2871e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2856e-04 - val_loss: 1.8661e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4846e-04 - val_loss: 1.4043e-04\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4357e-04 - val_loss: 1.7641e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3056e-04 - val_loss: 1.2214e-04\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2866e-04 - val_loss: 1.0675e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1896e-04 - val_loss: 1.6859e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3043e-04 - val_loss: 2.4141e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2499e-04 - val_loss: 1.9234e-04\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3280e-04 - val_loss: 2.2855e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1968e-04 - val_loss: 1.1786e-04\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5395e-04 - val_loss: 1.5246e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3598e-04 - val_loss: 1.2953e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3104e-04 - val_loss: 1.2501e-04\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3022e-04 - val_loss: 2.1250e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2454e-04 - val_loss: 1.2187e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3390e-04 - val_loss: 3.1683e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2044e-04 - val_loss: 1.1361e-04\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2238e-04 - val_loss: 1.2886e-04\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3328e-04 - val_loss: 3.3854e-04\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3785e-04 - val_loss: 1.1469e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4258e-04 - val_loss: 1.7826e-04\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0821e-04 - val_loss: 1.1869e-04\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2050e-04 - val_loss: 1.1425e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3308e-04 - val_loss: 1.4770e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1558e-04 - val_loss: 1.2235e-04\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1507e-04 - val_loss: 1.2099e-04\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1134e-04 - val_loss: 1.2247e-04\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3649e-04 - val_loss: 1.5521e-04\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6771e-04 - val_loss: 3.0468e-04\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0450e-04 - val_loss: 1.3656e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2007e-04 - val_loss: 1.1532e-04\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1293e-04 - val_loss: 1.7096e-04\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1570e-04 - val_loss: 2.8709e-04\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0855e-04 - val_loss: 1.3296e-04\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2281e-04 - val_loss: 1.1924e-04\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4188e-04 - val_loss: 1.1740e-04\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2860e-04 - val_loss: 1.5476e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0917e-04 - val_loss: 1.3085e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2460e-04 - val_loss: 1.2582e-04\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2141e-04 - val_loss: 1.4371e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1349e-04 - val_loss: 1.0785e-04\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4177e-04 - val_loss: 1.2584e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2979e-04 - val_loss: 1.3304e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1336e-04 - val_loss: 1.1980e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2715e-04 - val_loss: 1.4032e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5044e-04 - val_loss: 1.2944e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3851e-04 - val_loss: 1.2333e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.9183e-05 - val_loss: 2.1079e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1805e-04 - val_loss: 1.8545e-04\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2167e-04 - val_loss: 1.4294e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1451e-04 - val_loss: 1.0514e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2038e-04 - val_loss: 1.1476e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4452e-04 - val_loss: 3.2860e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2676e-04 - val_loss: 1.9874e-04\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3200e-04 - val_loss: 1.3745e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1130e-04 - val_loss: 1.4326e-04\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2787e-04 - val_loss: 1.2661e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3473e-04 - val_loss: 1.2133e-04\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0941e-04 - val_loss: 1.1541e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2828e-04 - val_loss: 1.4019e-04\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/AAPL/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 0.0464 - val_loss: 0.0016\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 5.0943e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 4.2478e-04 - val_loss: 3.7257e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.0464e-04 - val_loss: 2.0826e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.5901e-04 - val_loss: 2.3735e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3511e-04 - val_loss: 1.8829e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3396e-04 - val_loss: 2.4207e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2449e-04 - val_loss: 2.5312e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2545e-04 - val_loss: 1.5746e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 2.0542e-04 - val_loss: 5.5657e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0767e-04 - val_loss: 1.6618e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9405e-04 - val_loss: 1.7730e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9902e-04 - val_loss: 2.4499e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8923e-04 - val_loss: 1.4337e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0459e-04 - val_loss: 3.0674e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3091e-04 - val_loss: 1.5687e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9873e-04 - val_loss: 1.6058e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9647e-04 - val_loss: 2.4142e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0579e-04 - val_loss: 1.6088e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9312e-04 - val_loss: 1.3474e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7829e-04 - val_loss: 1.8009e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1079e-04 - val_loss: 1.3315e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7929e-04 - val_loss: 1.9950e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9241e-04 - val_loss: 1.2378e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8365e-04 - val_loss: 1.1764e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8437e-04 - val_loss: 1.2482e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5725e-04 - val_loss: 1.3447e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9171e-04 - val_loss: 2.8147e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7853e-04 - val_loss: 1.2766e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8635e-04 - val_loss: 1.3024e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8034e-04 - val_loss: 1.5365e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9288e-04 - val_loss: 1.3566e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4700e-04 - val_loss: 1.7659e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7619e-04 - val_loss: 1.5561e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8659e-04 - val_loss: 1.0873e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6835e-04 - val_loss: 9.5502e-05\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4581e-04 - val_loss: 1.3558e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6209e-04 - val_loss: 1.3805e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.6213e-04 - val_loss: 1.0441e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6112e-04 - val_loss: 3.7806e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9344e-04 - val_loss: 1.1177e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4066e-04 - val_loss: 1.3620e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5263e-04 - val_loss: 1.3029e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8640e-04 - val_loss: 1.0107e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3389e-04 - val_loss: 1.0644e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4972e-04 - val_loss: 1.7084e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3816e-04 - val_loss: 1.5324e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6609e-04 - val_loss: 1.0202e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.3613e-04 - val_loss: 9.4083e-05\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4342e-04 - val_loss: 1.4131e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4305e-04 - val_loss: 9.3918e-05\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6464e-04 - val_loss: 1.6036e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3890e-04 - val_loss: 1.4627e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4253e-04 - val_loss: 1.1384e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3838e-04 - val_loss: 1.8656e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3332e-04 - val_loss: 9.7356e-05\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5744e-04 - val_loss: 1.1212e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5223e-04 - val_loss: 8.7707e-05\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3882e-04 - val_loss: 9.8304e-05\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2580e-04 - val_loss: 9.7238e-05\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2133e-04 - val_loss: 3.2790e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5331e-04 - val_loss: 9.4701e-05\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4460e-04 - val_loss: 1.0001e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.4518e-04 - val_loss: 1.6850e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5700e-04 - val_loss: 1.2242e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4748e-04 - val_loss: 1.4730e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4197e-04 - val_loss: 9.7605e-05\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3908e-04 - val_loss: 3.1774e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9783e-04 - val_loss: 8.0433e-05\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3511e-04 - val_loss: 1.8992e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2241e-04 - val_loss: 7.4739e-05\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1450e-04 - val_loss: 8.5937e-05\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1479e-04 - val_loss: 8.5812e-05\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2866e-04 - val_loss: 1.0345e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3666e-04 - val_loss: 8.2797e-05\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4325e-04 - val_loss: 8.4233e-05\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1489e-04 - val_loss: 8.7540e-05\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1479e-04 - val_loss: 1.0507e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6417e-04 - val_loss: 7.7299e-05\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2529e-04 - val_loss: 9.7706e-05\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2965e-04 - val_loss: 1.2010e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3692e-04 - val_loss: 2.3834e-04\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4537e-04 - val_loss: 1.4275e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2983e-04 - val_loss: 7.9237e-05\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2524e-04 - val_loss: 7.1294e-05\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1971e-04 - val_loss: 1.1243e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.6031e-04 - val_loss: 7.9503e-05\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1775e-04 - val_loss: 9.1120e-05\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.2885e-04 - val_loss: 7.7400e-05\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6421e-04 - val_loss: 8.6192e-05\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.4229e-04 - val_loss: 8.5198e-05\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1679e-04 - val_loss: 1.0008e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2255e-04 - val_loss: 1.0861e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1359e-04 - val_loss: 7.8222e-05\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1903e-04 - val_loss: 1.6727e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3819e-04 - val_loss: 9.4300e-05\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2834e-04 - val_loss: 1.0917e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2980e-04 - val_loss: 1.0769e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3670e-04 - val_loss: 8.8893e-05\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1928e-04 - val_loss: 8.2576e-05\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3697e-04 - val_loss: 9.3279e-05\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3033e-04 - val_loss: 1.0242e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2333e-04 - val_loss: 9.2097e-05\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3105e-04 - val_loss: 1.2057e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2740e-04 - val_loss: 2.4185e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3015e-04 - val_loss: 1.0460e-04\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3532e-04 - val_loss: 1.4821e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2394e-04 - val_loss: 6.7592e-05\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2145e-04 - val_loss: 6.6961e-05\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2991e-04 - val_loss: 8.4479e-05\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2597e-04 - val_loss: 7.7494e-05\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1936e-04 - val_loss: 1.1470e-04\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3089e-04 - val_loss: 9.6759e-05\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2961e-04 - val_loss: 7.2644e-05\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0874e-04 - val_loss: 2.0704e-04\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1981e-04 - val_loss: 1.0158e-04\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0949e-04 - val_loss: 1.0512e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2134e-04 - val_loss: 1.4862e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3026e-04 - val_loss: 8.3491e-05\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0975e-04 - val_loss: 1.2178e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1770e-04 - val_loss: 7.4356e-05\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1252e-04 - val_loss: 7.9548e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1064e-04 - val_loss: 9.1395e-05\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4128e-04 - val_loss: 7.9730e-05\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2666e-04 - val_loss: 7.4136e-05\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0865e-04 - val_loss: 1.1128e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5573e-04 - val_loss: 7.5872e-05\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0433e-04 - val_loss: 1.7646e-04\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3263e-04 - val_loss: 1.0336e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3503e-04 - val_loss: 7.5997e-05\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2149e-04 - val_loss: 9.2491e-05\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2273e-04 - val_loss: 9.7717e-05\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2479e-04 - val_loss: 7.4079e-05\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1847e-04 - val_loss: 7.5150e-05\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1389e-04 - val_loss: 1.0786e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1136e-04 - val_loss: 9.2623e-05\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1571e-04 - val_loss: 1.0330e-04\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3954e-04 - val_loss: 8.9200e-05\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1176e-04 - val_loss: 8.1885e-05\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3307e-04 - val_loss: 8.2998e-05\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0938e-04 - val_loss: 6.9754e-05\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1208e-04 - val_loss: 8.3442e-05\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0201e-04 - val_loss: 7.1902e-05\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1426e-04 - val_loss: 3.2240e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2335e-04 - val_loss: 7.8281e-05\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1015e-04 - val_loss: 1.3906e-04\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4963e-04 - val_loss: 7.7888e-05\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1992e-04 - val_loss: 1.4493e-04\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1079e-04 - val_loss: 7.7670e-05\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2279e-04 - val_loss: 3.1368e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2477e-04 - val_loss: 1.1189e-04\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0883e-04 - val_loss: 1.1835e-04\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3972e-04 - val_loss: 8.6961e-05\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1468e-04 - val_loss: 9.4674e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2208e-04 - val_loss: 9.1680e-05\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2203e-04 - val_loss: 9.0988e-05\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1175e-04 - val_loss: 1.0998e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1694e-04 - val_loss: 7.2871e-05\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3940e-04 - val_loss: 1.7116e-04\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3808e-04 - val_loss: 8.6487e-05\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1498e-04 - val_loss: 8.9323e-05\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1214e-04 - val_loss: 1.4926e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2461e-04 - val_loss: 7.3329e-05\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1019e-04 - val_loss: 6.7631e-05\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1118e-04 - val_loss: 8.3943e-05\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0002e-04 - val_loss: 8.6366e-05\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3973e-04 - val_loss: 1.2223e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2912e-04 - val_loss: 7.3260e-05\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1278e-04 - val_loss: 7.6481e-05\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0555e-04 - val_loss: 8.1172e-05\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1059e-04 - val_loss: 3.9704e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3581e-04 - val_loss: 1.1724e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1538e-04 - val_loss: 9.0872e-05\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0243e-04 - val_loss: 7.1187e-05\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0438e-04 - val_loss: 6.5026e-05\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0374e-04 - val_loss: 8.9231e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2827e-04 - val_loss: 6.7035e-05\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1511e-04 - val_loss: 8.9332e-05\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.9693e-05 - val_loss: 8.4645e-05\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1400e-04 - val_loss: 7.5871e-05\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/AMZN/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0442 - val_loss: 0.0021\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0013 - val_loss: 7.6483e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.0544e-04 - val_loss: 3.7004e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 3.2742e-04 - val_loss: 3.5844e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 2.8215e-04 - val_loss: 3.1734e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.6032e-04 - val_loss: 2.3449e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 2.3974e-04 - val_loss: 2.0278e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4904e-04 - val_loss: 1.8472e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2429e-04 - val_loss: 1.7670e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4035e-04 - val_loss: 1.7185e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2957e-04 - val_loss: 1.7010e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3337e-04 - val_loss: 3.8145e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4006e-04 - val_loss: 2.4680e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1342e-04 - val_loss: 1.8656e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4508e-04 - val_loss: 2.3917e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9810e-04 - val_loss: 1.5658e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0718e-04 - val_loss: 2.1045e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3598e-04 - val_loss: 1.7390e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1709e-04 - val_loss: 2.1189e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4685e-04 - val_loss: 1.9125e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0991e-04 - val_loss: 2.4393e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2441e-04 - val_loss: 1.6110e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1841e-04 - val_loss: 2.0651e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0554e-04 - val_loss: 1.9464e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3364e-04 - val_loss: 1.2878e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8886e-04 - val_loss: 2.2063e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1141e-04 - val_loss: 1.4591e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2083e-04 - val_loss: 1.5749e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9553e-04 - val_loss: 1.6426e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2011e-04 - val_loss: 1.8551e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7663e-04 - val_loss: 1.1961e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1884e-04 - val_loss: 1.5909e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7323e-04 - val_loss: 1.1582e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7031e-04 - val_loss: 1.0807e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8707e-04 - val_loss: 1.6620e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8701e-04 - val_loss: 1.1904e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0355e-04 - val_loss: 1.1774e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8083e-04 - val_loss: 1.3540e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6742e-04 - val_loss: 1.1915e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0606e-04 - val_loss: 1.4142e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7722e-04 - val_loss: 1.1204e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5880e-04 - val_loss: 1.4331e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6933e-04 - val_loss: 2.8318e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6158e-04 - val_loss: 1.6856e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7150e-04 - val_loss: 1.2057e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7334e-04 - val_loss: 1.4494e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7555e-04 - val_loss: 1.3268e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5176e-04 - val_loss: 1.0278e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7774e-04 - val_loss: 1.1281e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6930e-04 - val_loss: 8.8973e-05\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5414e-04 - val_loss: 9.0450e-05\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5823e-04 - val_loss: 9.6169e-05\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6441e-04 - val_loss: 2.4361e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6616e-04 - val_loss: 1.2625e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6942e-04 - val_loss: 1.4097e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7415e-04 - val_loss: 1.0116e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7709e-04 - val_loss: 9.2393e-05\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4473e-04 - val_loss: 1.2355e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4614e-04 - val_loss: 9.1722e-05\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.6657e-04 - val_loss: 9.2951e-05\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3405e-04 - val_loss: 1.4315e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5726e-04 - val_loss: 8.3500e-05\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5634e-04 - val_loss: 1.3008e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4678e-04 - val_loss: 8.3931e-05\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5312e-04 - val_loss: 7.8912e-05\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5041e-04 - val_loss: 2.0943e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7675e-04 - val_loss: 1.1793e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3819e-04 - val_loss: 7.4667e-05\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6872e-04 - val_loss: 1.4858e-04\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5397e-04 - val_loss: 2.8142e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6004e-04 - val_loss: 1.0314e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4476e-04 - val_loss: 1.1233e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6864e-04 - val_loss: 8.0632e-05\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5637e-04 - val_loss: 1.4138e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5740e-04 - val_loss: 2.7120e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5302e-04 - val_loss: 8.8621e-05\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6833e-04 - val_loss: 1.7197e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5112e-04 - val_loss: 8.8366e-05\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5322e-04 - val_loss: 1.2517e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6942e-04 - val_loss: 1.1213e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4836e-04 - val_loss: 9.5316e-05\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6566e-04 - val_loss: 7.6845e-05\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3134e-04 - val_loss: 9.4876e-05\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.3656e-04 - val_loss: 1.7583e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.2836e-04 - val_loss: 2.0191e-04\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.3870e-04 - val_loss: 1.7735e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.3943e-04 - val_loss: 7.1093e-05\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2911e-04 - val_loss: 9.3156e-05\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4394e-04 - val_loss: 8.9907e-05\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3133e-04 - val_loss: 1.0166e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7500e-04 - val_loss: 7.0825e-05\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9131e-04 - val_loss: 1.0801e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5536e-04 - val_loss: 1.4698e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3592e-04 - val_loss: 1.5106e-04\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3711e-04 - val_loss: 8.8335e-05\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3244e-04 - val_loss: 1.3861e-04\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4699e-04 - val_loss: 1.1560e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4750e-04 - val_loss: 6.8135e-05\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5860e-04 - val_loss: 1.7971e-04\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4322e-04 - val_loss: 9.5250e-05\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5497e-04 - val_loss: 1.1431e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4864e-04 - val_loss: 6.8731e-05\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7294e-04 - val_loss: 8.3617e-05\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3236e-04 - val_loss: 7.7682e-05\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5293e-04 - val_loss: 8.0158e-05\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4846e-04 - val_loss: 8.2151e-05\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2636e-04 - val_loss: 1.0718e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5313e-04 - val_loss: 1.2138e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4008e-04 - val_loss: 1.9292e-04\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4481e-04 - val_loss: 6.4587e-05\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5121e-04 - val_loss: 7.4210e-05\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3427e-04 - val_loss: 8.5268e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3397e-04 - val_loss: 2.5650e-04\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4109e-04 - val_loss: 9.7750e-05\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3552e-04 - val_loss: 9.4490e-05\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5343e-04 - val_loss: 1.1392e-04\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3361e-04 - val_loss: 1.3211e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4325e-04 - val_loss: 9.7081e-05\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4518e-04 - val_loss: 9.1639e-05\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2797e-04 - val_loss: 1.2994e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5269e-04 - val_loss: 1.4306e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3676e-04 - val_loss: 9.0167e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4379e-04 - val_loss: 9.3823e-05\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3578e-04 - val_loss: 9.0493e-05\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7748e-04 - val_loss: 1.0493e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5189e-04 - val_loss: 6.8811e-05\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7722e-04 - val_loss: 1.2492e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2376e-04 - val_loss: 7.2664e-05\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2273e-04 - val_loss: 8.9904e-05\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4353e-04 - val_loss: 1.4377e-04\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4443e-04 - val_loss: 2.2977e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2219e-04 - val_loss: 2.1565e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3473e-04 - val_loss: 8.5164e-05\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5003e-04 - val_loss: 7.7853e-05\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3524e-04 - val_loss: 6.6200e-05\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8150e-04 - val_loss: 7.6229e-05\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3910e-04 - val_loss: 7.6584e-05\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3488e-04 - val_loss: 7.1033e-05\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3983e-04 - val_loss: 7.6432e-05\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3880e-04 - val_loss: 1.1015e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2494e-04 - val_loss: 7.1211e-05\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7704e-04 - val_loss: 1.5304e-04\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4320e-04 - val_loss: 9.0085e-05\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3045e-04 - val_loss: 7.5340e-05\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7723e-04 - val_loss: 6.6395e-05\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4420e-04 - val_loss: 7.1385e-05\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5356e-04 - val_loss: 1.6832e-04\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3161e-04 - val_loss: 1.2911e-04\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4965e-04 - val_loss: 2.0425e-04\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4140e-04 - val_loss: 8.3447e-05\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5128e-04 - val_loss: 8.9356e-05\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3696e-04 - val_loss: 6.6934e-05\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3241e-04 - val_loss: 1.1928e-04\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3352e-04 - val_loss: 6.5859e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5364e-04 - val_loss: 8.6089e-05\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4890e-04 - val_loss: 7.5469e-05\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2204e-04 - val_loss: 8.9983e-05\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6091e-04 - val_loss: 1.0317e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4976e-04 - val_loss: 6.8364e-05\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5298e-04 - val_loss: 1.0000e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2484e-04 - val_loss: 8.0758e-05\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4481e-04 - val_loss: 1.5502e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4887e-04 - val_loss: 1.2572e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2387e-04 - val_loss: 1.0682e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4854e-04 - val_loss: 7.4614e-05\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3793e-04 - val_loss: 1.5701e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4258e-04 - val_loss: 1.0973e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4999e-04 - val_loss: 8.6254e-05\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4061e-04 - val_loss: 9.3215e-05\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3900e-04 - val_loss: 1.3188e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2906e-04 - val_loss: 1.0825e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4837e-04 - val_loss: 7.7533e-05\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4049e-04 - val_loss: 9.1438e-05\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5971e-04 - val_loss: 1.1722e-04\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2116e-04 - val_loss: 2.0159e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6237e-04 - val_loss: 8.2630e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3123e-04 - val_loss: 3.0656e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5586e-04 - val_loss: 1.1746e-04\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1770e-04 - val_loss: 1.1470e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2253e-04 - val_loss: 1.5439e-04\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/AVGO/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0425 - val_loss: 0.0023\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 7.3707e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 4.5405e-04 - val_loss: 5.8523e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 3.1283e-04 - val_loss: 3.9387e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.8735e-04 - val_loss: 3.4013e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.7553e-04 - val_loss: 2.3774e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4038e-04 - val_loss: 2.5086e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4421e-04 - val_loss: 3.9704e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3827e-04 - val_loss: 2.3731e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3605e-04 - val_loss: 2.1532e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1504e-04 - val_loss: 1.9512e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2434e-04 - val_loss: 1.9006e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1899e-04 - val_loss: 1.9844e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9502e-04 - val_loss: 1.6050e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1039e-04 - val_loss: 2.1472e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2496e-04 - val_loss: 2.3245e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1395e-04 - val_loss: 2.9013e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 2.4540e-04 - val_loss: 1.5034e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7919e-04 - val_loss: 1.5763e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1421e-04 - val_loss: 2.1349e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9609e-04 - val_loss: 1.5889e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0910e-04 - val_loss: 1.9917e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9509e-04 - val_loss: 2.2787e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9208e-04 - val_loss: 2.7199e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9932e-04 - val_loss: 1.6300e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7754e-04 - val_loss: 1.9776e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8617e-04 - val_loss: 1.4289e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9877e-04 - val_loss: 2.0456e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7386e-04 - val_loss: 1.3344e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6821e-04 - val_loss: 2.7671e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9900e-04 - val_loss: 1.4409e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5785e-04 - val_loss: 1.9745e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6885e-04 - val_loss: 1.3020e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1997e-04 - val_loss: 1.9696e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6197e-04 - val_loss: 1.7428e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8044e-04 - val_loss: 2.2190e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5483e-04 - val_loss: 1.6711e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7242e-04 - val_loss: 2.6550e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6761e-04 - val_loss: 1.6189e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6515e-04 - val_loss: 1.3088e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7202e-04 - val_loss: 1.2629e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5395e-04 - val_loss: 1.1240e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5736e-04 - val_loss: 1.8608e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3352e-04 - val_loss: 1.2155e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6631e-04 - val_loss: 1.2410e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5473e-04 - val_loss: 8.8692e-05\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6383e-04 - val_loss: 1.1914e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4063e-04 - val_loss: 2.0800e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4817e-04 - val_loss: 2.7409e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4283e-04 - val_loss: 9.4318e-05\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6290e-04 - val_loss: 1.2500e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5378e-04 - val_loss: 2.3354e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5083e-04 - val_loss: 2.1902e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4057e-04 - val_loss: 1.0600e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4028e-04 - val_loss: 8.2772e-05\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5985e-04 - val_loss: 1.0628e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7417e-04 - val_loss: 1.1264e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4291e-04 - val_loss: 1.1648e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4371e-04 - val_loss: 1.2732e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3902e-04 - val_loss: 9.8566e-05\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5154e-04 - val_loss: 1.4904e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4500e-04 - val_loss: 1.3780e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4221e-04 - val_loss: 1.0303e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2509e-04 - val_loss: 1.8593e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2617e-04 - val_loss: 1.3141e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2607e-04 - val_loss: 3.0744e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3167e-04 - val_loss: 9.6397e-05\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4445e-04 - val_loss: 1.0322e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2387e-04 - val_loss: 1.4972e-04\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.5346e-04 - val_loss: 8.7280e-05\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8712e-04 - val_loss: 1.1617e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3685e-04 - val_loss: 1.3456e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3818e-04 - val_loss: 1.2067e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4535e-04 - val_loss: 9.9606e-05\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3315e-04 - val_loss: 1.0434e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2957e-04 - val_loss: 1.1832e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2824e-04 - val_loss: 2.7565e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2469e-04 - val_loss: 3.2913e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4182e-04 - val_loss: 1.9713e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4282e-04 - val_loss: 8.9223e-05\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4214e-04 - val_loss: 9.6165e-05\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3339e-04 - val_loss: 1.2194e-04\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3487e-04 - val_loss: 1.5038e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2976e-04 - val_loss: 1.6502e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2356e-04 - val_loss: 1.1598e-04\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6550e-04 - val_loss: 2.0576e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4999e-04 - val_loss: 9.3974e-05\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4168e-04 - val_loss: 2.0765e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3880e-04 - val_loss: 1.1808e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3107e-04 - val_loss: 1.3130e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1659e-04 - val_loss: 1.2801e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4106e-04 - val_loss: 1.4101e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.4757e-04 - val_loss: 8.3153e-05\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.1688e-04 - val_loss: 1.8209e-04\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3648e-04 - val_loss: 1.2711e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3093e-04 - val_loss: 2.1025e-04\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6020e-04 - val_loss: 2.0668e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1837e-04 - val_loss: 1.0344e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5549e-04 - val_loss: 1.0047e-04\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2485e-04 - val_loss: 1.4443e-04\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1068e-04 - val_loss: 1.0174e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2594e-04 - val_loss: 9.0674e-05\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3150e-04 - val_loss: 7.3744e-05\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4392e-04 - val_loss: 1.1468e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2984e-04 - val_loss: 2.1510e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2322e-04 - val_loss: 1.6987e-04\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1944e-04 - val_loss: 1.1492e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6963e-04 - val_loss: 1.7238e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5758e-04 - val_loss: 3.6948e-04\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2111e-04 - val_loss: 1.2950e-04\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5640e-04 - val_loss: 1.7406e-04\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.3627e-04 - val_loss: 7.9175e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2126e-04 - val_loss: 9.7336e-05\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1977e-04 - val_loss: 7.7822e-05\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4097e-04 - val_loss: 9.8739e-05\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2132e-04 - val_loss: 1.1167e-04\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0403e-04 - val_loss: 9.9927e-05\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2332e-04 - val_loss: 1.1524e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2882e-04 - val_loss: 1.0025e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3305e-04 - val_loss: 3.9317e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3150e-04 - val_loss: 8.7463e-05\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3373e-04 - val_loss: 8.3057e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1430e-04 - val_loss: 8.1257e-05\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2873e-04 - val_loss: 1.6635e-04\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2424e-04 - val_loss: 1.2548e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3175e-04 - val_loss: 1.0226e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2984e-04 - val_loss: 2.0985e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3595e-04 - val_loss: 1.2036e-04\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3378e-04 - val_loss: 9.5935e-05\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1431e-04 - val_loss: 1.3899e-04\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.3514e-04 - val_loss: 1.0106e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3931e-04 - val_loss: 2.1140e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2872e-04 - val_loss: 1.0833e-04\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2490e-04 - val_loss: 1.3943e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2090e-04 - val_loss: 9.0184e-05\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4594e-04 - val_loss: 1.0079e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4037e-04 - val_loss: 1.3725e-04\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2915e-04 - val_loss: 9.1318e-05\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3741e-04 - val_loss: 8.0183e-05\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3495e-04 - val_loss: 8.1067e-05\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3933e-04 - val_loss: 9.2880e-05\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2519e-04 - val_loss: 7.2806e-05\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2502e-04 - val_loss: 9.3618e-05\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1963e-04 - val_loss: 1.1342e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2966e-04 - val_loss: 9.0648e-05\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1565e-04 - val_loss: 1.8056e-04\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2226e-04 - val_loss: 9.4564e-05\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3923e-04 - val_loss: 9.8307e-05\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1545e-04 - val_loss: 8.1855e-05\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3351e-04 - val_loss: 1.0479e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2437e-04 - val_loss: 9.4676e-05\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3010e-04 - val_loss: 8.2938e-05\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2598e-04 - val_loss: 1.0592e-04\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1386e-04 - val_loss: 1.5764e-04\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2550e-04 - val_loss: 1.3816e-04\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3787e-04 - val_loss: 7.5230e-05\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4216e-04 - val_loss: 1.2190e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2272e-04 - val_loss: 8.9060e-05\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1815e-04 - val_loss: 8.8550e-05\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2573e-04 - val_loss: 1.3208e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2525e-04 - val_loss: 2.5417e-04\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2105e-04 - val_loss: 8.6416e-05\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2616e-04 - val_loss: 1.1844e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1789e-04 - val_loss: 1.0599e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1409e-04 - val_loss: 8.8533e-05\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6921e-04 - val_loss: 1.2535e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1759e-04 - val_loss: 7.5782e-05\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1455e-04 - val_loss: 8.1998e-05\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1932e-04 - val_loss: 8.6119e-05\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4211e-04 - val_loss: 8.6018e-05\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1257e-04 - val_loss: 8.7101e-05\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4319e-04 - val_loss: 1.0774e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2417e-04 - val_loss: 1.0639e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2272e-04 - val_loss: 1.1794e-04\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1988e-04 - val_loss: 1.1317e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5491e-04 - val_loss: 2.0440e-04\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2722e-04 - val_loss: 1.2909e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1336e-04 - val_loss: 1.1689e-04\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2263e-04 - val_loss: 7.8259e-05\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4424e-04 - val_loss: 3.1689e-04\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/CSCO/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 0.0455 - val_loss: 0.0023\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0011 - val_loss: 5.8782e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 3.9810e-04 - val_loss: 3.8898e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.1325e-04 - val_loss: 3.6465e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.7809e-04 - val_loss: 3.8527e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.6808e-04 - val_loss: 3.8972e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.5280e-04 - val_loss: 3.0149e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.8544e-04 - val_loss: 3.3813e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2689e-04 - val_loss: 3.5415e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.6989e-04 - val_loss: 2.7416e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3415e-04 - val_loss: 2.6474e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0913e-04 - val_loss: 2.5647e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0125e-04 - val_loss: 7.6789e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.2192e-04 - val_loss: 3.0835e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9719e-04 - val_loss: 3.6844e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1042e-04 - val_loss: 3.2395e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9261e-04 - val_loss: 3.9712e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.9607e-04 - val_loss: 2.0667e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1037e-04 - val_loss: 2.4980e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8609e-04 - val_loss: 5.1933e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9175e-04 - val_loss: 1.9446e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9736e-04 - val_loss: 2.1526e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6729e-04 - val_loss: 1.9932e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1297e-04 - val_loss: 1.9443e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6966e-04 - val_loss: 2.5886e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9062e-04 - val_loss: 2.2864e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6375e-04 - val_loss: 2.2199e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6761e-04 - val_loss: 1.6474e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5558e-04 - val_loss: 1.8148e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6058e-04 - val_loss: 1.7766e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7042e-04 - val_loss: 2.7967e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6711e-04 - val_loss: 1.7461e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5746e-04 - val_loss: 1.5350e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6061e-04 - val_loss: 1.6682e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3863e-04 - val_loss: 1.5674e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5689e-04 - val_loss: 1.7323e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6764e-04 - val_loss: 2.0682e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5427e-04 - val_loss: 2.0555e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6177e-04 - val_loss: 3.2609e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6845e-04 - val_loss: 1.7216e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3891e-04 - val_loss: 4.1463e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6022e-04 - val_loss: 1.6705e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2185e-04 - val_loss: 1.6986e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6807e-04 - val_loss: 1.9347e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5135e-04 - val_loss: 1.5031e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5468e-04 - val_loss: 1.8014e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3678e-04 - val_loss: 2.0188e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5867e-04 - val_loss: 1.4991e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3153e-04 - val_loss: 1.7487e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3999e-04 - val_loss: 1.8436e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4308e-04 - val_loss: 1.9689e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3671e-04 - val_loss: 1.7726e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3118e-04 - val_loss: 1.5430e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2662e-04 - val_loss: 1.4867e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4764e-04 - val_loss: 1.9514e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4101e-04 - val_loss: 1.8473e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4332e-04 - val_loss: 1.5616e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3020e-04 - val_loss: 1.4652e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3570e-04 - val_loss: 2.0993e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4713e-04 - val_loss: 1.7307e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2449e-04 - val_loss: 2.0440e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3481e-04 - val_loss: 1.4117e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4747e-04 - val_loss: 1.4015e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3828e-04 - val_loss: 1.8856e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4301e-04 - val_loss: 1.7279e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2724e-04 - val_loss: 1.8522e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2980e-04 - val_loss: 2.1673e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3776e-04 - val_loss: 1.5111e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3100e-04 - val_loss: 1.5243e-04\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3344e-04 - val_loss: 1.8324e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2273e-04 - val_loss: 1.5153e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5436e-04 - val_loss: 1.3566e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2838e-04 - val_loss: 2.2490e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2759e-04 - val_loss: 2.0670e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7085e-04 - val_loss: 2.3774e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4252e-04 - val_loss: 1.8681e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2454e-04 - val_loss: 1.6571e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2457e-04 - val_loss: 1.7010e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6159e-04 - val_loss: 1.3827e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1811e-04 - val_loss: 1.3745e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1840e-04 - val_loss: 1.3880e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2704e-04 - val_loss: 1.3281e-04\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2738e-04 - val_loss: 1.6718e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4722e-04 - val_loss: 2.0619e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3365e-04 - val_loss: 1.4681e-04\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2630e-04 - val_loss: 1.5924e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3769e-04 - val_loss: 1.6118e-04\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3874e-04 - val_loss: 1.3929e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3549e-04 - val_loss: 1.6277e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1334e-04 - val_loss: 2.7725e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5189e-04 - val_loss: 1.7004e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2924e-04 - val_loss: 1.7138e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1903e-04 - val_loss: 1.9648e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4393e-04 - val_loss: 2.8961e-04\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6890e-04 - val_loss: 1.4592e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4039e-04 - val_loss: 1.6253e-04\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2157e-04 - val_loss: 1.4515e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3538e-04 - val_loss: 3.6111e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3719e-04 - val_loss: 1.6704e-04\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2129e-04 - val_loss: 2.3402e-04\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3900e-04 - val_loss: 3.1923e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4084e-04 - val_loss: 2.3354e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2831e-04 - val_loss: 1.2529e-04\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4465e-04 - val_loss: 2.1817e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2277e-04 - val_loss: 1.4738e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2075e-04 - val_loss: 1.3368e-04\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1769e-04 - val_loss: 2.3146e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4591e-04 - val_loss: 1.3714e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2057e-04 - val_loss: 1.4693e-04\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2256e-04 - val_loss: 1.8489e-04\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1894e-04 - val_loss: 1.6815e-04\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2161e-04 - val_loss: 1.2106e-04\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2629e-04 - val_loss: 1.7785e-04\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4908e-04 - val_loss: 2.1060e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2976e-04 - val_loss: 1.7342e-04\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2555e-04 - val_loss: 1.4271e-04\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2339e-04 - val_loss: 2.2018e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3112e-04 - val_loss: 1.4493e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1440e-04 - val_loss: 1.3301e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1212e-04 - val_loss: 1.9304e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3977e-04 - val_loss: 3.0198e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2828e-04 - val_loss: 1.4365e-04\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8517e-04 - val_loss: 1.4110e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1934e-04 - val_loss: 1.1986e-04\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0174e-04 - val_loss: 1.5313e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1380e-04 - val_loss: 1.3890e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3161e-04 - val_loss: 1.2724e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3490e-04 - val_loss: 1.4400e-04\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1002e-04 - val_loss: 1.8664e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1936e-04 - val_loss: 2.0133e-04\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2054e-04 - val_loss: 1.7218e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2387e-04 - val_loss: 1.5070e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1865e-04 - val_loss: 1.6698e-04\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1199e-04 - val_loss: 1.4198e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0477e-04 - val_loss: 1.4140e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2839e-04 - val_loss: 1.5634e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3697e-04 - val_loss: 2.6480e-04\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2031e-04 - val_loss: 2.1470e-04\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1248e-04 - val_loss: 1.3811e-04\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2278e-04 - val_loss: 1.9740e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2665e-04 - val_loss: 1.4034e-04\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0616e-04 - val_loss: 1.4444e-04\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2153e-04 - val_loss: 3.6458e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3364e-04 - val_loss: 1.5746e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1937e-04 - val_loss: 1.7935e-04\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2466e-04 - val_loss: 1.3211e-04\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2268e-04 - val_loss: 2.1656e-04\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2401e-04 - val_loss: 1.6332e-04\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3939e-04 - val_loss: 1.6860e-04\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4917e-04 - val_loss: 1.9405e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2308e-04 - val_loss: 1.2660e-04\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1341e-04 - val_loss: 2.0397e-04\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3762e-04 - val_loss: 1.2421e-04\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2196e-04 - val_loss: 1.2835e-04\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1990e-04 - val_loss: 1.4874e-04\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4092e-04 - val_loss: 2.2507e-04\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0508e-04 - val_loss: 1.5009e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1702e-04 - val_loss: 1.2764e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1942e-04 - val_loss: 1.2836e-04\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1679e-04 - val_loss: 1.3478e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0742e-04 - val_loss: 1.2766e-04\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1417e-04 - val_loss: 1.9644e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3721e-04 - val_loss: 1.6157e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2611e-04 - val_loss: 1.1852e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2047e-04 - val_loss: 1.9969e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2904e-04 - val_loss: 1.4601e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2862e-04 - val_loss: 1.4884e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2399e-04 - val_loss: 2.9586e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2981e-04 - val_loss: 1.5322e-04\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2404e-04 - val_loss: 1.2571e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2174e-04 - val_loss: 1.2404e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2729e-04 - val_loss: 1.9258e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1255e-04 - val_loss: 1.2281e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2034e-04 - val_loss: 1.1006e-04\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2438e-04 - val_loss: 1.4718e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1911e-04 - val_loss: 1.8347e-04\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4332e-04 - val_loss: 2.0517e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2119e-04 - val_loss: 1.3299e-04\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0999e-04 - val_loss: 1.3832e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2861e-04 - val_loss: 1.4185e-04\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/FB/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0401 - val_loss: 0.0029\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0010 - val_loss: 9.0659e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 4.4914e-04 - val_loss: 5.3012e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.2966e-04 - val_loss: 4.8980e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.8178e-04 - val_loss: 3.3898e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.5876e-04 - val_loss: 3.2526e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3750e-04 - val_loss: 2.5978e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2424e-04 - val_loss: 2.7787e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.4543e-04 - val_loss: 2.4468e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1703e-04 - val_loss: 2.2776e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2748e-04 - val_loss: 2.0613e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1741e-04 - val_loss: 2.6868e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4127e-04 - val_loss: 2.5200e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0212e-04 - val_loss: 2.9651e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2076e-04 - val_loss: 1.6742e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0296e-04 - val_loss: 1.7220e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.2174e-04 - val_loss: 2.1825e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9256e-04 - val_loss: 2.1839e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2290e-04 - val_loss: 1.5540e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0407e-04 - val_loss: 1.4951e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9334e-04 - val_loss: 1.4523e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2548e-04 - val_loss: 1.4751e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6484e-04 - val_loss: 1.6377e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8400e-04 - val_loss: 3.1446e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7318e-04 - val_loss: 1.3400e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1019e-04 - val_loss: 3.0439e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7158e-04 - val_loss: 1.5220e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6618e-04 - val_loss: 1.3343e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8248e-04 - val_loss: 1.8347e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6454e-04 - val_loss: 1.3993e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7739e-04 - val_loss: 2.4283e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7010e-04 - val_loss: 1.5534e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6038e-04 - val_loss: 1.2442e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5044e-04 - val_loss: 1.2401e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7871e-04 - val_loss: 1.2273e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5751e-04 - val_loss: 1.3921e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4876e-04 - val_loss: 1.6083e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6390e-04 - val_loss: 1.1048e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5863e-04 - val_loss: 1.6060e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4249e-04 - val_loss: 1.1330e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5403e-04 - val_loss: 1.2904e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4530e-04 - val_loss: 1.0990e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7906e-04 - val_loss: 1.2618e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4584e-04 - val_loss: 1.6995e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4844e-04 - val_loss: 1.1551e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7139e-04 - val_loss: 1.4291e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7393e-04 - val_loss: 1.0558e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3622e-04 - val_loss: 1.2737e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5518e-04 - val_loss: 1.2506e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5017e-04 - val_loss: 2.1425e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4756e-04 - val_loss: 1.4463e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3811e-04 - val_loss: 1.0035e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3030e-04 - val_loss: 1.1818e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.4068e-04 - val_loss: 1.1701e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5024e-04 - val_loss: 1.0655e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4400e-04 - val_loss: 1.5924e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6261e-04 - val_loss: 2.0306e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6453e-04 - val_loss: 1.9750e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.4677e-04 - val_loss: 1.1794e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.3926e-04 - val_loss: 1.2804e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3851e-04 - val_loss: 8.9785e-05\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4291e-04 - val_loss: 1.0578e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4058e-04 - val_loss: 1.5758e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7245e-04 - val_loss: 2.5507e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3401e-04 - val_loss: 1.2837e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4882e-04 - val_loss: 9.5155e-05\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.3303e-04 - val_loss: 1.0041e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4135e-04 - val_loss: 1.8080e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4371e-04 - val_loss: 9.8867e-05\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3263e-04 - val_loss: 9.6677e-05\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5767e-04 - val_loss: 9.3200e-05\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1410e-04 - val_loss: 1.2956e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3399e-04 - val_loss: 1.4489e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2387e-04 - val_loss: 1.7765e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4829e-04 - val_loss: 2.1460e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5921e-04 - val_loss: 1.9610e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2705e-04 - val_loss: 1.3716e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2454e-04 - val_loss: 9.3110e-05\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2381e-04 - val_loss: 1.1552e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1944e-04 - val_loss: 1.0538e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0910e-04 - val_loss: 1.0600e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2609e-04 - val_loss: 9.4888e-05\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6221e-04 - val_loss: 1.9984e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4414e-04 - val_loss: 2.4504e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5891e-04 - val_loss: 1.4603e-04\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3908e-04 - val_loss: 1.8097e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2547e-04 - val_loss: 1.6393e-04\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1625e-04 - val_loss: 8.9502e-05\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2713e-04 - val_loss: 1.2880e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4275e-04 - val_loss: 3.7210e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5502e-04 - val_loss: 9.7239e-05\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3620e-04 - val_loss: 1.0306e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2571e-04 - val_loss: 2.2761e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1935e-04 - val_loss: 8.7393e-05\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2200e-04 - val_loss: 2.7527e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3222e-04 - val_loss: 9.0579e-05\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3057e-04 - val_loss: 1.0547e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1666e-04 - val_loss: 1.7210e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1271e-04 - val_loss: 7.8378e-05\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3477e-04 - val_loss: 5.0558e-04\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6369e-04 - val_loss: 1.6114e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2919e-04 - val_loss: 1.4344e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2243e-04 - val_loss: 1.1127e-04\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3725e-04 - val_loss: 1.2759e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2105e-04 - val_loss: 1.1405e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1355e-04 - val_loss: 1.0634e-04\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1113e-04 - val_loss: 8.8771e-05\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3009e-04 - val_loss: 1.1821e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5682e-04 - val_loss: 9.0889e-05\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2657e-04 - val_loss: 1.2414e-04\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2100e-04 - val_loss: 1.4395e-04\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2451e-04 - val_loss: 1.1194e-04\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1197e-04 - val_loss: 8.7823e-05\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4558e-04 - val_loss: 2.1803e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1176e-04 - val_loss: 1.0249e-04\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1618e-04 - val_loss: 8.2096e-05\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1432e-04 - val_loss: 8.3421e-05\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3790e-04 - val_loss: 1.5620e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4343e-04 - val_loss: 9.5596e-05\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2390e-04 - val_loss: 1.2595e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3726e-04 - val_loss: 1.0439e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1504e-04 - val_loss: 1.0626e-04\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1268e-04 - val_loss: 1.2286e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1361e-04 - val_loss: 9.6592e-05\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2823e-04 - val_loss: 1.1153e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4467e-04 - val_loss: 1.1641e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2774e-04 - val_loss: 8.8141e-05\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3666e-04 - val_loss: 8.5940e-05\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2251e-04 - val_loss: 1.2839e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2883e-04 - val_loss: 9.5038e-05\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0917e-04 - val_loss: 9.1549e-05\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1241e-04 - val_loss: 2.6586e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1584e-04 - val_loss: 1.0286e-04\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3754e-04 - val_loss: 2.4393e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1358e-04 - val_loss: 1.3094e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2514e-04 - val_loss: 1.4573e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3043e-04 - val_loss: 9.1647e-05\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1793e-04 - val_loss: 8.9127e-05\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2806e-04 - val_loss: 1.7621e-04\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1363e-04 - val_loss: 1.0536e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1825e-04 - val_loss: 9.3547e-05\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3087e-04 - val_loss: 1.1651e-04\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4881e-04 - val_loss: 9.9343e-05\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1730e-04 - val_loss: 1.0017e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3475e-04 - val_loss: 1.3990e-04\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3568e-04 - val_loss: 1.4825e-04\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1453e-04 - val_loss: 1.6293e-04\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3548e-04 - val_loss: 1.0489e-04\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2752e-04 - val_loss: 1.1862e-04\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2284e-04 - val_loss: 2.0369e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3282e-04 - val_loss: 2.8923e-04\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3342e-04 - val_loss: 1.0196e-04\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0882e-04 - val_loss: 8.7551e-05\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1723e-04 - val_loss: 9.2500e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2494e-04 - val_loss: 8.4757e-05\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1261e-04 - val_loss: 9.1809e-05\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1751e-04 - val_loss: 1.1575e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2177e-04 - val_loss: 2.2858e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2667e-04 - val_loss: 9.5941e-05\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.3005e-04 - val_loss: 8.6844e-05\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1625e-04 - val_loss: 2.2689e-04\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2575e-04 - val_loss: 2.3849e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1733e-04 - val_loss: 1.7457e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1653e-04 - val_loss: 9.3255e-05\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0943e-04 - val_loss: 1.3370e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1945e-04 - val_loss: 1.0127e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1242e-04 - val_loss: 1.0179e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3455e-04 - val_loss: 1.0044e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1940e-04 - val_loss: 9.1278e-05\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4501e-04 - val_loss: 1.6659e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3160e-04 - val_loss: 1.5473e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3362e-04 - val_loss: 1.5712e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2650e-04 - val_loss: 9.7474e-05\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2852e-04 - val_loss: 9.4731e-05\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0058e-04 - val_loss: 1.0352e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1985e-04 - val_loss: 8.4368e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1324e-04 - val_loss: 1.0259e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1716e-04 - val_loss: 1.1009e-04\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3550e-04 - val_loss: 8.9023e-05\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1655e-04 - val_loss: 1.4428e-04\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/GOOG/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 0.0470 - val_loss: 0.0026\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 6.8300e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 4.4406e-04 - val_loss: 3.7069e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.6377e-04 - val_loss: 3.1770e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.2989e-04 - val_loss: 2.9870e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.7483e-04 - val_loss: 2.9831e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.2876e-04 - val_loss: 3.0266e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.8118e-04 - val_loss: 3.2173e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4227e-04 - val_loss: 2.1101e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4115e-04 - val_loss: 1.9626e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.4147e-04 - val_loss: 4.6994e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.5083e-04 - val_loss: 3.6258e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.2755e-04 - val_loss: 2.9383e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5173e-04 - val_loss: 4.6275e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.5356e-04 - val_loss: 2.4220e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1867e-04 - val_loss: 1.7682e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2581e-04 - val_loss: 2.6617e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0647e-04 - val_loss: 1.9786e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9686e-04 - val_loss: 1.7266e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9352e-04 - val_loss: 1.8154e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9506e-04 - val_loss: 3.1876e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0003e-04 - val_loss: 1.7091e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1351e-04 - val_loss: 1.6529e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2043e-04 - val_loss: 1.6208e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9449e-04 - val_loss: 1.8319e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2598e-04 - val_loss: 1.6869e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9088e-04 - val_loss: 1.9708e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7179e-04 - val_loss: 1.7678e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9939e-04 - val_loss: 2.5889e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7424e-04 - val_loss: 4.6530e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1801e-04 - val_loss: 3.0946e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7998e-04 - val_loss: 1.4278e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6582e-04 - val_loss: 1.9632e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6770e-04 - val_loss: 1.4814e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9515e-04 - val_loss: 4.4661e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5415e-04 - val_loss: 2.0093e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4371e-04 - val_loss: 1.6053e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9698e-04 - val_loss: 1.5182e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5986e-04 - val_loss: 1.3731e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6320e-04 - val_loss: 1.4293e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4610e-04 - val_loss: 2.3698e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7045e-04 - val_loss: 2.3530e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7383e-04 - val_loss: 1.6285e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6047e-04 - val_loss: 1.3192e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7874e-04 - val_loss: 4.6515e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0835e-04 - val_loss: 1.6428e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5523e-04 - val_loss: 1.5697e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5887e-04 - val_loss: 2.1368e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8009e-04 - val_loss: 2.9666e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6545e-04 - val_loss: 1.4730e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5065e-04 - val_loss: 1.5151e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5680e-04 - val_loss: 2.3515e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6164e-04 - val_loss: 1.3443e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4793e-04 - val_loss: 1.5158e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5254e-04 - val_loss: 3.5708e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3532e-04 - val_loss: 1.2739e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3630e-04 - val_loss: 1.2771e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4651e-04 - val_loss: 1.4315e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5484e-04 - val_loss: 3.6900e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8664e-04 - val_loss: 1.4142e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4147e-04 - val_loss: 1.5387e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5358e-04 - val_loss: 2.0888e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3977e-04 - val_loss: 1.5207e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4644e-04 - val_loss: 1.3186e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4605e-04 - val_loss: 1.2729e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2717e-04 - val_loss: 1.3514e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3722e-04 - val_loss: 1.7296e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3461e-04 - val_loss: 1.9776e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4702e-04 - val_loss: 1.5074e-04\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4497e-04 - val_loss: 1.1829e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1313e-04 - val_loss: 2.0043e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4830e-04 - val_loss: 1.2407e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4548e-04 - val_loss: 1.8436e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3761e-04 - val_loss: 1.2498e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3636e-04 - val_loss: 1.9927e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5335e-04 - val_loss: 1.5187e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2861e-04 - val_loss: 1.5869e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1943e-04 - val_loss: 1.3119e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3091e-04 - val_loss: 3.0379e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3838e-04 - val_loss: 1.2491e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3597e-04 - val_loss: 1.7813e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4692e-04 - val_loss: 1.7697e-04\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2452e-04 - val_loss: 1.7298e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4441e-04 - val_loss: 1.1880e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5223e-04 - val_loss: 1.1675e-04\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3978e-04 - val_loss: 1.9348e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.3506e-04 - val_loss: 1.4477e-04\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2370e-04 - val_loss: 1.3724e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3592e-04 - val_loss: 1.1903e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4435e-04 - val_loss: 1.1911e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3532e-04 - val_loss: 1.1695e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5604e-04 - val_loss: 1.5060e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2496e-04 - val_loss: 1.2342e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2781e-04 - val_loss: 1.5457e-04\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2736e-04 - val_loss: 1.9654e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2626e-04 - val_loss: 1.2545e-04\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4771e-04 - val_loss: 2.3552e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3701e-04 - val_loss: 1.8417e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2177e-04 - val_loss: 3.0353e-04\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4840e-04 - val_loss: 1.3400e-04\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4595e-04 - val_loss: 1.4450e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3496e-04 - val_loss: 1.4129e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3107e-04 - val_loss: 1.4721e-04\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2864e-04 - val_loss: 1.0898e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2026e-04 - val_loss: 1.7260e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3404e-04 - val_loss: 1.4183e-04\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2319e-04 - val_loss: 1.6877e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3281e-04 - val_loss: 1.4113e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3132e-04 - val_loss: 2.3562e-04\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1743e-04 - val_loss: 1.3445e-04\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5762e-04 - val_loss: 2.1324e-04\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2463e-04 - val_loss: 3.0665e-04\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3252e-04 - val_loss: 1.5941e-04\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4706e-04 - val_loss: 1.1565e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5411e-04 - val_loss: 1.7930e-04\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2565e-04 - val_loss: 1.1536e-04\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4331e-04 - val_loss: 1.1327e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1411e-04 - val_loss: 1.2731e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0354e-04 - val_loss: 2.1530e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1103e-04 - val_loss: 1.4261e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3948e-04 - val_loss: 1.1785e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3230e-04 - val_loss: 1.1388e-04\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2104e-04 - val_loss: 1.3314e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2885e-04 - val_loss: 1.8850e-04\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2833e-04 - val_loss: 1.3201e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2169e-04 - val_loss: 1.5063e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0706e-04 - val_loss: 1.1955e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1423e-04 - val_loss: 1.6531e-04\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1864e-04 - val_loss: 1.5985e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1682e-04 - val_loss: 1.1967e-04\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3101e-04 - val_loss: 1.1286e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4075e-04 - val_loss: 1.0261e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0895e-04 - val_loss: 1.1489e-04\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1946e-04 - val_loss: 1.3884e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1206e-04 - val_loss: 1.1517e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1384e-04 - val_loss: 1.4740e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6241e-04 - val_loss: 1.5135e-04\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4523e-04 - val_loss: 1.0774e-04\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2074e-04 - val_loss: 1.4200e-04\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0860e-04 - val_loss: 3.4476e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2983e-04 - val_loss: 1.7576e-04\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3026e-04 - val_loss: 1.2154e-04\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4277e-04 - val_loss: 9.8000e-05\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1592e-04 - val_loss: 1.4140e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1735e-04 - val_loss: 1.1744e-04\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1238e-04 - val_loss: 2.8120e-04\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1485e-04 - val_loss: 2.0268e-04\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2581e-04 - val_loss: 1.6535e-04\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1562e-04 - val_loss: 1.6338e-04\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1694e-04 - val_loss: 1.1130e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8326e-04 - val_loss: 1.0500e-04\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2712e-04 - val_loss: 1.2138e-04\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3620e-04 - val_loss: 1.0447e-04\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2368e-04 - val_loss: 1.1139e-04\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1197e-04 - val_loss: 1.3355e-04\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3937e-04 - val_loss: 1.9659e-04\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0685e-04 - val_loss: 1.0759e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2184e-04 - val_loss: 1.6302e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1033e-04 - val_loss: 1.3267e-04\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0420e-04 - val_loss: 1.2556e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1268e-04 - val_loss: 1.6669e-04\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2055e-04 - val_loss: 2.0060e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4175e-04 - val_loss: 1.4346e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1273e-04 - val_loss: 1.2293e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0937e-04 - val_loss: 1.2977e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2310e-04 - val_loss: 2.8531e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0690e-04 - val_loss: 1.0993e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1742e-04 - val_loss: 2.0060e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1116e-04 - val_loss: 1.4177e-04\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2925e-04 - val_loss: 1.0226e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3630e-04 - val_loss: 1.3381e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2599e-04 - val_loss: 1.1672e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2253e-04 - val_loss: 1.4551e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3400e-04 - val_loss: 1.0685e-04\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1733e-04 - val_loss: 1.4588e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4618e-04 - val_loss: 1.5053e-04\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1465e-04 - val_loss: 1.9196e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2452e-04 - val_loss: 1.0933e-04\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1307e-04 - val_loss: 1.0478e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0437e-04 - val_loss: 1.2310e-04\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/GOOGL/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0454 - val_loss: 0.0016\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 8.0040e-04 - val_loss: 5.4339e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.6133e-04 - val_loss: 4.1874e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.7913e-04 - val_loss: 3.6653e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4476e-04 - val_loss: 4.4872e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 2.4900e-04 - val_loss: 3.8566e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0505e-04 - val_loss: 3.7365e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1188e-04 - val_loss: 3.4124e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0213e-04 - val_loss: 3.2591e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1554e-04 - val_loss: 3.3536e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8724e-04 - val_loss: 3.5226e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9429e-04 - val_loss: 3.0382e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0696e-04 - val_loss: 3.4326e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0691e-04 - val_loss: 3.0100e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0629e-04 - val_loss: 2.8678e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8701e-04 - val_loss: 2.7244e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8132e-04 - val_loss: 3.1391e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8361e-04 - val_loss: 4.4398e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8614e-04 - val_loss: 4.6087e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7768e-04 - val_loss: 2.6514e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7647e-04 - val_loss: 3.8305e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6609e-04 - val_loss: 2.5488e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9702e-04 - val_loss: 2.7284e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6362e-04 - val_loss: 2.6144e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8539e-04 - val_loss: 2.7810e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0919e-04 - val_loss: 2.6750e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1471e-04 - val_loss: 2.5119e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6881e-04 - val_loss: 2.4493e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5030e-04 - val_loss: 2.4196e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6393e-04 - val_loss: 2.7263e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5347e-04 - val_loss: 2.9392e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4882e-04 - val_loss: 2.3081e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5305e-04 - val_loss: 2.6404e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3736e-04 - val_loss: 5.6794e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6470e-04 - val_loss: 2.4838e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5970e-04 - val_loss: 2.1633e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.5085e-04 - val_loss: 2.3478e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5377e-04 - val_loss: 2.7451e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2770e-04 - val_loss: 2.4310e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4745e-04 - val_loss: 2.3977e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3866e-04 - val_loss: 2.2543e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4903e-04 - val_loss: 2.4401e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5400e-04 - val_loss: 3.2383e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.5403e-04 - val_loss: 2.2439e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.4755e-04 - val_loss: 2.1311e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5419e-04 - val_loss: 3.0909e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4869e-04 - val_loss: 2.5047e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3100e-04 - val_loss: 3.0161e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4267e-04 - val_loss: 2.1416e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2593e-04 - val_loss: 2.6169e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4741e-04 - val_loss: 2.4513e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2781e-04 - val_loss: 2.3554e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5322e-04 - val_loss: 2.3285e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9916e-04 - val_loss: 2.2455e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4318e-04 - val_loss: 2.0082e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2447e-04 - val_loss: 1.9480e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2337e-04 - val_loss: 1.9243e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6079e-04 - val_loss: 2.2088e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4382e-04 - val_loss: 3.3710e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3935e-04 - val_loss: 2.0013e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2693e-04 - val_loss: 2.0633e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2056e-04 - val_loss: 1.9795e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3932e-04 - val_loss: 2.8032e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6176e-04 - val_loss: 1.8169e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3938e-04 - val_loss: 2.1598e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4686e-04 - val_loss: 2.1492e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.1893e-04 - val_loss: 2.0637e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3818e-04 - val_loss: 1.9499e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2137e-04 - val_loss: 3.3114e-04\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3333e-04 - val_loss: 1.7937e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3680e-04 - val_loss: 1.8114e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2616e-04 - val_loss: 2.1467e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3264e-04 - val_loss: 6.6160e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.5453e-04 - val_loss: 2.5221e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2159e-04 - val_loss: 2.1418e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3247e-04 - val_loss: 2.3190e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.3191e-04 - val_loss: 2.2754e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1150e-04 - val_loss: 2.4012e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2177e-04 - val_loss: 1.8208e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5565e-04 - val_loss: 4.2125e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1101e-04 - val_loss: 3.5666e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3473e-04 - val_loss: 3.6759e-04\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2992e-04 - val_loss: 2.1149e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1146e-04 - val_loss: 1.7500e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0116e-04 - val_loss: 1.9518e-04\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3502e-04 - val_loss: 1.7335e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1315e-04 - val_loss: 3.4718e-04\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2937e-04 - val_loss: 1.8806e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.4270e-04 - val_loss: 1.7515e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0826e-04 - val_loss: 1.9572e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1137e-04 - val_loss: 1.7243e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1821e-04 - val_loss: 2.3144e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2838e-04 - val_loss: 2.1486e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0921e-04 - val_loss: 6.0871e-04\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6810e-04 - val_loss: 1.7541e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3504e-04 - val_loss: 2.3315e-04\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2887e-04 - val_loss: 1.6970e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0438e-04 - val_loss: 2.0515e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0008e-04 - val_loss: 1.7787e-04\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1771e-04 - val_loss: 1.7182e-04\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1872e-04 - val_loss: 1.9781e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1685e-04 - val_loss: 1.7686e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1288e-04 - val_loss: 1.6672e-04\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2039e-04 - val_loss: 2.7666e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0942e-04 - val_loss: 1.6699e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2433e-04 - val_loss: 1.8284e-04\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0101e-04 - val_loss: 1.8671e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0354e-04 - val_loss: 1.7867e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1984e-04 - val_loss: 1.6975e-04\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2094e-04 - val_loss: 2.1028e-04\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0355e-04 - val_loss: 1.7185e-04\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2160e-04 - val_loss: 2.7686e-04\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1641e-04 - val_loss: 1.6495e-04\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0969e-04 - val_loss: 1.9386e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5646e-04 - val_loss: 1.7171e-04\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1347e-04 - val_loss: 2.3077e-04\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3495e-04 - val_loss: 2.4362e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4103e-04 - val_loss: 1.9182e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4349e-04 - val_loss: 1.6787e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2007e-04 - val_loss: 2.9398e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0867e-04 - val_loss: 1.8301e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1152e-04 - val_loss: 3.2736e-04\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1643e-04 - val_loss: 2.1465e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2455e-04 - val_loss: 2.2621e-04\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2852e-04 - val_loss: 1.5587e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0917e-04 - val_loss: 1.6306e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0091e-04 - val_loss: 1.9909e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1858e-04 - val_loss: 1.5523e-04\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2160e-04 - val_loss: 1.7201e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0380e-04 - val_loss: 3.0714e-04\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1048e-04 - val_loss: 1.7572e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4373e-04 - val_loss: 1.6015e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3587e-04 - val_loss: 2.1750e-04\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1840e-04 - val_loss: 1.8620e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0706e-04 - val_loss: 1.6072e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2022e-04 - val_loss: 2.1448e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1551e-04 - val_loss: 1.4982e-04\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0936e-04 - val_loss: 2.2007e-04\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2994e-04 - val_loss: 2.1061e-04\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1978e-04 - val_loss: 1.8306e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3054e-04 - val_loss: 1.5614e-04\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4255e-04 - val_loss: 2.0394e-04\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2569e-04 - val_loss: 1.6125e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1662e-04 - val_loss: 2.0471e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0792e-04 - val_loss: 1.8693e-04\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1492e-04 - val_loss: 1.6968e-04\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0650e-04 - val_loss: 2.3241e-04\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0905e-04 - val_loss: 1.5190e-04\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2198e-04 - val_loss: 1.5793e-04\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1915e-04 - val_loss: 2.0825e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1470e-04 - val_loss: 2.4937e-04\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2120e-04 - val_loss: 1.6534e-04\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1230e-04 - val_loss: 1.8684e-04\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1656e-04 - val_loss: 1.5480e-04\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2725e-04 - val_loss: 1.9517e-04\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0370e-04 - val_loss: 2.5176e-04\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2899e-04 - val_loss: 1.7405e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1899e-04 - val_loss: 1.7856e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0373e-04 - val_loss: 1.7464e-04\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4402e-04 - val_loss: 1.6380e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1742e-04 - val_loss: 1.9113e-04\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1073e-04 - val_loss: 1.7664e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0084e-04 - val_loss: 1.5413e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1446e-04 - val_loss: 1.8877e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0247e-04 - val_loss: 1.4711e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2475e-04 - val_loss: 2.7387e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4160e-04 - val_loss: 1.5169e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1444e-04 - val_loss: 2.1015e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3381e-04 - val_loss: 1.5857e-04\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0782e-04 - val_loss: 3.5525e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1957e-04 - val_loss: 1.6590e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0787e-04 - val_loss: 2.4247e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0248e-04 - val_loss: 1.6272e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0307e-04 - val_loss: 1.4406e-04\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1281e-04 - val_loss: 2.7832e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3213e-04 - val_loss: 1.5916e-04\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1043e-04 - val_loss: 1.7224e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1475e-04 - val_loss: 1.6794e-04\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1938e-04 - val_loss: 1.5207e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2911e-04 - val_loss: 1.7916e-04\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/MSFT/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0414 - val_loss: 0.0018\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 7.8532e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 4.5898e-04 - val_loss: 6.2853e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.2244e-04 - val_loss: 3.5444e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4734e-04 - val_loss: 3.2772e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 2.1683e-04 - val_loss: 2.7744e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.5381e-04 - val_loss: 2.9208e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1316e-04 - val_loss: 2.3863e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1410e-04 - val_loss: 2.1499e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5677e-04 - val_loss: 2.3112e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0098e-04 - val_loss: 2.3054e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8431e-04 - val_loss: 2.5592e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8809e-04 - val_loss: 3.0068e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8533e-04 - val_loss: 1.9579e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7922e-04 - val_loss: 2.4818e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7656e-04 - val_loss: 2.0248e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9227e-04 - val_loss: 2.5839e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6413e-04 - val_loss: 1.9799e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8880e-04 - val_loss: 1.9156e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5705e-04 - val_loss: 1.7370e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6677e-04 - val_loss: 1.9853e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7446e-04 - val_loss: 1.9684e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7106e-04 - val_loss: 2.4967e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1839e-04 - val_loss: 1.5923e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6413e-04 - val_loss: 1.6088e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6974e-04 - val_loss: 3.2172e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7625e-04 - val_loss: 2.3929e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6096e-04 - val_loss: 2.1899e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6953e-04 - val_loss: 2.0989e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7866e-04 - val_loss: 1.6324e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4171e-04 - val_loss: 2.0256e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5062e-04 - val_loss: 2.1175e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5869e-04 - val_loss: 1.7444e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6037e-04 - val_loss: 1.5234e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7415e-04 - val_loss: 1.5413e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5277e-04 - val_loss: 1.5463e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8984e-04 - val_loss: 1.7281e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5316e-04 - val_loss: 1.6021e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6067e-04 - val_loss: 2.3240e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8384e-04 - val_loss: 2.3343e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3571e-04 - val_loss: 1.4567e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6705e-04 - val_loss: 1.6871e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4349e-04 - val_loss: 2.1548e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4853e-04 - val_loss: 2.1753e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5322e-04 - val_loss: 1.3377e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3705e-04 - val_loss: 1.4960e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4058e-04 - val_loss: 1.8145e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4955e-04 - val_loss: 1.3920e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3329e-04 - val_loss: 1.7881e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4880e-04 - val_loss: 1.5321e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3474e-04 - val_loss: 1.8630e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3104e-04 - val_loss: 2.1018e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3328e-04 - val_loss: 1.2372e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1161e-04 - val_loss: 1.2507e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2936e-04 - val_loss: 5.4593e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4789e-04 - val_loss: 1.1528e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3113e-04 - val_loss: 3.2678e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7555e-04 - val_loss: 1.3512e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2891e-04 - val_loss: 2.0918e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4345e-04 - val_loss: 1.4902e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3592e-04 - val_loss: 1.7621e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4309e-04 - val_loss: 1.6903e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2265e-04 - val_loss: 1.7232e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4543e-04 - val_loss: 1.1150e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2841e-04 - val_loss: 1.8098e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2889e-04 - val_loss: 1.2025e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3573e-04 - val_loss: 1.2415e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1939e-04 - val_loss: 1.0843e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0787e-04 - val_loss: 2.5871e-04\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4282e-04 - val_loss: 1.1701e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1364e-04 - val_loss: 2.6173e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3200e-04 - val_loss: 1.1790e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1967e-04 - val_loss: 1.9192e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5341e-04 - val_loss: 1.3094e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1668e-04 - val_loss: 1.0490e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3803e-04 - val_loss: 1.1726e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3571e-04 - val_loss: 1.2395e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1990e-04 - val_loss: 1.3150e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2703e-04 - val_loss: 1.2517e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5666e-04 - val_loss: 1.1390e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1491e-04 - val_loss: 9.9879e-05\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2181e-04 - val_loss: 1.4849e-04\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3022e-04 - val_loss: 1.1003e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1790e-04 - val_loss: 1.1173e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.3920e-04 - val_loss: 9.6190e-05\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3301e-04 - val_loss: 1.2776e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2199e-04 - val_loss: 2.0505e-04\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5150e-04 - val_loss: 3.8260e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2965e-04 - val_loss: 1.2935e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2968e-04 - val_loss: 1.0983e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1060e-04 - val_loss: 1.6962e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4255e-04 - val_loss: 1.0147e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1388e-04 - val_loss: 1.2635e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2839e-04 - val_loss: 1.0393e-04\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2605e-04 - val_loss: 9.7376e-05\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1900e-04 - val_loss: 1.5590e-04\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2679e-04 - val_loss: 1.0403e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1661e-04 - val_loss: 9.1586e-05\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3945e-04 - val_loss: 1.6358e-04\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2030e-04 - val_loss: 9.9144e-05\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2467e-04 - val_loss: 1.0450e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1871e-04 - val_loss: 1.2189e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2620e-04 - val_loss: 1.1629e-04\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1355e-04 - val_loss: 1.1196e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1209e-04 - val_loss: 1.0532e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4688e-04 - val_loss: 9.3440e-05\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1886e-04 - val_loss: 1.1155e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2641e-04 - val_loss: 2.0143e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3513e-04 - val_loss: 2.0853e-04\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2314e-04 - val_loss: 1.0459e-04\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3343e-04 - val_loss: 1.0013e-04\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3096e-04 - val_loss: 9.7514e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2695e-04 - val_loss: 1.2914e-04\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0813e-04 - val_loss: 1.1767e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1264e-04 - val_loss: 1.6327e-04\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1824e-04 - val_loss: 9.6645e-05\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1768e-04 - val_loss: 1.0329e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2208e-04 - val_loss: 1.1281e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1420e-04 - val_loss: 9.0045e-05\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1248e-04 - val_loss: 9.8210e-05\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3109e-04 - val_loss: 2.1159e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0971e-04 - val_loss: 1.1753e-04\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2584e-04 - val_loss: 1.2755e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.3150e-04 - val_loss: 9.1367e-05\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1962e-04 - val_loss: 1.1585e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1369e-04 - val_loss: 1.4524e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0819e-04 - val_loss: 9.4807e-05\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1230e-04 - val_loss: 1.0483e-04\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1462e-04 - val_loss: 1.1261e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0326e-04 - val_loss: 1.1707e-04\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2859e-04 - val_loss: 1.1789e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1966e-04 - val_loss: 1.0398e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4013e-04 - val_loss: 9.6543e-05\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0892e-04 - val_loss: 1.5524e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0384e-04 - val_loss: 2.4895e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6151e-04 - val_loss: 9.5759e-05\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1622e-04 - val_loss: 1.7031e-04\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2334e-04 - val_loss: 1.6537e-04\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2533e-04 - val_loss: 1.5729e-04\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4554e-04 - val_loss: 2.5173e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3389e-04 - val_loss: 1.1176e-04\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2383e-04 - val_loss: 1.4311e-04\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2186e-04 - val_loss: 1.5053e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2410e-04 - val_loss: 1.4096e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3097e-04 - val_loss: 9.8962e-05\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1312e-04 - val_loss: 1.4932e-04\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2597e-04 - val_loss: 1.3764e-04\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1806e-04 - val_loss: 8.9696e-05\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0881e-04 - val_loss: 1.2799e-04\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1911e-04 - val_loss: 1.0228e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0983e-04 - val_loss: 1.8372e-04\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0987e-04 - val_loss: 1.6777e-04\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1130e-04 - val_loss: 8.7834e-05\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1405e-04 - val_loss: 9.0543e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2864e-04 - val_loss: 1.3369e-04\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1598e-04 - val_loss: 1.4502e-04\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1514e-04 - val_loss: 1.1641e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2862e-04 - val_loss: 1.1296e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3025e-04 - val_loss: 1.7326e-04\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0786e-04 - val_loss: 1.3690e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2008e-04 - val_loss: 2.7220e-04\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1404e-04 - val_loss: 9.3953e-05\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - ETA: 0s - loss: 1.0923e-0 - 1s 3ms/step - loss: 1.0777e-04 - val_loss: 2.0320e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0536e-04 - val_loss: 1.5173e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0315e-04 - val_loss: 1.0563e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5280e-04 - val_loss: 9.4198e-05\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3440e-04 - val_loss: 1.2195e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2011e-04 - val_loss: 1.0618e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1220e-04 - val_loss: 1.8554e-04\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0261e-04 - val_loss: 1.2211e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.6703e-05 - val_loss: 1.3548e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2063e-04 - val_loss: 1.7022e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1337e-04 - val_loss: 1.0390e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1232e-04 - val_loss: 1.9698e-04\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.7482e-05 - val_loss: 1.4280e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0842e-04 - val_loss: 2.0734e-04\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3526e-04 - val_loss: 1.0515e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0887e-04 - val_loss: 9.6011e-05\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0370e-04 - val_loss: 1.4297e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0787e-04 - val_loss: 9.2822e-05\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/NVDA/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0373 - val_loss: 0.0014\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 8.1912e-04 - val_loss: 5.4328e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 4.2618e-04 - val_loss: 3.5307e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.1570e-04 - val_loss: 3.2745e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.8681e-04 - val_loss: 2.9399e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.5295e-04 - val_loss: 2.8717e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.5601e-04 - val_loss: 2.3802e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4279e-04 - val_loss: 2.6125e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4829e-04 - val_loss: 2.4241e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.5686e-04 - val_loss: 5.0419e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4510e-04 - val_loss: 4.9900e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5832e-04 - val_loss: 2.3847e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4488e-04 - val_loss: 2.6663e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0550e-04 - val_loss: 2.4057e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1318e-04 - val_loss: 2.1103e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3121e-04 - val_loss: 2.1120e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3091e-04 - val_loss: 2.8154e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8819e-04 - val_loss: 1.9865e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9296e-04 - val_loss: 1.8658e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1475e-04 - val_loss: 2.1210e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7970e-04 - val_loss: 1.9293e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0313e-04 - val_loss: 2.4861e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2072e-04 - val_loss: 3.0642e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2381e-04 - val_loss: 2.5636e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9703e-04 - val_loss: 2.0814e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4275e-04 - val_loss: 2.1732e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9686e-04 - val_loss: 1.9149e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7370e-04 - val_loss: 3.3633e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8901e-04 - val_loss: 1.6185e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5800e-04 - val_loss: 2.2680e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8045e-04 - val_loss: 1.6308e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8772e-04 - val_loss: 4.3853e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8528e-04 - val_loss: 1.5782e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8985e-04 - val_loss: 2.5439e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8150e-04 - val_loss: 1.6134e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7149e-04 - val_loss: 2.5581e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7151e-04 - val_loss: 1.5775e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9144e-04 - val_loss: 1.9690e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9331e-04 - val_loss: 1.4724e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4993e-04 - val_loss: 1.5045e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5000e-04 - val_loss: 1.4699e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7302e-04 - val_loss: 2.5440e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6497e-04 - val_loss: 2.8294e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6706e-04 - val_loss: 1.4247e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5772e-04 - val_loss: 1.6663e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3588e-04 - val_loss: 1.3453e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8637e-04 - val_loss: 1.6110e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5798e-04 - val_loss: 1.4226e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4025e-04 - val_loss: 1.4482e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3469e-04 - val_loss: 2.7254e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4341e-04 - val_loss: 1.6363e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7821e-04 - val_loss: 1.3895e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3725e-04 - val_loss: 1.7342e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5707e-04 - val_loss: 1.3236e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4262e-04 - val_loss: 1.4259e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4444e-04 - val_loss: 1.3423e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4961e-04 - val_loss: 1.4890e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3498e-04 - val_loss: 1.1839e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6222e-04 - val_loss: 1.6021e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5386e-04 - val_loss: 1.3637e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3871e-04 - val_loss: 1.3623e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4267e-04 - val_loss: 1.3661e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0323e-04 - val_loss: 1.3547e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3902e-04 - val_loss: 1.4815e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5648e-04 - val_loss: 1.8537e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6049e-04 - val_loss: 1.3136e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5307e-04 - val_loss: 2.7397e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4145e-04 - val_loss: 1.3133e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4702e-04 - val_loss: 1.4016e-04\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2348e-04 - val_loss: 1.3616e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3571e-04 - val_loss: 1.3592e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3739e-04 - val_loss: 1.3230e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1889e-04 - val_loss: 1.1479e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4363e-04 - val_loss: 1.6773e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5270e-04 - val_loss: 1.3664e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4571e-04 - val_loss: 2.0196e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2545e-04 - val_loss: 1.3438e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3607e-04 - val_loss: 1.3229e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2449e-04 - val_loss: 1.2505e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3219e-04 - val_loss: 4.2804e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4535e-04 - val_loss: 1.5109e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2492e-04 - val_loss: 1.2590e-04\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2600e-04 - val_loss: 1.2727e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4116e-04 - val_loss: 1.6456e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5432e-04 - val_loss: 1.1871e-04\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3606e-04 - val_loss: 1.4570e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2684e-04 - val_loss: 1.2046e-04\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3186e-04 - val_loss: 1.3017e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3621e-04 - val_loss: 2.7672e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3682e-04 - val_loss: 2.0226e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2390e-04 - val_loss: 1.5980e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4415e-04 - val_loss: 1.3083e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4751e-04 - val_loss: 1.7100e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3482e-04 - val_loss: 1.5452e-04\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2683e-04 - val_loss: 1.2422e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3342e-04 - val_loss: 1.8781e-04\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5390e-04 - val_loss: 1.1378e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2077e-04 - val_loss: 2.0431e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7938e-04 - val_loss: 1.2467e-04\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2363e-04 - val_loss: 1.5478e-04\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3038e-04 - val_loss: 1.1506e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2203e-04 - val_loss: 1.8102e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2308e-04 - val_loss: 1.1907e-04\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5171e-04 - val_loss: 1.2348e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3163e-04 - val_loss: 1.5047e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1339e-04 - val_loss: 1.1896e-04\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3174e-04 - val_loss: 1.2599e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3969e-04 - val_loss: 1.9049e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.4608e-04 - val_loss: 1.4117e-04\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1905e-04 - val_loss: 1.3475e-04\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7151e-04 - val_loss: 1.6641e-04\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1728e-04 - val_loss: 1.3810e-04\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3373e-04 - val_loss: 1.3702e-04\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2759e-04 - val_loss: 2.0887e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3642e-04 - val_loss: 1.5248e-04\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1404e-04 - val_loss: 1.2124e-04\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2985e-04 - val_loss: 1.1146e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4920e-04 - val_loss: 1.0842e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3655e-04 - val_loss: 1.2888e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3458e-04 - val_loss: 1.6032e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2051e-04 - val_loss: 2.1337e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5325e-04 - val_loss: 1.1003e-04\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1944e-04 - val_loss: 1.6524e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4248e-04 - val_loss: 1.3576e-04\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2925e-04 - val_loss: 1.6768e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2398e-04 - val_loss: 2.2791e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2275e-04 - val_loss: 1.8724e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3934e-04 - val_loss: 1.2063e-04\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2926e-04 - val_loss: 1.2170e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2951e-04 - val_loss: 1.6073e-04\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3325e-04 - val_loss: 1.7225e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3361e-04 - val_loss: 1.1620e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3432e-04 - val_loss: 1.2197e-04\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2145e-04 - val_loss: 1.2715e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1633e-04 - val_loss: 1.1703e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2830e-04 - val_loss: 2.1107e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2099e-04 - val_loss: 1.1517e-04\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2631e-04 - val_loss: 1.2988e-04\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2556e-04 - val_loss: 1.2045e-04\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2004e-04 - val_loss: 1.3751e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3636e-04 - val_loss: 1.4978e-04\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1749e-04 - val_loss: 1.1872e-04\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3440e-04 - val_loss: 1.2208e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3589e-04 - val_loss: 1.2334e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2163e-04 - val_loss: 1.4050e-04\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8582e-04 - val_loss: 3.3227e-04\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3295e-04 - val_loss: 3.5843e-04\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4831e-04 - val_loss: 1.1882e-04\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4649e-04 - val_loss: 1.3197e-04\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2576e-04 - val_loss: 1.1889e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2353e-04 - val_loss: 1.4415e-04\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1573e-04 - val_loss: 1.5904e-04\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1650e-04 - val_loss: 1.5740e-04\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2963e-04 - val_loss: 1.0903e-04\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4592e-04 - val_loss: 1.3393e-04\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1687e-04 - val_loss: 1.3859e-04\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1356e-04 - val_loss: 1.3172e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2872e-04 - val_loss: 3.7377e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3910e-04 - val_loss: 1.2260e-04\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3024e-04 - val_loss: 1.1060e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1292e-04 - val_loss: 2.9869e-04\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3335e-04 - val_loss: 1.3352e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3486e-04 - val_loss: 1.2115e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3066e-04 - val_loss: 1.3145e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2931e-04 - val_loss: 1.6672e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2806e-04 - val_loss: 1.1005e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1370e-04 - val_loss: 1.0718e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4335e-04 - val_loss: 1.7033e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3140e-04 - val_loss: 1.1842e-04\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0907e-04 - val_loss: 1.3836e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1695e-04 - val_loss: 1.1351e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2146e-04 - val_loss: 1.2797e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0517e-04 - val_loss: 1.7048e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3412e-04 - val_loss: 1.0402e-04\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1341e-04 - val_loss: 1.0357e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2418e-04 - val_loss: 1.1267e-04\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2007e-04 - val_loss: 1.2265e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3143e-04 - val_loss: 1.4972e-04\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1136e-04 - val_loss: 1.2165e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2434e-04 - val_loss: 1.1596e-04\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/TSLA/ann\\assets\n"
     ]
    }
   ],
   "source": [
    "histories = {}\n",
    "\n",
    "def get_all_file_paths(directory):\n",
    "  \n",
    "    # initializing empty file paths list\n",
    "    file_paths = []\n",
    "  \n",
    "    # crawling through directory and subdirectories\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for fileName_model in files:\n",
    "            # join the two strings in order to form the full filepath.\n",
    "            filepath = os.path.join(root, fileName_model)\n",
    "            file_paths.append(filepath)\n",
    "  \n",
    "    # returning all file paths\n",
    "    return file_paths\n",
    "    \n",
    "for tick in data.keys():\n",
    "    stock_data = data['FB']\n",
    "    df = pd.DataFrame(stock_data).T\n",
    "    df['H-L'] = df['High'] - df['Low']\n",
    "    df['O-C'] = df['Open'] - df['Close']\n",
    "    df['7MA'] = df['Adj Close'].rolling(window=7).mean()\n",
    "    df['14MA'] = df['Adj Close'].rolling(window=14).mean()\n",
    "    df['21MA'] = df['Adj Close'].rolling(window=21).mean()\n",
    "    df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "\n",
    "    features = ['H-L','O-C','7MA','14MA','21MA','7SD','Volume','Close']\n",
    "    df = df[features].apply(pd.to_numeric)\n",
    "    df_final = df[20:].copy()\n",
    "    df_final['Close'] = df_final['Close'].shift(1)\n",
    "\n",
    "    features = ['H-L','O-C','7MA','14MA','21MA','7SD','Volume']\n",
    "    #https://stackoverflow.com/questions/36926140/how-to-convert-numpy-arrays-to-standard-tensorflow-format\n",
    "    X = np.asarray(df_final[1:][features], np.float32)\n",
    "    Y = np.asarray(df_final[1:]['Close'], np.float32)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    scaler_x = MinMaxScaler().fit(X_train)\n",
    "    scaler_y = MinMaxScaler().fit(y_train)\n",
    "\n",
    "    X_train = scaler_x.transform(X_train)\n",
    "    y_train = scaler_y.transform(y_train)\n",
    "    # Defining the Input layer and FIRST hidden layer, both are same!\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=50, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "    # Defining the Second layer of the model\n",
    "    # after the first layer we don't have to specify input_dim as keras configure it automatically\n",
    "    model.add(Dense(units=25, kernel_initializer='normal', activation='tanh'))\n",
    "\n",
    "    model.add(Dense(units=10, kernel_initializer='normal', activation='tanh'))\n",
    "    \n",
    "    # The output neuron is a single fully connected node \n",
    "    # Since we will be predicting a single number\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    # Fitting the ANN to the Training set\n",
    "    history = model.fit(X_train, y_train ,batch_size = 10, validation_split = 0.1, epochs = 180, verbose=1)\n",
    "\n",
    "    histories[tick] = history\n",
    "\n",
    "    filepath_model = \"../../../data/models/\" + tick + \"/ann\"\n",
    "    model.save(filepath_model)\n",
    "    file_paths = get_all_file_paths(filepath_model)\n",
    "\n",
    "    \n",
    "    #took help from this: https://www.geeksforgeeks.org/working-zip-files-python/\n",
    "    with ZipFile(filepath_model + \".zip\",'w') as zip:\n",
    "            # writing each file one by one\n",
    "            for file in file_paths:\n",
    "                zip.write(file)\n",
    "    \n",
    "    \n",
    "    fileName_model = \"ann.zip\"\n",
    "    bucket = storage.bucket()\n",
    "    #upload models\n",
    "    blob = bucket.blob(\"models/\" + tick + \"/\" + fileName_model)\n",
    "    blob.upload_from_filename(filepath_model+\".zip\")\n",
    "    \n",
    "    #upload normalizer training data\n",
    "    filepath_normalizer = \"../../../data/normalizers/\" + tick + \"/ann_x.pkl\"\n",
    "    pickle.dump(scaler_x, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "    filename_normalizer = \"ann_x.pkl\"\n",
    "    blob = bucket.blob(\"normalizers/\" + tick + \"/ann_x.pkl\")\n",
    "    blob.upload_from_filename(filepath_normalizer)\n",
    "\n",
    "    #upload normalizer predicted value\n",
    "    filepath_normalizer = \"../../../data/normalizers/\" + tick + \"/ann_y.pkl\"\n",
    "    pickle.dump(scaler_y, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "    filename_normalizer = \"ann_y.pkl\"\n",
    "    blob = bucket.blob(\"normalizers/\" + tick + \"/ann_y.pkl\")\n",
    "    blob.upload_from_filename(filepath_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['AAPL', 'AMZN', 'AVGO', 'CSCO', 'FB', 'GOOG', 'GOOGL', 'MSFT', 'NVDA', 'TSLA'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histories.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2816e62dd90>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(histories.values())[0].history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000102086204"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_test = y_test.reshape(-1,1)\n",
    "y_test_transform = scaler_y.transform(y_test)\n",
    "X_test = scaler_x.transform(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "mean_squared_error(y_test_transform, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[275.82086 ],\n",
       "       [132.01283 ],\n",
       "       [137.7724  ],\n",
       "       [212.44344 ],\n",
       "       [186.31624 ],\n",
       "       [177.45625 ],\n",
       "       [ 24.046793],\n",
       "       [181.95026 ],\n",
       "       [ 81.453514],\n",
       "       [176.90802 ],\n",
       "       [183.17456 ],\n",
       "       [182.11317 ],\n",
       "       [105.79778 ],\n",
       "       [158.21648 ],\n",
       "       [196.85593 ],\n",
       "       [128.52538 ],\n",
       "       [189.04941 ],\n",
       "       [345.6005  ],\n",
       "       [265.8512  ],\n",
       "       [102.1883  ],\n",
       "       [113.7507  ],\n",
       "       [104.49941 ],\n",
       "       [ 63.362526],\n",
       "       [161.01546 ],\n",
       "       [186.20975 ],\n",
       "       [191.63206 ],\n",
       "       [146.45222 ],\n",
       "       [ 45.7251  ],\n",
       "       [ 65.18101 ],\n",
       "       [169.92487 ],\n",
       "       [142.63333 ],\n",
       "       [193.73663 ],\n",
       "       [ 91.46128 ],\n",
       "       [180.10179 ],\n",
       "       [119.68643 ],\n",
       "       [105.71038 ],\n",
       "       [273.8028  ],\n",
       "       [139.82077 ],\n",
       "       [138.38922 ],\n",
       "       [ 25.708197],\n",
       "       [184.50807 ],\n",
       "       [ 76.17111 ],\n",
       "       [282.23877 ],\n",
       "       [271.3276  ],\n",
       "       [ 90.22754 ],\n",
       "       [174.09407 ],\n",
       "       [164.25815 ],\n",
       "       [179.92426 ],\n",
       "       [285.4602  ],\n",
       "       [ 79.10763 ],\n",
       "       [ 93.63441 ],\n",
       "       [ 23.982454],\n",
       "       [ 25.296406],\n",
       "       [372.90433 ],\n",
       "       [128.85146 ],\n",
       "       [364.83185 ],\n",
       "       [152.7113  ],\n",
       "       [193.29004 ],\n",
       "       [147.6866  ],\n",
       "       [ 55.264126],\n",
       "       [107.57619 ],\n",
       "       [ 93.35951 ],\n",
       "       [136.64041 ],\n",
       "       [ 61.96492 ],\n",
       "       [ 24.191319],\n",
       "       [173.94139 ],\n",
       "       [272.2231  ],\n",
       "       [277.61554 ],\n",
       "       [ 75.2673  ],\n",
       "       [155.04323 ],\n",
       "       [ 27.390419],\n",
       "       [192.53334 ],\n",
       "       [ 75.50625 ],\n",
       "       [ 55.832916],\n",
       "       [117.32752 ],\n",
       "       [138.85558 ],\n",
       "       [200.19043 ],\n",
       "       [168.29155 ],\n",
       "       [194.81413 ],\n",
       "       [152.76471 ],\n",
       "       [ 74.01182 ],\n",
       "       [173.35947 ],\n",
       "       [233.07622 ],\n",
       "       [184.43378 ],\n",
       "       [ 94.574776],\n",
       "       [ 75.81567 ],\n",
       "       [181.42967 ],\n",
       "       [ 61.286785],\n",
       "       [189.85674 ],\n",
       "       [ 94.769585],\n",
       "       [ 76.91856 ],\n",
       "       [167.03082 ],\n",
       "       [339.48682 ],\n",
       "       [124.4043  ],\n",
       "       [176.20422 ],\n",
       "       [ 77.015175],\n",
       "       [ 43.87328 ],\n",
       "       [ 93.20273 ],\n",
       "       [194.83063 ],\n",
       "       [335.95966 ],\n",
       "       [169.28044 ],\n",
       "       [ 77.47409 ],\n",
       "       [174.89441 ],\n",
       "       [137.18944 ],\n",
       "       [339.5189  ],\n",
       "       [ 81.41093 ],\n",
       "       [199.58746 ],\n",
       "       [126.05431 ],\n",
       "       [274.6929  ],\n",
       "       [183.78198 ],\n",
       "       [ 91.765205],\n",
       "       [ 80.450455],\n",
       "       [239.41162 ],\n",
       "       [378.98038 ],\n",
       "       [117.25913 ],\n",
       "       [260.01575 ],\n",
       "       [162.43488 ],\n",
       "       [313.8184  ],\n",
       "       [176.77913 ],\n",
       "       [ 25.737255],\n",
       "       [239.69788 ],\n",
       "       [210.94917 ],\n",
       "       [207.69136 ],\n",
       "       [104.50328 ],\n",
       "       [ 44.213463],\n",
       "       [ 79.38711 ],\n",
       "       [ 72.36871 ],\n",
       "       [ 68.4109  ],\n",
       "       [163.3856  ],\n",
       "       [230.74739 ],\n",
       "       [ 77.34278 ],\n",
       "       [220.86922 ],\n",
       "       [153.93103 ],\n",
       "       [ 79.4353  ],\n",
       "       [ 25.76023 ],\n",
       "       [153.05313 ],\n",
       "       [343.9584  ],\n",
       "       [ 75.89517 ],\n",
       "       [ 73.166145],\n",
       "       [164.98994 ],\n",
       "       [361.41507 ],\n",
       "       [193.19316 ],\n",
       "       [116.84858 ],\n",
       "       [163.05676 ],\n",
       "       [ 78.37069 ],\n",
       "       [106.63292 ],\n",
       "       [118.311874],\n",
       "       [273.02673 ],\n",
       "       [ 90.767456],\n",
       "       [139.83766 ],\n",
       "       [124.90364 ],\n",
       "       [210.28726 ],\n",
       "       [ 75.27394 ],\n",
       "       [130.97458 ],\n",
       "       [154.53076 ],\n",
       "       [147.9575  ],\n",
       "       [206.82706 ],\n",
       "       [211.29271 ],\n",
       "       [181.07268 ],\n",
       "       [183.13731 ],\n",
       "       [153.60883 ],\n",
       "       [219.63585 ],\n",
       "       [139.26701 ],\n",
       "       [119.119644],\n",
       "       [185.81285 ],\n",
       "       [239.79851 ],\n",
       "       [163.59764 ],\n",
       "       [335.40854 ],\n",
       "       [204.82513 ],\n",
       "       [323.43216 ],\n",
       "       [145.57967 ],\n",
       "       [180.47449 ],\n",
       "       [144.99109 ],\n",
       "       [152.07458 ],\n",
       "       [105.96673 ],\n",
       "       [ 90.43471 ],\n",
       "       [ 98.06055 ],\n",
       "       [ 73.68548 ],\n",
       "       [ 80.957344],\n",
       "       [ 75.83867 ],\n",
       "       [124.63994 ],\n",
       "       [212.31607 ],\n",
       "       [109.838684],\n",
       "       [192.19205 ],\n",
       "       [131.22047 ],\n",
       "       [149.35747 ],\n",
       "       [131.4006  ],\n",
       "       [230.89825 ],\n",
       "       [180.75246 ],\n",
       "       [154.31538 ],\n",
       "       [134.01622 ],\n",
       "       [ 83.525955],\n",
       "       [201.19586 ],\n",
       "       [137.41846 ],\n",
       "       [178.79852 ],\n",
       "       [336.11594 ],\n",
       "       [ 23.068563],\n",
       "       [ 73.76922 ],\n",
       "       [128.43896 ],\n",
       "       [117.15197 ],\n",
       "       [173.66261 ],\n",
       "       [ 73.01925 ],\n",
       "       [149.61937 ],\n",
       "       [195.73257 ],\n",
       "       [146.4311  ],\n",
       "       [ 35.91121 ],\n",
       "       [ 75.60044 ],\n",
       "       [268.75293 ],\n",
       "       [155.7314  ],\n",
       "       [120.03577 ],\n",
       "       [152.67715 ],\n",
       "       [181.33058 ],\n",
       "       [188.25189 ],\n",
       "       [259.36087 ],\n",
       "       [258.06964 ],\n",
       "       [ 76.77243 ],\n",
       "       [113.51879 ],\n",
       "       [ 57.192394],\n",
       "       [112.31941 ],\n",
       "       [128.4517  ],\n",
       "       [234.94516 ],\n",
       "       [ 81.71647 ],\n",
       "       [171.67815 ],\n",
       "       [ 68.66816 ],\n",
       "       [ 81.81831 ],\n",
       "       [213.6309  ],\n",
       "       [272.98572 ],\n",
       "       [343.8371  ],\n",
       "       [128.6014  ],\n",
       "       [163.0761  ],\n",
       "       [335.49887 ],\n",
       "       [287.00128 ],\n",
       "       [ 68.07018 ],\n",
       "       [ 26.284687],\n",
       "       [185.505   ],\n",
       "       [343.46722 ],\n",
       "       [ 62.441177],\n",
       "       [ 67.317726],\n",
       "       [242.91454 ],\n",
       "       [105.87041 ],\n",
       "       [ 26.289692],\n",
       "       [165.43636 ],\n",
       "       [186.03064 ],\n",
       "       [287.58954 ],\n",
       "       [168.33403 ],\n",
       "       [119.4971  ],\n",
       "       [121.11117 ],\n",
       "       [202.41312 ],\n",
       "       [111.85081 ],\n",
       "       [162.90793 ],\n",
       "       [ 76.69278 ],\n",
       "       [278.08606 ],\n",
       "       [265.78415 ],\n",
       "       [172.29291 ],\n",
       "       [106.26047 ],\n",
       "       [237.8559  ],\n",
       "       [133.75406 ],\n",
       "       [ 78.16882 ],\n",
       "       [115.23071 ],\n",
       "       [172.88896 ],\n",
       "       [123.65972 ],\n",
       "       [ 57.889225],\n",
       "       [ 62.376102],\n",
       "       [138.64381 ],\n",
       "       [ 57.581123],\n",
       "       [148.1306  ],\n",
       "       [336.26987 ],\n",
       "       [163.72676 ],\n",
       "       [268.2308  ],\n",
       "       [ 64.11995 ],\n",
       "       [317.4474  ],\n",
       "       [347.28177 ],\n",
       "       [207.16896 ],\n",
       "       [128.81471 ],\n",
       "       [201.02017 ],\n",
       "       [293.35083 ],\n",
       "       [271.48465 ],\n",
       "       [217.48766 ],\n",
       "       [192.34537 ],\n",
       "       [363.95944 ],\n",
       "       [ 76.14876 ],\n",
       "       [103.920906],\n",
       "       [163.4091  ],\n",
       "       [333.1982  ],\n",
       "       [331.96088 ],\n",
       "       [128.89944 ],\n",
       "       [ 56.784843],\n",
       "       [ 23.373701],\n",
       "       [ 66.7361  ],\n",
       "       [134.40747 ],\n",
       "       [181.56569 ],\n",
       "       [325.4243  ],\n",
       "       [271.26477 ],\n",
       "       [ 24.421082],\n",
       "       [ 63.6807  ],\n",
       "       [313.83347 ],\n",
       "       [269.88458 ],\n",
       "       [ 55.86956 ],\n",
       "       [116.704285],\n",
       "       [ 62.2582  ],\n",
       "       [ 75.130936],\n",
       "       [ 46.19666 ],\n",
       "       [281.2757  ],\n",
       "       [ 56.699497],\n",
       "       [ 50.352493],\n",
       "       [299.6203  ],\n",
       "       [ 76.88405 ],\n",
       "       [182.70813 ],\n",
       "       [105.05354 ],\n",
       "       [ 24.412762],\n",
       "       [192.94339 ],\n",
       "       [ 75.83806 ],\n",
       "       [ 80.75252 ],\n",
       "       [180.7711  ],\n",
       "       [ 46.176174],\n",
       "       [135.02742 ],\n",
       "       [116.3989  ],\n",
       "       [315.9995  ],\n",
       "       [120.33523 ],\n",
       "       [157.21098 ],\n",
       "       [147.97812 ],\n",
       "       [ 77.2869  ],\n",
       "       [365.4746  ],\n",
       "       [199.91898 ],\n",
       "       [258.73846 ],\n",
       "       [ 98.759224],\n",
       "       [209.8579  ],\n",
       "       [102.698845],\n",
       "       [103.99232 ],\n",
       "       [ 76.40372 ],\n",
       "       [195.52719 ],\n",
       "       [337.05856 ],\n",
       "       [ 25.15724 ],\n",
       "       [267.89697 ],\n",
       "       [176.98438 ],\n",
       "       [ 93.136765],\n",
       "       [245.2187  ],\n",
       "       [362.39157 ],\n",
       "       [132.87723 ],\n",
       "       [163.8803  ],\n",
       "       [111.85827 ],\n",
       "       [184.55116 ],\n",
       "       [170.9604  ],\n",
       "       [117.94415 ],\n",
       "       [171.82504 ],\n",
       "       [ 53.96667 ],\n",
       "       [187.68895 ],\n",
       "       [228.72983 ],\n",
       "       [ 24.418293],\n",
       "       [ 61.558235],\n",
       "       [124.60079 ],\n",
       "       [113.84779 ],\n",
       "       [ 25.152626],\n",
       "       [194.21045 ],\n",
       "       [178.84628 ],\n",
       "       [155.16576 ],\n",
       "       [276.2576  ],\n",
       "       [ 92.05215 ],\n",
       "       [172.09541 ],\n",
       "       [105.56809 ],\n",
       "       [ 25.238838],\n",
       "       [ 75.72938 ],\n",
       "       [ 25.14922 ],\n",
       "       [268.21976 ],\n",
       "       [ 38.436153],\n",
       "       [185.42595 ],\n",
       "       [170.93558 ],\n",
       "       [260.78238 ],\n",
       "       [120.20303 ],\n",
       "       [336.77783 ],\n",
       "       [116.792244],\n",
       "       [211.66846 ],\n",
       "       [164.48448 ],\n",
       "       [169.05457 ],\n",
       "       [114.26703 ],\n",
       "       [287.0281  ],\n",
       "       [149.96368 ],\n",
       "       [165.428   ],\n",
       "       [ 78.51965 ],\n",
       "       [374.06335 ],\n",
       "       [119.791794],\n",
       "       [ 26.175451],\n",
       "       [239.37999 ],\n",
       "       [ 46.905235],\n",
       "       [ 23.802332],\n",
       "       [ 42.705555],\n",
       "       [ 24.348959],\n",
       "       [116.785736],\n",
       "       [ 81.59362 ],\n",
       "       [288.3005  ],\n",
       "       [142.16402 ],\n",
       "       [186.6683  ],\n",
       "       [ 26.090141],\n",
       "       [ 92.861115],\n",
       "       [ 24.254488],\n",
       "       [ 25.916426],\n",
       "       [191.634   ],\n",
       "       [ 55.970005],\n",
       "       [153.45744 ],\n",
       "       [263.16626 ],\n",
       "       [ 77.73884 ],\n",
       "       [223.92719 ],\n",
       "       [ 80.26333 ],\n",
       "       [355.24362 ],\n",
       "       [ 76.481285],\n",
       "       [117.43972 ],\n",
       "       [138.3872  ],\n",
       "       [287.37796 ],\n",
       "       [147.87796 ],\n",
       "       [110.66141 ],\n",
       "       [ 65.13219 ],\n",
       "       [195.13718 ],\n",
       "       [ 56.91782 ],\n",
       "       [242.79848 ],\n",
       "       [ 75.711   ],\n",
       "       [ 74.98059 ],\n",
       "       [119.94807 ],\n",
       "       [128.5518  ],\n",
       "       [342.8317  ],\n",
       "       [301.18884 ],\n",
       "       [174.17197 ],\n",
       "       [324.2492  ],\n",
       "       [169.86095 ],\n",
       "       [182.50182 ],\n",
       "       [ 73.91531 ],\n",
       "       [224.32835 ],\n",
       "       [183.95886 ],\n",
       "       [210.04207 ],\n",
       "       [154.4507  ],\n",
       "       [ 72.12455 ],\n",
       "       [187.3357  ],\n",
       "       [141.2697  ],\n",
       "       [123.73432 ],\n",
       "       [262.42145 ],\n",
       "       [346.07486 ],\n",
       "       [ 67.72371 ],\n",
       "       [ 69.285225],\n",
       "       [139.6625  ],\n",
       "       [362.74057 ],\n",
       "       [214.93274 ],\n",
       "       [115.05827 ],\n",
       "       [109.11601 ],\n",
       "       [186.38823 ],\n",
       "       [140.57222 ],\n",
       "       [271.42282 ],\n",
       "       [262.5372  ],\n",
       "       [ 94.9275  ],\n",
       "       [176.75743 ],\n",
       "       [ 47.026115],\n",
       "       [161.4536  ],\n",
       "       [198.21574 ],\n",
       "       [173.15408 ]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_y.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick = 'NVDA'\n",
    "import pandas_datareader as pdr\n",
    "def getTestData(ticker, start):\n",
    "    data = pdr.get_data_yahoo(ticker, start=start, end=today)\n",
    "    # dataname= ticker+\"_\"+str(today)\n",
    "    return data[-100:]\n",
    "    \n",
    "from datetime import date  \n",
    "today = date.today()\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "start = d = today - timedelta(days=190)\n",
    "\n",
    "df = getTestData(tick,start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['H-L'] = df['High'] - df['Low']\n",
    "df['O-C'] = df['Open'] - df['Close']\n",
    "df['7MA'] = df['Adj Close'].rolling(window=7).mean()\n",
    "df['14MA'] = df['Adj Close'].rolling(window=14).mean()\n",
    "df['21MA'] = df['Adj Close'].rolling(window=21).mean()\n",
    "df['7SD'] = df['Adj Close'].rolling(window=7).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H-L', 'O-C', '7MA', '14MA', '21MA', '7SD', 'Volume']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.asarray(df[-1:][features], np.float32)\n",
    "test = scaler_x.transform(test_data)\n",
    "pred = model.predict(test)  \n",
    "pred_price = scaler_y.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AAPL': <keras.callbacks.History at 0x28351a6bd00>,\n",
       " 'AMZN': <keras.callbacks.History at 0x283600eeaf0>,\n",
       " 'AVGO': <keras.callbacks.History at 0x2815b60a790>,\n",
       " 'CSCO': <keras.callbacks.History at 0x2810cebd040>,\n",
       " 'FB': <keras.callbacks.History at 0x2810c84b9d0>,\n",
       " 'GOOG': <keras.callbacks.History at 0x28351a5dbb0>,\n",
       " 'GOOGL': <keras.callbacks.History at 0x28364c78850>,\n",
       " 'MSFT': <keras.callbacks.History at 0x281765a0ee0>,\n",
       " 'NVDA': <keras.callbacks.History at 0x2833c813fa0>,\n",
       " 'TSLA': <keras.callbacks.History at 0x2836b817f40>}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca1edcbf756503ef84b6dec62a8a514ccc7037ff00e4b32f050febc64cc6ba90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('FYP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
