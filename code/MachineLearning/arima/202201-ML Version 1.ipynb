{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "from datetime import date\n",
    "import yfinance as yf\n",
    "\n",
    "yf.pdr_override()\n",
    "import pandas as pd\n",
    "\n",
    "ticker_list=[\"AAPL\", \"MSFT\", \"AMZN\", \"TSLA\", \"GOOGL\"]\n",
    "today = date.today()\n",
    "# We can get data by our choice by giving days bracket\n",
    "start_date= \"2015-01-01\"\n",
    "end_date=\"2020-11-30\"\n",
    "\n",
    "files=[]\n",
    "def getData(ticker):\n",
    "    data = pdr.get_data_yahoo(ticker, start=start_date, end=today)\n",
    "    # dataname= ticker+\"_\"+str(today)\n",
    "    files.append((data,ticker))\n",
    "    \n",
    "for tik in ticker_list:\n",
    "    getData(tik)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-12-31</th>\n",
       "      <td>18.233213</td>\n",
       "      <td>19.121429</td>\n",
       "      <td>18.178572</td>\n",
       "      <td>19.006071</td>\n",
       "      <td>16.439852</td>\n",
       "      <td>659492400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>19.779285</td>\n",
       "      <td>19.821428</td>\n",
       "      <td>19.343929</td>\n",
       "      <td>19.608213</td>\n",
       "      <td>16.960695</td>\n",
       "      <td>560518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>19.567142</td>\n",
       "      <td>19.631071</td>\n",
       "      <td>19.321428</td>\n",
       "      <td>19.360714</td>\n",
       "      <td>16.746613</td>\n",
       "      <td>352965200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>19.177500</td>\n",
       "      <td>19.236786</td>\n",
       "      <td>18.779642</td>\n",
       "      <td>18.821428</td>\n",
       "      <td>16.280138</td>\n",
       "      <td>594333600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-07</th>\n",
       "      <td>18.642857</td>\n",
       "      <td>18.903570</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>18.710714</td>\n",
       "      <td>16.184374</td>\n",
       "      <td>484156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-24</th>\n",
       "      <td>160.020004</td>\n",
       "      <td>162.300003</td>\n",
       "      <td>154.699997</td>\n",
       "      <td>161.619995</td>\n",
       "      <td>161.619995</td>\n",
       "      <td>162706700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-25</th>\n",
       "      <td>158.979996</td>\n",
       "      <td>162.759995</td>\n",
       "      <td>157.020004</td>\n",
       "      <td>159.779999</td>\n",
       "      <td>159.779999</td>\n",
       "      <td>115798400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-26</th>\n",
       "      <td>163.500000</td>\n",
       "      <td>164.389999</td>\n",
       "      <td>157.820007</td>\n",
       "      <td>159.690002</td>\n",
       "      <td>159.690002</td>\n",
       "      <td>108275300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-27</th>\n",
       "      <td>162.449997</td>\n",
       "      <td>163.839996</td>\n",
       "      <td>158.279999</td>\n",
       "      <td>159.220001</td>\n",
       "      <td>159.220001</td>\n",
       "      <td>116691400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-28</th>\n",
       "      <td>165.710007</td>\n",
       "      <td>170.350006</td>\n",
       "      <td>162.800003</td>\n",
       "      <td>170.330002</td>\n",
       "      <td>170.330002</td>\n",
       "      <td>179485800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2287 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2012-12-31   18.233213   19.121429   18.178572   19.006071   16.439852   \n",
       "2013-01-02   19.779285   19.821428   19.343929   19.608213   16.960695   \n",
       "2013-01-03   19.567142   19.631071   19.321428   19.360714   16.746613   \n",
       "2013-01-04   19.177500   19.236786   18.779642   18.821428   16.280138   \n",
       "2013-01-07   18.642857   18.903570   18.400000   18.710714   16.184374   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2022-01-24  160.020004  162.300003  154.699997  161.619995  161.619995   \n",
       "2022-01-25  158.979996  162.759995  157.020004  159.779999  159.779999   \n",
       "2022-01-26  163.500000  164.389999  157.820007  159.690002  159.690002   \n",
       "2022-01-27  162.449997  163.839996  158.279999  159.220001  159.220001   \n",
       "2022-01-28  165.710007  170.350006  162.800003  170.330002  170.330002   \n",
       "\n",
       "               Volume  \n",
       "Date                   \n",
       "2012-12-31  659492400  \n",
       "2013-01-02  560518000  \n",
       "2013-01-03  352965200  \n",
       "2013-01-04  594333600  \n",
       "2013-01-07  484156400  \n",
       "...               ...  \n",
       "2022-01-24  162706700  \n",
       "2022-01-25  115798400  \n",
       "2022-01-26  108275300  \n",
       "2022-01-27  116691400  \n",
       "2022-01-28  179485800  \n",
       "\n",
       "[2287 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = 'AAPL'\n",
    "data = pdr.get_data_yahoo(ticker, start=start_date, end=today)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup firebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "\n",
    "cred_obj = firebase_admin.credentials.Certificate('../../../fyp2022-stockpriceprediction-firebase-adminsdk-ku62m-f9ed330292.json')\n",
    "fyp_app = firebase_admin.initialize_app(cred_obj, {\n",
    "\t'databaseURL':\"https://fyp2022-stockpriceprediction-default-rtdb.asia-southeast1.firebasedatabase.app/\",\n",
    "\t'storageBucket': 'fyp2022-stockpriceprediction.appspot.com'\n",
    "\t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://cloud.google.com/storage/docs/downloading-objects#storage-download-object-python\n",
    "from firebase_admin import storage\n",
    "b = storage.bucket()\n",
    "for ind, file in enumerate(b.list_blobs()):\n",
    "    if(i<3):\n",
    "        continue\n",
    "    #print(file.download_to_filename('1.zip'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://storage.googleapis.com/fyp2022-stockpriceprediction.appspot.com/normalizers/TSLA/multi_lstm.pkl?Expires=1644330859&GoogleAccessId=firebase-adminsdk-ku62m%40fyp2022-stockpriceprediction.iam.gserviceaccount.com&Signature=lWzBiW7D3%2Foa%2Fi8ySOsUe8JGUzMFPS%2BYBDqrEDzLDM%2BBX1BcVtWKqZHDc7zQzc42QDlcYwP2Bivb7ohRhoDUFXvfgbUUQlZDZe3kxVvC45ZR8%2BcMcpuf6seUChm4l24K6ZGGS4m4b73OTAJo3lpvCIJL6BfkajvsqRxcxxYDtXJS7cAlHCuQTZh3IA0pMWbVXDaHeqKpcuO4dfWpbkFLHmin9jpwxTQZHX9ttbuPQ%2Fb791v%2FnlUyzT8aatWMYkh%2BzmWzjB4v94u3RmSbcxc3kvdVk4aHPE1YAYI3bNDBWd4nYEYQ9iR4OSDRC2%2B7Ll9qbEEP8cC9%2F0gQbMFRcHzLxg%3D%3D'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/53304517/how-to-retrieve-image-from-firebase-storage-using-python\n",
    "file.generate_signed_url(timedelta(seconds=300), method='GET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://storage.googleapis.com/fyp2022-stockpriceprediction.appspot.com/normalizers/TSLA/multi_lstm.pkl?Expires=1644330859&GoogleAccessId=firebase-adminsdk-ku62m%40fyp2022-stockpriceprediction.iam.gserviceaccount.com&Signature=lWzBiW7D3%2Foa%2Fi8ySOsUe8JGUzMFPS%2BYBDqrEDzLDM%2BBX1BcVtWKqZHDc7zQzc42QDlcYwP2Bivb7ohRhoDUFXvfgbUUQlZDZe3kxVvC45ZR8%2BcMcpuf6seUChm4l24K6ZGGS4m4b73OTAJo3lpvCIJL6BfkajvsqRxcxxYDtXJS7cAlHCuQTZh3IA0pMWbVXDaHeqKpcuO4dfWpbkFLHmin9jpwxTQZHX9ttbuPQ%2Fb791v%2FnlUyzT8aatWMYkh%2BzmWzjB4v94u3RmSbcxc3kvdVk4aHPE1YAYI3bNDBWd4nYEYQ9iR4OSDRC2%2B7Ll9qbEEP8cC9%2F0gQbMFRcHzLxg%3D%3D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle as cp\n",
    "from urllib.request import urlopen\n",
    "loaded_pickle_object = cp.load(urlopen(url, 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cloudpickle\n",
      "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: cloudpickle\n",
      "Successfully installed cloudpickle-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrebase\n",
    "\n",
    "config = {\n",
    "  \"apiKey\": firebase_admin.credentials.Certificate('../../../fyp2022-stockpriceprediction-firebase-adminsdk-ku62m-f9ed330292.json'),\n",
    "  \"authDomain\": \"fyp2022-stockpriceprediction.firebaseapp.com\",\n",
    "  'databaseURL':\"https://fyp2022-stockpriceprediction-default-rtdb.asia-southeast1.firebasedatabase.app/\",\n",
    "  'storageBucket': 'fyp2022-stockpriceprediction.appspot.com',\n",
    "  'serviceAccount': '../../../fyp2022-stockpriceprediction-firebase-adminsdk-ku62m-f9ed330292.json'\n",
    "}\n",
    "\n",
    "firebase = pyrebase.initialize_app(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/48181580/download-all-files-from-firebase-storage-using-python\n",
    "#https://github.com/thisbejim/Pyrebase#add-pyrebase-to-your-application\n",
    "storage = firebase.storage()\n",
    "for file in storage.child('models/AAPL/').list_files():\n",
    "    with open(storage.child(file.name)) as f:\n",
    "        word = f\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data into firebase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firebase_admin import db\n",
    "from datetime import datetime as dt\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    tick = file[1]\n",
    "    data = file[0]\n",
    "    data['Ticker'] = tick\n",
    "    \n",
    "    #convert date index to string, as firebsae cant have datetime as an index\n",
    "    data.index = data.index.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    #convert the dataframe to json/dictionary\n",
    "    data_dict = data.to_dict(orient=\"index\")\n",
    "    \n",
    "    #upload it to the database\n",
    "    ref = db.reference(\"/data/\"+tick)\n",
    "    ref.set(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from firebase database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from firebase_admin import db\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "ref = db.reference(\"/data\")\n",
    "data = ref.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_msft = {}\n",
    "data_msft['MSFT'] = data['MSFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_file_paths(directory):\n",
    "  \n",
    "    # initializing empty file paths list\n",
    "    file_paths = []\n",
    "  \n",
    "    # crawling through directory and subdirectories\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for fileName_model in files:\n",
    "            # join the two strings in order to form the full filepath.\n",
    "            filepath = os.path.join(root, fileName_model)\n",
    "            file_paths.append(filepath)\n",
    "  \n",
    "    # returning all file paths\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import math\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import os\n",
    "from firebase_admin import storage\n",
    "from sklearn.metrics import mean_squared_error\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "  # Specify an invalid GPU device\n",
    "  #with tf.device('/device:GPU:2'):\n",
    "  \n",
    "#   with tf.device('/device:GPU:2'):\n",
    "    for tick, stock_data in data_msft.items():\n",
    "      print(tick)\n",
    "      df = pd.DataFrame(stock_data).T\n",
    "      df = df.reset_index()['Close']\n",
    "      scaler = MinMaxScaler(feature_range=(0,1))\n",
    "      df=scaler.fit_transform(np.array(df).reshape(-1,1))\n",
    "      train_data = df[:int(0.9*len(df))]\n",
    "      test_data = df[int(0.9*len(df)):]\n",
    "      train_data = train_data.reshape(-1,1)\n",
    "      test_data = test_data.reshape(-1,1)\n",
    "      \n",
    "      def create_dataset(dataset, time_step=1):\n",
    "          dataX, dataY = [], []\n",
    "          for i in range(len(dataset)-time_step-1):\n",
    "              a = dataset[i:(i+time_step), 0]\n",
    "              dataX.append(a)\n",
    "              dataY.append(dataset[i + time_step, 0])\n",
    "          return np.array(dataX), np.array(dataY)\n",
    "      time_step = 60\n",
    "      X_train, y_train = create_dataset(train_data, time_step)\n",
    "      X_test, ytest = create_dataset(test_data, time_step)\n",
    "      \n",
    "      X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "      X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "      \n",
    "      model=Sequential()\n",
    "      model.add(LSTM(50,return_sequences=True,input_shape=(100,1)))\n",
    "      model.add(LSTM(50,return_sequences=True))\n",
    "      model.add(LSTM(50))\n",
    "      model.add(Dense(1))\n",
    "      model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "      history = model.fit(X_train, y_train,epochs=100, validation_split = 0.2)\n",
    "      # break\n",
    "      \n",
    "      # calling function to get all file paths in the directory\n",
    "      filepath_model = \"../../../data/models/\" + tick + \"/lstm\"\n",
    "      model.save(filepath_model)\n",
    "      file_paths = get_all_file_paths(filepath_model)\n",
    "\n",
    "      \n",
    "      #took help from this: https://www.geeksforgeeks.org/working-zip-files-python/\n",
    "      with ZipFile(filepath_model + \".zip\",'w') as zip:\n",
    "              # writing each file one by one\n",
    "              for file in file_paths:\n",
    "                  zip.write(file)\n",
    "      \n",
    "      \n",
    "      fileName_model = \"lstm\"\n",
    "      bucket = storage.bucket()\n",
    "    #   #upload models\n",
    "      blob = bucket.blob(\"models/\" + tick + \"/\" + fileName_model)\n",
    "      blob.upload_from_filename(fileName_model+\".zip\")\n",
    "      \n",
    "      #upload normalizer\n",
    "      filepath_normalizer = \"../../../data/normalizers/\" + tick + \"/lstm.pkl\"\n",
    "      pickle.dump(scaler, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "      filename_normalizer = \"lstm.pkl\"\n",
    "      blob = bucket.blob(\"normalizers/\" + tick + \"/\" + filename_normalizer)\n",
    "      blob.upload_from_filename(filepath_normalizer)\n",
    "      \n",
    "      # Opt : if you want to make public access from the URL\n",
    "      #blob.make_public()\n",
    "    # checking whether folder exists or not\n",
    "      folder_path = filepath_model\n",
    "      if os.path.exists(folder_path):\n",
    "\n",
    "          # checking whether the folder is empty or not\n",
    "          if len(os.listdir(folder_path)) == 0:\n",
    "              # removing the file using the os.remove() method\n",
    "              os.rmdir(folder_path)\n",
    "          else:\n",
    "              # messaging saying folder not empty\n",
    "              print(\"Folder is not empty\")\n",
    "      else:\n",
    "          # file not found message\n",
    "          print(\"File not found in the directory\")\n",
    "        \n",
    "except RuntimeError as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op OptionsDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_predict_function_291545 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_predict_function_291545 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_predict_function_291545 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_predict_function_291545 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ConcatV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.64311682215732"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict=model.predict(X_test)\n",
    "test_predict=scaler.inverse_transform(test_predict)\n",
    "\n",
    "ytest = scaler.inverse_transform(ytest.reshape(-1,1))\n",
    "error = math.sqrt(mean_squared_error(ytest,test_predict))\n",
    "error    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0307830607276087"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predict=model.predict(X_train)\n",
    "train_predict=scaler.inverse_transform(train_predict)\n",
    "y_train = scaler.inverse_transform(y_train.reshape(-1,1))\n",
    "math.sqrt(mean_squared_error(y_train,train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                             Modified             Size\n",
      "AAPL_model/keras_metadata.pb                   2022-01-12 19:16:08        17068\n",
      "AAPL_model/saved_model.pb                      2022-01-12 19:16:08      2382689\n",
      "AAPL_model/variables/variables.data-00000-of-00001 2022-01-12 19:16:08       618372\n",
      "AAPL_model/variables/variables.index           2022-01-12 19:16:08         2731\n",
      "Extracting all the files now...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# specifying the zip file name\n",
    "file_name = \"AAPL_model.zip\"\n",
    "  \n",
    "# opening the zip file in READ mode\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "    # printing all the contents of the zip file\n",
    "    zip.printdir()\n",
    "  \n",
    "    # extracting all the files\n",
    "    print('Extracting all the files now...')\n",
    "    zip.extractall()\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Qr in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DiagPart in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sign in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Transpose in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ConcatV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Qr in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DiagPart in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sign in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Transpose in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ConcatV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Qr in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DiagPart in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sign in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Transpose in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ConcatV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RestoreV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op OptionsDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_predict_function_332981 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[174.37128]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the last 100 days and make a prediction\n",
    "tick = 'AAPL'\n",
    "def getTestData(ticker, start):\n",
    "    data = pdr.get_data_yahoo(ticker, start=start, end=today)\n",
    "    # dataname= ticker+\"_\"+str(today)\n",
    "    return data[-100:]\n",
    "    \n",
    "   \n",
    "   \n",
    "from datetime import datetime, timedelta\n",
    "start = d = today - timedelta(days=190)\n",
    "\n",
    "df = getTestData(tick,start) \n",
    "\n",
    "#df = pd.DataFrame(stock_data).T\n",
    "df = df.reset_index()['Close']\n",
    "df=scaler.transform(np.array(df).reshape(-1,1))\n",
    "test_data = df.reshape(-1,1)\n",
    "\n",
    "import keras.models\n",
    "model = keras.models.load_model(tick + '_model')\n",
    "prediction = model.predict( np.array( [test_data,] )  )\n",
    "scaler.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(1,1,1)(0,1,1)[12]             : AIC=inf, Time=1.47 sec\n",
      " ARIMA(0,1,0)(0,1,0)[12]             : AIC=8903.869, Time=0.05 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12]             : AIC=8357.856, Time=0.23 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12]             : AIC=inf, Time=0.92 sec\n",
      " ARIMA(1,1,0)(0,1,0)[12]             : AIC=8891.064, Time=0.08 sec\n",
      " ARIMA(1,1,0)(2,1,0)[12]             : AIC=8009.664, Time=0.47 sec\n",
      " ARIMA(1,1,0)(2,1,1)[12]             : AIC=inf, Time=2.58 sec\n",
      " ARIMA(1,1,0)(1,1,1)[12]             : AIC=inf, Time=1.20 sec\n",
      " ARIMA(0,1,0)(2,1,0)[12]             : AIC=8023.083, Time=0.39 sec\n",
      " ARIMA(2,1,0)(2,1,0)[12]             : AIC=8011.656, Time=0.56 sec\n",
      " ARIMA(1,1,1)(2,1,0)[12]             : AIC=8011.619, Time=1.13 sec\n",
      " ARIMA(0,1,1)(2,1,0)[12]             : AIC=8009.624, Time=0.50 sec\n",
      " ARIMA(0,1,1)(1,1,0)[12]             : AIC=8358.974, Time=0.24 sec\n",
      " ARIMA(0,1,1)(2,1,1)[12]             : AIC=inf, Time=2.62 sec\n",
      " ARIMA(0,1,1)(1,1,1)[12]             : AIC=inf, Time=1.39 sec\n",
      " ARIMA(0,1,2)(2,1,0)[12]             : AIC=8011.623, Time=0.53 sec\n",
      " ARIMA(1,1,2)(2,1,0)[12]             : AIC=8013.286, Time=1.57 sec\n",
      " ARIMA(0,1,1)(2,1,0)[12] intercept   : AIC=8011.601, Time=1.44 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,1)(2,1,0)[12]          \n",
      "Total fit time: 17.367 seconds\n"
     ]
    }
   ],
   "source": [
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "stock_data = data['AAPL']\n",
    "df = pd.DataFrame(stock_data).T\n",
    "\n",
    "data = df.sort_index(ascending=True, axis=0)\n",
    "\n",
    "train = data[:-2]\n",
    "valid = data[-2:]\n",
    "\n",
    "training = train['Close']\n",
    "validation = valid['Close']\n",
    "\n",
    "model = auto_arima(training, start_p=1, start_q=1,max_p=3, max_q=3, m=12,start_P=0, seasonal=True,d=1, D=1, trace=True,error_action='ignore',suppress_warnings=True)\n",
    "model.fit(training)\n",
    "\n",
    "forecast = model.predict(n_periods=len(valid))\n",
    "forecast = pd.DataFrame(forecast,index = valid.index,columns=['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(forecast['Prediction'],label='Prediction')\n",
    "plt.plot(valid['Close'], label='True value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SVOBBI~1\\AppData\\Local\\Temp/ipykernel_13036/2386101552.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#importing prophet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfbprophet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProphet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstock_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AAPL'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fbprophet'"
     ]
    }
   ],
   "source": [
    "#importing prophet\n",
    "from fbprophet import Prophet\n",
    "\n",
    "stock_data = data['AAPL']\n",
    "df = pd.DataFrame(stock_data).T\n",
    "\n",
    "#creating dataframe\n",
    "new_data = pd.DataFrame(index=range(0,len(df)),columns=['Date', 'Close'])\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    new_data['Date'][i] = data['Date'][i]\n",
    "    new_data['Close'][i] = data['Close'][i]\n",
    "\n",
    "new_data['Date'] = pd.to_datetime(new_data.Date,format='%Y-%m-%d')\n",
    "new_data.index = new_data['Date']\n",
    "\n",
    "#preparing data\n",
    "new_data.rename(columns={'Close': 'y', 'Date': 'ds'}, inplace=True)\n",
    "\n",
    "#train and validation\n",
    "train = new_data[:-10]\n",
    "valid = new_data[-10:]\n",
    "\n",
    "#fit the model\n",
    "model = Prophet()\n",
    "model.fit(train)\n",
    "\n",
    "#predictions\n",
    "close_prices = model.make_future_dataframe(periods=len(valid))\n",
    "forecast = model.predict(close_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom ANN (wih MA, H-L, O-C, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import os\n",
    "from firebase_admin import storage\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['AAPL', 'AMZN', 'AVGO', 'CSCO', 'FB', 'GOOG', 'GOOGL', 'MSFT', 'NVDA', 'TSLA'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0251 - val_loss: 6.2169e-04\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.6893e-04 - val_loss: 5.1575e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 2.8317e-04 - val_loss: 5.5462e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 2.3498e-04 - val_loss: 2.5689e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 2.0628e-04 - val_loss: 2.4210e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 2.1313e-04 - val_loss: 2.4511e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.9792e-04 - val_loss: 2.5153e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.8857e-04 - val_loss: 1.8776e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.6274e-04 - val_loss: 2.9878e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.7159e-04 - val_loss: 3.0600e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.7318e-04 - val_loss: 1.8479e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.4473e-04 - val_loss: 1.4452e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.5436e-04 - val_loss: 1.7844e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.7113e-04 - val_loss: 1.2822e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2482e-04 - val_loss: 2.1185e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.5218e-04 - val_loss: 1.4033e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6937e-04 - val_loss: 1.0847e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4820e-04 - val_loss: 1.3885e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2437e-04 - val_loss: 8.5589e-05\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3821e-04 - val_loss: 1.2464e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5595e-04 - val_loss: 1.1295e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3046e-04 - val_loss: 2.1224e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2413e-04 - val_loss: 7.1044e-05\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2730e-04 - val_loss: 1.0441e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2166e-04 - val_loss: 7.6708e-05\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6776e-04 - val_loss: 4.7085e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3276e-04 - val_loss: 1.4980e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2427e-04 - val_loss: 2.5587e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2761e-04 - val_loss: 1.6143e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4532e-04 - val_loss: 1.0660e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2452e-04 - val_loss: 9.5431e-05\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8363e-05 - val_loss: 7.2834e-05\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.3652e-04 - val_loss: 1.0398e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0119e-04 - val_loss: 1.1169e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2147e-04 - val_loss: 2.2692e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3885e-04 - val_loss: 8.8561e-05\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2675e-04 - val_loss: 1.1131e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1164e-04 - val_loss: 1.2084e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0422e-04 - val_loss: 7.1610e-05\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0386e-04 - val_loss: 1.3797e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1930e-04 - val_loss: 6.3445e-05\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1317e-04 - val_loss: 1.6170e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2910e-04 - val_loss: 1.6972e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2160e-04 - val_loss: 2.2060e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0697e-04 - val_loss: 7.2459e-05\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0949e-04 - val_loss: 8.4367e-05\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3328e-04 - val_loss: 9.0280e-05\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1212e-04 - val_loss: 2.0313e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0660e-04 - val_loss: 7.7732e-05\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8153e-05 - val_loss: 2.5344e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2280e-04 - val_loss: 1.1870e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0999e-04 - val_loss: 1.0901e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8050e-05 - val_loss: 1.4644e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0687e-04 - val_loss: 7.3014e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2256e-04 - val_loss: 8.0273e-05\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 9.2854e-05 - val_loss: 7.6858e-05\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1884e-04 - val_loss: 1.0973e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8715e-05 - val_loss: 5.4538e-05\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4565e-05 - val_loss: 5.9716e-05\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 8.2958e-05 - val_loss: 9.3364e-05\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2621e-04 - val_loss: 1.2758e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0261e-04 - val_loss: 6.4621e-05\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0806e-04 - val_loss: 8.6169e-05\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1021e-04 - val_loss: 1.3260e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1673e-04 - val_loss: 1.4411e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0149e-04 - val_loss: 1.5266e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2051e-04 - val_loss: 1.0661e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.4874e-05 - val_loss: 6.2257e-05\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.8527e-05 - val_loss: 6.4708e-05\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.2591e-05 - val_loss: 1.7838e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5516e-05 - val_loss: 6.1778e-05\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.9633e-05 - val_loss: 7.7326e-05\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.3652e-05 - val_loss: 7.1211e-05\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.9796e-05 - val_loss: 6.0358e-05\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.4945e-05 - val_loss: 8.1875e-05\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.3053e-04 - val_loss: 5.1560e-05\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2802e-05 - val_loss: 2.1672e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5942e-05 - val_loss: 6.7354e-05\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.9078e-05 - val_loss: 1.2008e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9792e-05 - val_loss: 1.1755e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0957e-04 - val_loss: 7.1221e-05\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3658e-05 - val_loss: 1.2347e-04\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.4316e-05 - val_loss: 9.0327e-05\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.5099e-05 - val_loss: 6.7066e-05\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0044e-05 - val_loss: 8.9639e-05\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 9.6963e-05 - val_loss: 1.0976e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0086e-04 - val_loss: 5.0299e-05\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.4871e-05 - val_loss: 4.8600e-05\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4560e-05 - val_loss: 8.7773e-05\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3316e-05 - val_loss: 1.2593e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0458e-04 - val_loss: 8.2475e-05\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6864e-05 - val_loss: 8.3455e-05\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1483e-05 - val_loss: 5.3653e-05\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5225e-05 - val_loss: 7.9249e-05\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2566e-04 - val_loss: 1.3419e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8997e-05 - val_loss: 6.2342e-05\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6148e-05 - val_loss: 1.9125e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6134e-05 - val_loss: 5.6642e-05\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3327e-05 - val_loss: 6.4227e-05\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0813e-05 - val_loss: 4.4258e-05\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0503e-05 - val_loss: 7.1024e-05\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.4871e-05 - val_loss: 8.6492e-05\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.0718e-05 - val_loss: 5.4723e-05\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.8303e-05 - val_loss: 7.9298e-05\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.8306e-05 - val_loss: 5.7159e-05\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0567e-04 - val_loss: 6.9901e-05\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.0842e-05 - val_loss: 7.7879e-05\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.6043e-05 - val_loss: 1.0988e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1280e-04 - val_loss: 6.5939e-05\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.4968e-05 - val_loss: 5.7706e-05\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.0186e-05 - val_loss: 9.5363e-05\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.6929e-05 - val_loss: 9.4585e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3534e-05 - val_loss: 4.8129e-05\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7759e-05 - val_loss: 4.8115e-05\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1214e-05 - val_loss: 4.1286e-05\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.7421e-05 - val_loss: 5.0365e-05\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8433e-05 - val_loss: 8.2381e-05\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8693e-05 - val_loss: 6.4602e-05\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.3351e-05 - val_loss: 2.1154e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.5232e-05 - val_loss: 7.1928e-05\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1488e-05 - val_loss: 6.1847e-05\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2093e-05 - val_loss: 5.7374e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0659e-04 - val_loss: 5.7209e-05\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4568e-05 - val_loss: 7.5540e-05\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0785e-05 - val_loss: 1.7332e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7870e-05 - val_loss: 1.4621e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0777e-04 - val_loss: 1.0721e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3366e-05 - val_loss: 1.0764e-04\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9757e-05 - val_loss: 7.2804e-05\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7357e-05 - val_loss: 6.9149e-05\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5816e-05 - val_loss: 1.4608e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2791e-05 - val_loss: 2.4999e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3310e-05 - val_loss: 9.2472e-05\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3016e-05 - val_loss: 5.1403e-05\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7419e-05 - val_loss: 1.0399e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0766e-04 - val_loss: 6.8898e-05\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6729e-05 - val_loss: 7.8793e-05\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5297e-05 - val_loss: 8.2653e-05\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4434e-05 - val_loss: 4.1301e-05\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4404e-05 - val_loss: 4.7515e-05\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5922e-05 - val_loss: 8.6397e-05\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6456e-05 - val_loss: 1.4743e-04\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5492e-05 - val_loss: 1.3045e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.9911e-05 - val_loss: 6.2636e-05\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1859e-05 - val_loss: 6.1281e-05\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7630e-05 - val_loss: 5.4757e-05\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7312e-05 - val_loss: 2.6570e-04\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7157e-05 - val_loss: 5.0606e-05\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.6946e-05 - val_loss: 4.3930e-05\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9464e-05 - val_loss: 7.4973e-05\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1056e-05 - val_loss: 7.6529e-05\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6086e-05 - val_loss: 1.4322e-04\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4264e-05 - val_loss: 4.9418e-05\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1626e-05 - val_loss: 9.5201e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0207e-05 - val_loss: 6.6910e-05\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0522e-05 - val_loss: 1.1988e-04\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1641e-05 - val_loss: 8.6454e-05\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9002e-05 - val_loss: 7.5037e-05\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6042e-05 - val_loss: 4.8455e-05\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1926e-05 - val_loss: 1.6888e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5936e-05 - val_loss: 7.7099e-05\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9476e-05 - val_loss: 6.7129e-05\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6317e-05 - val_loss: 6.7220e-05\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 6.7038e-05 - val_loss: 3.6347e-05\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0591e-04 - val_loss: 4.8036e-05\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.7970e-05 - val_loss: 4.4614e-05\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.4054e-05 - val_loss: 4.8230e-05\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.0592e-05 - val_loss: 4.7426e-05\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5059e-05 - val_loss: 6.7860e-05\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9473e-05 - val_loss: 6.1963e-05\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2871e-05 - val_loss: 6.0064e-05\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9231e-05 - val_loss: 9.1245e-05\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2044e-05 - val_loss: 8.9592e-05\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8909e-05 - val_loss: 9.3935e-05\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.1948e-05 - val_loss: 1.3760e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4682e-05 - val_loss: 4.4157e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5204e-05 - val_loss: 4.6648e-05\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2132e-05 - val_loss: 4.5217e-05\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.3205e-05 - val_loss: 5.7085e-05\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.6560e-05 - val_loss: 5.6463e-05\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/AAPL/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0544 - val_loss: 0.0020\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.2797e-04 - val_loss: 4.2703e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 3.7344e-04 - val_loss: 2.9372e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 2.9274e-04 - val_loss: 2.7322e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.6671e-04 - val_loss: 2.6701e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5689e-04 - val_loss: 2.3383e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.4717e-04 - val_loss: 2.3513e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 2.4611e-04 - val_loss: 2.2798e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.5762e-04 - val_loss: 3.4231e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5201e-04 - val_loss: 2.5524e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3774e-04 - val_loss: 2.0705e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3271e-04 - val_loss: 2.0153e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1188e-04 - val_loss: 2.3167e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1129e-04 - val_loss: 2.9724e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1005e-04 - val_loss: 1.6652e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1312e-04 - val_loss: 2.3870e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1116e-04 - val_loss: 1.9984e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5417e-04 - val_loss: 2.5504e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0471e-04 - val_loss: 1.5975e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0443e-04 - val_loss: 1.5743e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.2788e-04 - val_loss: 1.6048e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8892e-04 - val_loss: 2.4159e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0002e-04 - val_loss: 1.4877e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3306e-04 - val_loss: 2.0355e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3085e-04 - val_loss: 2.6035e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9495e-04 - val_loss: 2.0214e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8474e-04 - val_loss: 1.5274e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.2328e-04 - val_loss: 1.9992e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.8399e-04 - val_loss: 1.3952e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9448e-04 - val_loss: 2.7955e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.7812e-04 - val_loss: 2.0882e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7223e-04 - val_loss: 1.5278e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8268e-04 - val_loss: 2.1239e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8420e-04 - val_loss: 1.9063e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8331e-04 - val_loss: 1.7075e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5777e-04 - val_loss: 1.3519e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5844e-04 - val_loss: 1.2958e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.3371e-04 - val_loss: 1.4395e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8904e-04 - val_loss: 1.3750e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6126e-04 - val_loss: 1.3809e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4711e-04 - val_loss: 1.4142e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6082e-04 - val_loss: 1.7118e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4438e-04 - val_loss: 1.4773e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6733e-04 - val_loss: 2.3495e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8593e-04 - val_loss: 1.6387e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7478e-04 - val_loss: 1.2862e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7588e-04 - val_loss: 1.4431e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6540e-04 - val_loss: 1.5269e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4202e-04 - val_loss: 1.4782e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5321e-04 - val_loss: 1.4329e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3695e-04 - val_loss: 2.4870e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3756e-04 - val_loss: 2.7741e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3432e-04 - val_loss: 1.8807e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4137e-04 - val_loss: 1.4108e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2659e-04 - val_loss: 1.5020e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2429e-04 - val_loss: 1.5217e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4483e-04 - val_loss: 1.7652e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4703e-04 - val_loss: 4.4069e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6127e-04 - val_loss: 1.1410e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5604e-04 - val_loss: 1.4891e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1832e-04 - val_loss: 1.3565e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1504e-04 - val_loss: 1.6941e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2568e-04 - val_loss: 1.3933e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2257e-04 - val_loss: 1.2362e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9558e-04 - val_loss: 1.0925e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1994e-04 - val_loss: 1.5314e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1984e-04 - val_loss: 1.1283e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1595e-04 - val_loss: 1.6190e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1191e-04 - val_loss: 1.3438e-04\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2307e-04 - val_loss: 1.8351e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1699e-04 - val_loss: 1.0881e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0697e-04 - val_loss: 1.2081e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2042e-04 - val_loss: 1.5366e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2151e-04 - val_loss: 1.1871e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0744e-04 - val_loss: 1.1167e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4409e-04 - val_loss: 2.9698e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1661e-04 - val_loss: 1.0478e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1598e-04 - val_loss: 2.5947e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4143e-04 - val_loss: 2.5005e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.4646e-04 - val_loss: 2.5044e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1277e-04 - val_loss: 1.1355e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0870e-04 - val_loss: 9.6543e-05\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3831e-04 - val_loss: 1.3776e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1180e-04 - val_loss: 1.4117e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4372e-04 - val_loss: 9.4977e-05\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0441e-04 - val_loss: 1.8327e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0170e-04 - val_loss: 2.3418e-04\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2103e-04 - val_loss: 1.5875e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3063e-04 - val_loss: 2.2067e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0424e-04 - val_loss: 1.0727e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1242e-04 - val_loss: 2.6024e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2465e-04 - val_loss: 1.4589e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3679e-04 - val_loss: 2.2275e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1096e-04 - val_loss: 1.2310e-04\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9804e-05 - val_loss: 9.7552e-05\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1239e-04 - val_loss: 1.3509e-04\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3079e-04 - val_loss: 1.4317e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0472e-04 - val_loss: 2.8624e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1124e-04 - val_loss: 9.9874e-05\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1273e-04 - val_loss: 3.2120e-04\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5450e-04 - val_loss: 1.6679e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2761e-04 - val_loss: 1.8549e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0367e-04 - val_loss: 1.1522e-04\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2299e-04 - val_loss: 1.3875e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8648e-05 - val_loss: 1.3622e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3094e-04 - val_loss: 3.6922e-04\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1078e-04 - val_loss: 1.5139e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0164e-04 - val_loss: 1.6435e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0841e-04 - val_loss: 1.6260e-04\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0247e-04 - val_loss: 9.8876e-05\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0971e-04 - val_loss: 9.3799e-05\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1384e-04 - val_loss: 1.0684e-04\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1078e-04 - val_loss: 1.2767e-04\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1511e-04 - val_loss: 1.0185e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0844e-04 - val_loss: 1.6064e-04\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2113e-04 - val_loss: 1.2761e-04\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0268e-04 - val_loss: 1.7602e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3140e-04 - val_loss: 1.1659e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1623e-04 - val_loss: 1.2712e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2821e-04 - val_loss: 1.3671e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6908e-04 - val_loss: 1.0373e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7497e-05 - val_loss: 1.2285e-04\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0813e-04 - val_loss: 2.6846e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9287e-05 - val_loss: 1.3657e-04\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0579e-04 - val_loss: 1.1066e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9870e-05 - val_loss: 1.0443e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5303e-04 - val_loss: 2.2015e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2172e-04 - val_loss: 9.4626e-05\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1077e-04 - val_loss: 1.5305e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1270e-04 - val_loss: 1.0561e-04\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0242e-04 - val_loss: 9.6603e-05\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1358e-04 - val_loss: 1.4221e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1236e-04 - val_loss: 1.0536e-04\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5353e-04 - val_loss: 1.1018e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1393e-04 - val_loss: 2.0233e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7743e-05 - val_loss: 1.1023e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0836e-04 - val_loss: 9.6287e-05\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8779e-05 - val_loss: 1.4298e-04\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2454e-04 - val_loss: 2.2474e-04\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0176e-04 - val_loss: 1.0547e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0259e-04 - val_loss: 1.1340e-04\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0742e-04 - val_loss: 9.5549e-05\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2930e-04 - val_loss: 1.7085e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3233e-04 - val_loss: 1.0012e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8934e-05 - val_loss: 1.0904e-04\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0732e-04 - val_loss: 1.8861e-04\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4392e-05 - val_loss: 1.0044e-04\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3793e-04 - val_loss: 9.1228e-05\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8784e-05 - val_loss: 9.0193e-05\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1139e-04 - val_loss: 1.8167e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9265e-05 - val_loss: 1.3100e-04\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9007e-05 - val_loss: 9.4984e-05\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1118e-04 - val_loss: 2.2205e-04\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0167e-04 - val_loss: 8.7601e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0264e-04 - val_loss: 1.0930e-04\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1853e-05 - val_loss: 1.0128e-04\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2665e-04 - val_loss: 1.1725e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1319e-04 - val_loss: 1.2221e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3493e-05 - val_loss: 9.4333e-05\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0444e-04 - val_loss: 1.0553e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2143e-05 - val_loss: 1.0155e-04\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0565e-04 - val_loss: 1.0279e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1261e-04 - val_loss: 9.8664e-05\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5796e-05 - val_loss: 1.5261e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2314e-04 - val_loss: 1.6912e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1947e-04 - val_loss: 9.2549e-05\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0096e-04 - val_loss: 1.3577e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6372e-05 - val_loss: 9.8115e-05\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2176e-04 - val_loss: 1.0117e-04\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0523e-04 - val_loss: 1.0687e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8095e-05 - val_loss: 2.5924e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0350e-04 - val_loss: 1.1756e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1785e-04 - val_loss: 1.0942e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0971e-05 - val_loss: 1.0535e-04\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3168e-05 - val_loss: 1.1859e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0582e-04 - val_loss: 1.4361e-04\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0277e-04 - val_loss: 1.1296e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6090e-05 - val_loss: 2.9840e-04\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0227e-04 - val_loss: 1.0515e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7222e-05 - val_loss: 1.1511e-04\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/AMZN/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0254 - val_loss: 7.8946e-04\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9757e-04 - val_loss: 5.1181e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 4.9650e-04 - val_loss: 2.5958e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 3.8895e-04 - val_loss: 2.2083e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.7840e-04 - val_loss: 5.3844e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.5054e-04 - val_loss: 3.2777e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 3.3533e-04 - val_loss: 2.2985e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 3.1141e-04 - val_loss: 2.4773e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.1170e-04 - val_loss: 4.1834e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.0788e-04 - val_loss: 3.0247e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.9175e-04 - val_loss: 1.7254e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.9750e-04 - val_loss: 1.6870e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.1364e-04 - val_loss: 2.3496e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 3.1561e-04 - val_loss: 2.1814e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.6515e-04 - val_loss: 1.9568e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 3.2955e-04 - val_loss: 1.8270e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.6881e-04 - val_loss: 2.1735e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.8189e-04 - val_loss: 1.7296e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.8768e-04 - val_loss: 3.1520e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.7611e-04 - val_loss: 1.4682e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2423e-04 - val_loss: 2.6704e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5212e-04 - val_loss: 1.9334e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.2902e-04 - val_loss: 1.6526e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3004e-04 - val_loss: 1.7544e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5309e-04 - val_loss: 1.5548e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3969e-04 - val_loss: 1.5831e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0584e-04 - val_loss: 1.5150e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5271e-04 - val_loss: 1.4349e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9623e-04 - val_loss: 4.2243e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3972e-04 - val_loss: 2.2890e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0562e-04 - val_loss: 1.2364e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1584e-04 - val_loss: 1.2118e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9233e-04 - val_loss: 1.6140e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0852e-04 - val_loss: 1.2967e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9669e-04 - val_loss: 1.4399e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.1866e-04 - val_loss: 1.4887e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8372e-04 - val_loss: 2.0980e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6379e-04 - val_loss: 1.5674e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.4231e-04 - val_loss: 6.7193e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9428e-04 - val_loss: 1.7105e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7861e-04 - val_loss: 1.2137e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0437e-04 - val_loss: 1.7577e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6485e-04 - val_loss: 1.8195e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6743e-04 - val_loss: 1.9618e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6491e-04 - val_loss: 1.1152e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6554e-04 - val_loss: 1.1637e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5174e-04 - val_loss: 1.1982e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7461e-04 - val_loss: 2.2561e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4346e-04 - val_loss: 1.1846e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4094e-04 - val_loss: 1.2775e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5394e-04 - val_loss: 3.2803e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4390e-04 - val_loss: 1.1380e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2245e-04 - val_loss: 1.3393e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1039e-04 - val_loss: 5.1381e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.8045e-04 - val_loss: 1.6073e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4478e-04 - val_loss: 1.1763e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1709e-04 - val_loss: 1.7876e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3328e-04 - val_loss: 1.5401e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2214e-04 - val_loss: 1.2505e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2739e-04 - val_loss: 1.7903e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3591e-04 - val_loss: 1.1025e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3317e-04 - val_loss: 1.5098e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2540e-04 - val_loss: 1.2211e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4074e-04 - val_loss: 1.2768e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2710e-04 - val_loss: 1.1619e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3009e-04 - val_loss: 1.2274e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3595e-04 - val_loss: 1.8483e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1539e-04 - val_loss: 1.5380e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2831e-04 - val_loss: 1.2983e-04\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1592e-04 - val_loss: 1.2131e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4068e-04 - val_loss: 1.1197e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1129e-04 - val_loss: 9.9433e-05\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1448e-04 - val_loss: 9.3042e-05\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3626e-04 - val_loss: 9.8370e-05\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1951e-04 - val_loss: 9.6222e-05\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2439e-04 - val_loss: 1.1465e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2086e-04 - val_loss: 1.1524e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1542e-04 - val_loss: 2.9512e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2798e-04 - val_loss: 1.2850e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1781e-04 - val_loss: 9.2537e-05\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1854e-04 - val_loss: 1.1127e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4233e-04 - val_loss: 1.0958e-04\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0030e-04 - val_loss: 2.2365e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.3508e-04 - val_loss: 1.3970e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1435e-04 - val_loss: 1.3972e-04\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2054e-04 - val_loss: 1.0962e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.1507e-04 - val_loss: 1.1001e-04\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0797e-04 - val_loss: 1.1805e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5724e-04 - val_loss: 1.5791e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2912e-04 - val_loss: 1.5268e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6648e-04 - val_loss: 8.8791e-05\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1464e-04 - val_loss: 1.5159e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1805e-04 - val_loss: 1.0370e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0049e-04 - val_loss: 9.9212e-05\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3649e-04 - val_loss: 1.4542e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4067e-04 - val_loss: 1.3768e-04\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9206e-05 - val_loss: 9.5733e-05\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0402e-04 - val_loss: 1.0260e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.2321e-04 - val_loss: 1.6497e-04\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2282e-04 - val_loss: 8.7661e-05\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0898e-04 - val_loss: 1.6988e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2562e-04 - val_loss: 1.0378e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0712e-04 - val_loss: 9.6103e-05\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1310e-04 - val_loss: 1.0606e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0502e-04 - val_loss: 9.5744e-05\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1181e-04 - val_loss: 8.6975e-05\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2521e-04 - val_loss: 1.6630e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2961e-04 - val_loss: 8.7896e-05\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4938e-04 - val_loss: 1.3946e-04\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1762e-04 - val_loss: 1.8414e-04\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3161e-04 - val_loss: 1.1151e-04\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1304e-04 - val_loss: 9.6155e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6736e-05 - val_loss: 1.7863e-04\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1636e-04 - val_loss: 8.8282e-05\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1273e-04 - val_loss: 8.9042e-05\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.2476e-04 - val_loss: 2.5681e-04\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.4103e-04 - val_loss: 1.4463e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1176e-04 - val_loss: 1.2863e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0837e-04 - val_loss: 1.1084e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1049e-04 - val_loss: 1.0374e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1321e-04 - val_loss: 1.8240e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0749e-04 - val_loss: 9.9296e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8126e-05 - val_loss: 9.9917e-05\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2672e-04 - val_loss: 9.6340e-05\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0960e-04 - val_loss: 2.0869e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0578e-04 - val_loss: 8.5367e-05\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0167e-04 - val_loss: 1.6633e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3053e-04 - val_loss: 1.7683e-04\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4366e-04 - val_loss: 1.0714e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0332e-04 - val_loss: 8.5889e-05\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.2322e-04 - val_loss: 2.8223e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0468e-04 - val_loss: 1.0572e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1222e-04 - val_loss: 8.4410e-05\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0214e-04 - val_loss: 1.1794e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4735e-05 - val_loss: 9.7393e-05\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2038e-04 - val_loss: 1.0336e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0727e-04 - val_loss: 1.0878e-04\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0321e-04 - val_loss: 9.0268e-05\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.6326e-05 - val_loss: 9.5057e-05\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2306e-04 - val_loss: 1.7311e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1063e-04 - val_loss: 1.4302e-04\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1230e-04 - val_loss: 9.2521e-05\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1304e-04 - val_loss: 1.0488e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2981e-04 - val_loss: 1.1914e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0975e-04 - val_loss: 1.7077e-04\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0890e-04 - val_loss: 9.4916e-05\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1749e-05 - val_loss: 9.9529e-05\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1553e-04 - val_loss: 1.0416e-04\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0873e-04 - val_loss: 1.7071e-04\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5188e-05 - val_loss: 1.6959e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0264e-04 - val_loss: 1.8521e-04\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0223e-04 - val_loss: 1.0742e-04\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3047e-04 - val_loss: 1.9801e-04\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1911e-04 - val_loss: 9.6343e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.0092e-04 - val_loss: 9.7452e-05\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.1767e-05 - val_loss: 1.1285e-04\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9825e-05 - val_loss: 1.6093e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0532e-04 - val_loss: 1.0380e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1933e-04 - val_loss: 9.4065e-05\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0846e-04 - val_loss: 1.3948e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1342e-04 - val_loss: 8.8482e-05\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1185e-04 - val_loss: 1.0558e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1723e-04 - val_loss: 1.7435e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4835e-05 - val_loss: 9.1228e-05\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.9872e-05 - val_loss: 8.9273e-05\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0272e-04 - val_loss: 1.1012e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.3332e-04 - val_loss: 1.5029e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.2069e-04 - val_loss: 4.5077e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1580e-04 - val_loss: 2.1490e-04\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.2492e-04 - val_loss: 1.3457e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.5985e-05 - val_loss: 1.8591e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.2027e-04 - val_loss: 1.3830e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.7461e-05 - val_loss: 2.3200e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1544e-04 - val_loss: 9.5757e-05\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0361e-04 - val_loss: 1.0801e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7654e-05 - val_loss: 9.0459e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0616e-04 - val_loss: 1.0387e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7869e-05 - val_loss: 8.8284e-05\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1902e-05 - val_loss: 1.0280e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1290e-04 - val_loss: 9.1092e-05\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/AVGO/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0497 - val_loss: 0.0017\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0012 - val_loss: 8.4610e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0010 - val_loss: 7.5672e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7538e-04 - val_loss: 9.3914e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 6.9589e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1185e-04 - val_loss: 6.8196e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5960e-04 - val_loss: 0.0012\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8098e-04 - val_loss: 6.4522e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.1585e-04 - val_loss: 6.4346e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.0373e-04 - val_loss: 6.4480e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.8679e-04 - val_loss: 7.8261e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.8200e-04 - val_loss: 6.1898e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.3705e-04 - val_loss: 7.2149e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.4525e-04 - val_loss: 6.3886e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.2767e-04 - val_loss: 6.2351e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.9330e-04 - val_loss: 0.0010\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.9663e-04 - val_loss: 6.0070e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.9712e-04 - val_loss: 6.2640e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9944e-04 - val_loss: 5.9919e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5086e-04 - val_loss: 5.7274e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9202e-04 - val_loss: 5.9193e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6599e-04 - val_loss: 7.4484e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6713e-04 - val_loss: 6.0364e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7201e-04 - val_loss: 5.7415e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4088e-04 - val_loss: 6.5696e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9290e-04 - val_loss: 7.2216e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 7.8384e-04 - val_loss: 0.0013\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1511e-04 - val_loss: 5.8215e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.7547e-04 - val_loss: 5.6449e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7995e-04 - val_loss: 5.4097e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.3215e-04 - val_loss: 5.4263e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.2152e-04 - val_loss: 6.2747e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.2159e-04 - val_loss: 5.4545e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7275e-04 - val_loss: 7.6526e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5193e-04 - val_loss: 6.0696e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3425e-04 - val_loss: 6.8295e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4232e-04 - val_loss: 7.9317e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8005e-04 - val_loss: 5.2676e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9480e-04 - val_loss: 0.0010\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0029e-04 - val_loss: 5.4448e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1280e-04 - val_loss: 6.0907e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0379e-04 - val_loss: 5.5810e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1312e-04 - val_loss: 5.0832e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0840e-04 - val_loss: 7.0679e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5209e-04 - val_loss: 4.9503e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 7.4424e-04 - val_loss: 5.9683e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9309e-04 - val_loss: 8.0515e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6385e-04 - val_loss: 8.3236e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0296e-04 - val_loss: 5.2913e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1012e-04 - val_loss: 6.8750e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9566e-04 - val_loss: 5.1698e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8489e-04 - val_loss: 5.3766e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3582e-04 - val_loss: 5.0242e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4489e-04 - val_loss: 5.2838e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3681e-04 - val_loss: 5.3977e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2577e-04 - val_loss: 4.9420e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1268e-04 - val_loss: 7.6628e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.4795e-04 - val_loss: 4.8035e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.8223e-04 - val_loss: 5.5388e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.6377e-04 - val_loss: 6.8530e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.4154e-04 - val_loss: 4.7157e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.6946e-04 - val_loss: 4.9696e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.4406e-04 - val_loss: 5.2961e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.0082e-04 - val_loss: 4.6550e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.2109e-04 - val_loss: 4.7476e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.7383e-04 - val_loss: 4.7885e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.4089e-04 - val_loss: 4.6207e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.3673e-04 - val_loss: 5.4771e-04\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4317e-04 - val_loss: 4.7472e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6259e-04 - val_loss: 7.4089e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6019e-04 - val_loss: 5.5427e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2799e-04 - val_loss: 5.1404e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3551e-04 - val_loss: 4.9775e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9145e-04 - val_loss: 5.1131e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7459e-04 - val_loss: 5.1706e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2984e-04 - val_loss: 6.9087e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 7.3358e-04 - val_loss: 4.6783e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0911e-04 - val_loss: 5.3423e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.0150e-04 - val_loss: 4.6309e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.2734e-04 - val_loss: 4.5514e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2528e-04 - val_loss: 7.0607e-04\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9938e-04 - val_loss: 4.8778e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0727e-04 - val_loss: 6.1706e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8008e-04 - val_loss: 4.6781e-04\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.4553e-04 - val_loss: 6.6908e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4920e-04 - val_loss: 4.7201e-04\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6150e-04 - val_loss: 4.6281e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0265e-04 - val_loss: 5.9643e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1775e-04 - val_loss: 4.5661e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2259e-04 - val_loss: 6.7977e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5192e-04 - val_loss: 4.6332e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3067e-04 - val_loss: 5.6900e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4095e-04 - val_loss: 4.5246e-04\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4963e-04 - val_loss: 4.8480e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6277e-04 - val_loss: 4.6534e-04\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4587e-04 - val_loss: 4.5208e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4695e-04 - val_loss: 4.6195e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8610e-04 - val_loss: 6.3700e-04\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.4716e-04 - val_loss: 5.8551e-04\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8614e-04 - val_loss: 4.4567e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.2831e-04 - val_loss: 4.8412e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.4260e-04 - val_loss: 4.4558e-04\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.1255e-04 - val_loss: 4.8725e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.1577e-04 - val_loss: 4.5905e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.1737e-04 - val_loss: 4.9049e-04\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.4506e-04 - val_loss: 5.0953e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.1825e-04 - val_loss: 4.3300e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.5392e-04 - val_loss: 4.6275e-04\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.2883e-04 - val_loss: 5.0756e-04\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4953e-04 - val_loss: 4.5236e-04\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0753e-04 - val_loss: 4.5029e-04\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2260e-04 - val_loss: 5.2291e-04\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8117e-04 - val_loss: 5.3345e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6514e-04 - val_loss: 4.9899e-04\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2579e-04 - val_loss: 0.0012\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3281e-04 - val_loss: 4.9201e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7218e-04 - val_loss: 5.3002e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3348e-04 - val_loss: 8.2933e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4190e-04 - val_loss: 5.5832e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3782e-04 - val_loss: 5.3223e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4443e-04 - val_loss: 4.5037e-04\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.3475e-04 - val_loss: 4.5053e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1382e-04 - val_loss: 9.8928e-04\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6018e-04 - val_loss: 4.6229e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4986e-04 - val_loss: 4.4442e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3103e-04 - val_loss: 5.2027e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2768e-04 - val_loss: 4.3643e-04\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2984e-04 - val_loss: 4.7712e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0345e-04 - val_loss: 4.4953e-04\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0066e-04 - val_loss: 4.6767e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2883e-04 - val_loss: 4.6338e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5348e-04 - val_loss: 4.7375e-04\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3882e-04 - val_loss: 5.6943e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0156e-04 - val_loss: 4.3749e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1384e-04 - val_loss: 7.5005e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4386e-04 - val_loss: 4.6215e-04\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0322e-04 - val_loss: 8.4358e-04\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5564e-04 - val_loss: 6.3180e-04\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3041e-04 - val_loss: 5.2516e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2537e-04 - val_loss: 4.6315e-04\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7684e-04 - val_loss: 5.3397e-04\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8505e-04 - val_loss: 4.4931e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0131e-04 - val_loss: 4.6591e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4482e-04 - val_loss: 4.9326e-04\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1497e-04 - val_loss: 5.9000e-04\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.1840e-04 - val_loss: 4.3934e-04\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2154e-04 - val_loss: 4.5225e-04\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9790e-04 - val_loss: 4.4702e-04\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0873e-04 - val_loss: 4.8811e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2220e-04 - val_loss: 5.5747e-04\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6159e-04 - val_loss: 5.0035e-04\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5174e-04 - val_loss: 0.0011\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.3803e-04 - val_loss: 4.6183e-04\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1579e-04 - val_loss: 4.4589e-04\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0808e-04 - val_loss: 4.5194e-04\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0948e-04 - val_loss: 4.5503e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2794e-04 - val_loss: 5.6274e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1893e-04 - val_loss: 6.5964e-04\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3735e-04 - val_loss: 4.4461e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0353e-04 - val_loss: 4.7912e-04\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9257e-04 - val_loss: 4.8364e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0367e-04 - val_loss: 4.6537e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0177e-04 - val_loss: 5.3576e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4809e-04 - val_loss: 4.4096e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.3082e-04 - val_loss: 4.4061e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4397e-04 - val_loss: 4.6458e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8795e-04 - val_loss: 4.4065e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1738e-04 - val_loss: 4.4489e-04\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9214e-04 - val_loss: 4.3021e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2972e-04 - val_loss: 5.0517e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0998e-04 - val_loss: 4.3708e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4052e-04 - val_loss: 5.8924e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7691e-04 - val_loss: 4.8218e-04\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6154e-04 - val_loss: 4.5407e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0670e-04 - val_loss: 4.5038e-04\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9834e-04 - val_loss: 4.5372e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3038e-04 - val_loss: 5.4934e-04\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9887e-04 - val_loss: 4.2065e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.1039e-04 - val_loss: 4.5041e-04\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/CSCO/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0436 - val_loss: 0.0018\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0011 - val_loss: 5.9133e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 4.3914e-04 - val_loss: 3.0381e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 3.0250e-04 - val_loss: 2.0190e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.7095e-04 - val_loss: 1.7121e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.4709e-04 - val_loss: 1.9852e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5393e-04 - val_loss: 1.7266e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.5588e-04 - val_loss: 2.3287e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.7057e-04 - val_loss: 1.7677e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3490e-04 - val_loss: 3.3001e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.4725e-04 - val_loss: 1.9052e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.4418e-04 - val_loss: 1.6146e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9990e-04 - val_loss: 1.6531e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.2920e-04 - val_loss: 1.6652e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1699e-04 - val_loss: 3.4211e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 2.6069e-04 - val_loss: 1.8886e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 2.1326e-04 - val_loss: 1.5066e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.7988e-04 - val_loss: 1.6997e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.9828e-04 - val_loss: 1.6105e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.9555e-04 - val_loss: 1.5623e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.8267e-04 - val_loss: 1.6666e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.6001e-04 - val_loss: 1.5002e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8428e-04 - val_loss: 1.4892e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7540e-04 - val_loss: 1.7368e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0536e-04 - val_loss: 2.8764e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.0870e-04 - val_loss: 1.9782e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7956e-04 - val_loss: 2.4067e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1660e-04 - val_loss: 2.9646e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8712e-04 - val_loss: 1.7995e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6466e-04 - val_loss: 1.2839e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5938e-04 - val_loss: 1.4699e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5410e-04 - val_loss: 1.1188e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5576e-04 - val_loss: 2.1501e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6445e-04 - val_loss: 1.3428e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6236e-04 - val_loss: 2.0367e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3082e-04 - val_loss: 1.7701e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7000e-04 - val_loss: 1.4498e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5992e-04 - val_loss: 1.9230e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.4510e-04 - val_loss: 1.9675e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4856e-04 - val_loss: 1.0887e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5036e-04 - val_loss: 1.4743e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5683e-04 - val_loss: 1.9266e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6949e-04 - val_loss: 1.0562e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4551e-04 - val_loss: 1.0978e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4051e-04 - val_loss: 1.2345e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5937e-04 - val_loss: 1.1348e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2639e-04 - val_loss: 1.7339e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4440e-04 - val_loss: 1.1983e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5603e-04 - val_loss: 1.1596e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3598e-04 - val_loss: 1.2105e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5099e-04 - val_loss: 1.4025e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3856e-04 - val_loss: 1.0453e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3886e-04 - val_loss: 1.1965e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3681e-04 - val_loss: 1.2907e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3043e-04 - val_loss: 2.3366e-04\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3103e-04 - val_loss: 1.7745e-04\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4731e-04 - val_loss: 1.0617e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2570e-04 - val_loss: 1.0665e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4694e-04 - val_loss: 3.5880e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.6018e-04 - val_loss: 1.4848e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4161e-04 - val_loss: 1.3388e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5082e-04 - val_loss: 1.5963e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2800e-04 - val_loss: 1.1405e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4004e-04 - val_loss: 1.4235e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5196e-04 - val_loss: 1.2157e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2403e-04 - val_loss: 1.5829e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2987e-04 - val_loss: 1.3120e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2804e-04 - val_loss: 1.5571e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.3067e-04 - val_loss: 1.5501e-04\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3243e-04 - val_loss: 1.4875e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3386e-04 - val_loss: 1.1109e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3532e-04 - val_loss: 2.2353e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2025e-04 - val_loss: 1.5505e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1413e-04 - val_loss: 1.5178e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4363e-04 - val_loss: 1.9726e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2323e-04 - val_loss: 1.2305e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3468e-04 - val_loss: 1.7936e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2129e-04 - val_loss: 1.5485e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4086e-04 - val_loss: 1.9426e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3660e-04 - val_loss: 1.5877e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1871e-04 - val_loss: 1.3797e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2557e-04 - val_loss: 1.4529e-04\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4439e-04 - val_loss: 1.3958e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3840e-04 - val_loss: 1.1193e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1962e-04 - val_loss: 1.0694e-04\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2603e-04 - val_loss: 1.2484e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3364e-04 - val_loss: 1.1163e-04\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2654e-04 - val_loss: 1.0128e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3013e-04 - val_loss: 1.1926e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3480e-04 - val_loss: 1.2650e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4141e-04 - val_loss: 1.1369e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2961e-04 - val_loss: 1.1923e-04\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1816e-04 - val_loss: 1.0899e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3329e-04 - val_loss: 3.2095e-04\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5478e-04 - val_loss: 2.7133e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0740e-04 - val_loss: 1.3421e-04\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2578e-04 - val_loss: 1.1578e-04\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4462e-04 - val_loss: 1.2914e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3691e-04 - val_loss: 1.3166e-04\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3207e-04 - val_loss: 1.3467e-04\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5540e-04 - val_loss: 1.4753e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1376e-04 - val_loss: 1.1629e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1396e-04 - val_loss: 1.2576e-04\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1341e-04 - val_loss: 1.4083e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0981e-04 - val_loss: 1.0879e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1801e-04 - val_loss: 1.0580e-04\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0721e-04 - val_loss: 1.1520e-04\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2014e-04 - val_loss: 2.4398e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1959e-04 - val_loss: 1.1150e-04\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2972e-04 - val_loss: 1.6160e-04\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2599e-04 - val_loss: 1.1231e-04\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3145e-04 - val_loss: 1.3913e-04\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4566e-04 - val_loss: 1.1226e-04\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2102e-04 - val_loss: 2.6216e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2486e-04 - val_loss: 1.2687e-04\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2763e-04 - val_loss: 2.7475e-04\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3136e-04 - val_loss: 1.3855e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3653e-04 - val_loss: 2.3140e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3224e-04 - val_loss: 1.1223e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3281e-04 - val_loss: 1.0418e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1149e-04 - val_loss: 1.8903e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1292e-04 - val_loss: 1.0155e-04\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1681e-04 - val_loss: 1.2187e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2313e-04 - val_loss: 1.5235e-04\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3008e-04 - val_loss: 9.8559e-05\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2633e-04 - val_loss: 1.1840e-04\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0341e-04 - val_loss: 1.2283e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2616e-04 - val_loss: 1.6615e-04\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3590e-04 - val_loss: 2.0929e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1080e-04 - val_loss: 1.0646e-04\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3556e-04 - val_loss: 1.4043e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1684e-04 - val_loss: 1.1908e-04\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2834e-04 - val_loss: 1.0068e-04\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2413e-04 - val_loss: 1.4299e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3683e-04 - val_loss: 1.2689e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2400e-04 - val_loss: 1.2460e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2182e-04 - val_loss: 1.0103e-04\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1268e-04 - val_loss: 1.7056e-04\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3403e-04 - val_loss: 2.1683e-04\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4095e-04 - val_loss: 1.2243e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0711e-04 - val_loss: 1.9598e-04\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3040e-04 - val_loss: 1.0084e-04\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1295e-04 - val_loss: 1.6930e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1964e-04 - val_loss: 1.1548e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2340e-04 - val_loss: 1.1531e-04\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3153e-04 - val_loss: 1.5238e-04\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2859e-04 - val_loss: 1.3832e-04\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1682e-04 - val_loss: 1.8319e-04\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5158e-04 - val_loss: 1.2143e-04\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1971e-04 - val_loss: 1.0674e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2225e-04 - val_loss: 1.0840e-04\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3765e-04 - val_loss: 1.4291e-04\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3272e-04 - val_loss: 1.0305e-04\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0293e-04 - val_loss: 1.1731e-04\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1426e-04 - val_loss: 1.0971e-04\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0321e-04 - val_loss: 1.6996e-04\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1361e-04 - val_loss: 1.1330e-04\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1489e-04 - val_loss: 1.6729e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2424e-04 - val_loss: 1.2137e-04\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1597e-04 - val_loss: 1.0216e-04\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0818e-04 - val_loss: 1.7336e-04\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1084e-04 - val_loss: 1.1757e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1615e-04 - val_loss: 1.5208e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1940e-04 - val_loss: 3.0453e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1468e-04 - val_loss: 1.3068e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3182e-04 - val_loss: 2.0524e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6740e-04 - val_loss: 2.0683e-04\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2738e-04 - val_loss: 1.7514e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0471e-04 - val_loss: 1.0805e-04\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3134e-04 - val_loss: 2.0818e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2933e-04 - val_loss: 1.1649e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2779e-04 - val_loss: 1.3452e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1361e-04 - val_loss: 1.1988e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1389e-04 - val_loss: 1.0323e-04\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1379e-04 - val_loss: 1.2650e-04\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4286e-04 - val_loss: 1.1306e-04\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0301e-04 - val_loss: 1.2436e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1571e-04 - val_loss: 1.2700e-04\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1867e-04 - val_loss: 1.4656e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0663e-04 - val_loss: 1.2247e-04\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/FB/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0335 - val_loss: 0.0019\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.5621e-04 - val_loss: 3.0617e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 2.4035e-04 - val_loss: 1.3388e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6776e-04 - val_loss: 1.1692e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5154e-04 - val_loss: 1.0221e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4498e-04 - val_loss: 1.0808e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4031e-04 - val_loss: 1.6845e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4087e-04 - val_loss: 1.6279e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3073e-04 - val_loss: 9.5788e-05\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3080e-04 - val_loss: 1.7504e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3278e-04 - val_loss: 1.1519e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2406e-04 - val_loss: 7.8455e-05\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2379e-04 - val_loss: 7.9697e-05\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2524e-04 - val_loss: 7.8300e-05\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1860e-04 - val_loss: 8.5488e-05\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3256e-04 - val_loss: 1.4559e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1468e-04 - val_loss: 9.0716e-05\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2766e-04 - val_loss: 6.3751e-05\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0802e-04 - val_loss: 7.2616e-05\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0936e-04 - val_loss: 1.0162e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0122e-04 - val_loss: 1.3285e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1304e-04 - val_loss: 1.1344e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4289e-05 - val_loss: 8.6476e-05\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1823e-04 - val_loss: 6.7614e-05\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2021e-04 - val_loss: 6.8958e-05\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1888e-04 - val_loss: 1.5629e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1191e-04 - val_loss: 4.1739e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2818e-04 - val_loss: 1.0651e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1279e-04 - val_loss: 6.7392e-05\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.9409e-05 - val_loss: 6.0726e-05\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1951e-05 - val_loss: 9.3427e-05\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1552e-05 - val_loss: 2.0668e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3574e-04 - val_loss: 2.3156e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0104e-04 - val_loss: 7.6397e-05\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1693e-04 - val_loss: 8.2450e-05\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0629e-04 - val_loss: 5.5378e-05\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4019e-05 - val_loss: 5.7651e-05\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8888e-05 - val_loss: 1.3610e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6274e-05 - val_loss: 1.1822e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5303e-05 - val_loss: 4.8809e-05\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0050e-04 - val_loss: 4.7531e-05\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3228e-05 - val_loss: 1.2451e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0084e-04 - val_loss: 7.2971e-05\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0130e-05 - val_loss: 5.7496e-05\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6935e-05 - val_loss: 4.9290e-05\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.9375e-05 - val_loss: 3.3718e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0751e-04 - val_loss: 6.8652e-05\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0105e-04 - val_loss: 5.7445e-05\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9495e-05 - val_loss: 4.6002e-05\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6889e-05 - val_loss: 5.8617e-05\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0005e-04 - val_loss: 6.9977e-05\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5383e-05 - val_loss: 5.4694e-05\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6328e-05 - val_loss: 1.1128e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3176e-05 - val_loss: 5.1938e-05\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9048e-05 - val_loss: 4.7945e-05\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6165e-05 - val_loss: 9.1257e-05\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6645e-05 - val_loss: 6.7340e-05\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6095e-05 - val_loss: 1.3345e-04\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5352e-05 - val_loss: 4.0888e-05\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9712e-05 - val_loss: 8.3062e-05\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7808e-05 - val_loss: 4.3363e-05\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6514e-05 - val_loss: 5.1832e-05\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4869e-05 - val_loss: 1.2314e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1009e-05 - val_loss: 3.6354e-05\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8512e-05 - val_loss: 6.4009e-05\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7159e-05 - val_loss: 4.3710e-05\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5219e-05 - val_loss: 5.5300e-05\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1778e-05 - val_loss: 3.8567e-05\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8777e-05 - val_loss: 9.8518e-05\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8750e-05 - val_loss: 1.1746e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8202e-05 - val_loss: 7.8774e-05\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3186e-05 - val_loss: 1.2654e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2469e-05 - val_loss: 1.3856e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3473e-05 - val_loss: 9.4817e-05\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0880e-05 - val_loss: 6.1199e-05\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9304e-05 - val_loss: 4.1227e-05\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3110e-05 - val_loss: 5.5179e-05\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9020e-05 - val_loss: 3.1679e-04\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5773e-05 - val_loss: 1.4663e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0964e-04 - val_loss: 5.4564e-05\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.2503e-05 - val_loss: 3.6221e-05\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2909e-05 - val_loss: 4.6915e-05\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7477e-05 - val_loss: 8.0322e-05\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9840e-05 - val_loss: 4.7643e-05\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7034e-05 - val_loss: 7.4953e-05\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2170e-05 - val_loss: 4.3112e-05\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4348e-05 - val_loss: 5.1362e-05\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6313e-05 - val_loss: 1.3100e-04\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2205e-05 - val_loss: 5.8401e-05\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2349e-05 - val_loss: 3.8197e-05\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8330e-05 - val_loss: 1.1286e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0148e-04 - val_loss: 7.3774e-05\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6306e-05 - val_loss: 3.5096e-05\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7471e-05 - val_loss: 4.3816e-05\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1482e-05 - val_loss: 8.2661e-05\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0305e-05 - val_loss: 7.1598e-05\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9291e-05 - val_loss: 3.5859e-05\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1583e-05 - val_loss: 5.6836e-05\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2014e-05 - val_loss: 5.5422e-05\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7499e-05 - val_loss: 3.4700e-05\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9274e-05 - val_loss: 3.4558e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0618e-05 - val_loss: 5.8446e-05\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1485e-05 - val_loss: 6.8283e-05\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8773e-05 - val_loss: 8.1613e-05\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4567e-05 - val_loss: 3.6242e-05\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8629e-05 - val_loss: 5.8684e-05\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8925e-05 - val_loss: 4.8514e-05\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0953e-05 - val_loss: 4.4469e-05\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1869e-05 - val_loss: 6.5864e-05\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8547e-05 - val_loss: 3.3757e-05\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8603e-05 - val_loss: 3.5513e-05\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5868e-05 - val_loss: 9.9674e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5250e-05 - val_loss: 5.4011e-05\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4557e-05 - val_loss: 5.1719e-05\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6240e-05 - val_loss: 1.0598e-04\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5281e-05 - val_loss: 7.9074e-05\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5195e-05 - val_loss: 3.9443e-05\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2995e-05 - val_loss: 1.0890e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6088e-05 - val_loss: 4.4128e-05\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0768e-05 - val_loss: 3.6928e-05\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3971e-05 - val_loss: 1.3798e-04\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2139e-05 - val_loss: 4.5590e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9112e-05 - val_loss: 1.0627e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9372e-05 - val_loss: 1.5642e-04\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9290e-05 - val_loss: 2.6430e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6174e-05 - val_loss: 5.3137e-05\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1272e-05 - val_loss: 4.5333e-05\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1153e-05 - val_loss: 4.4735e-05\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5042e-05 - val_loss: 1.0165e-04\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8291e-05 - val_loss: 1.5443e-04\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7182e-05 - val_loss: 1.5137e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1644e-05 - val_loss: 6.7641e-05\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2422e-05 - val_loss: 8.2396e-05\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3739e-05 - val_loss: 4.0801e-05\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1642e-05 - val_loss: 4.0479e-05\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6665e-05 - val_loss: 3.9411e-05\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8315e-05 - val_loss: 5.7447e-05\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4815e-05 - val_loss: 4.9942e-05\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4720e-05 - val_loss: 4.4976e-05\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2831e-05 - val_loss: 9.7746e-05\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8754e-05 - val_loss: 3.5971e-05\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6130e-05 - val_loss: 4.0566e-05\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6537e-05 - val_loss: 9.5381e-05\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0740e-05 - val_loss: 5.8030e-05\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1538e-05 - val_loss: 3.3397e-05\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9721e-05 - val_loss: 4.0054e-05\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.2462e-05 - val_loss: 3.8067e-05\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8192e-05 - val_loss: 4.6717e-05\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3313e-05 - val_loss: 3.8061e-05\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1003e-05 - val_loss: 7.2731e-05\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0773e-05 - val_loss: 4.1361e-05\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4501e-05 - val_loss: 3.7990e-05\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4714e-05 - val_loss: 3.9758e-05\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8232e-05 - val_loss: 4.9339e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2692e-05 - val_loss: 7.1450e-05\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6884e-05 - val_loss: 5.8989e-05\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6756e-05 - val_loss: 3.8396e-05\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0881e-05 - val_loss: 8.1381e-05\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9970e-05 - val_loss: 3.8272e-05\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3199e-05 - val_loss: 3.5179e-05\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1945e-05 - val_loss: 9.6775e-05\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.2671e-05 - val_loss: 5.3113e-05\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7006e-05 - val_loss: 4.1531e-05\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7248e-05 - val_loss: 1.2335e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0925e-05 - val_loss: 5.1866e-05\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9387e-05 - val_loss: 1.5413e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1440e-05 - val_loss: 7.7891e-05\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0671e-05 - val_loss: 7.0964e-05\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8889e-05 - val_loss: 4.4086e-05\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7857e-05 - val_loss: 4.9379e-05\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7927e-05 - val_loss: 6.8748e-05\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1809e-05 - val_loss: 5.4159e-05\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6664e-05 - val_loss: 1.1331e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2432e-05 - val_loss: 4.6664e-05\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7076e-05 - val_loss: 7.6247e-05\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3731e-05 - val_loss: 4.4425e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7910e-05 - val_loss: 5.4910e-05\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3220e-05 - val_loss: 3.7392e-05\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2409e-05 - val_loss: 3.6335e-05\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6623e-05 - val_loss: 1.2120e-04\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/GOOG/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0290 - val_loss: 0.0013\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.3160e-04 - val_loss: 3.1219e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.6006e-04 - val_loss: 1.6811e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7869e-04 - val_loss: 1.3948e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7142e-04 - val_loss: 1.2300e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5179e-04 - val_loss: 1.1750e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5013e-04 - val_loss: 1.0045e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4133e-04 - val_loss: 9.1664e-05\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3238e-04 - val_loss: 9.0279e-05\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5766e-04 - val_loss: 1.0145e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4852e-04 - val_loss: 1.1823e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4546e-04 - val_loss: 1.1664e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3351e-04 - val_loss: 7.5901e-05\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2854e-04 - val_loss: 7.8293e-05\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3763e-04 - val_loss: 1.3363e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3337e-04 - val_loss: 1.1074e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3002e-04 - val_loss: 1.0216e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3595e-04 - val_loss: 8.1711e-05\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1393e-04 - val_loss: 1.0915e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1680e-04 - val_loss: 9.8873e-05\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2949e-04 - val_loss: 8.4191e-05\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2949e-04 - val_loss: 6.7833e-05\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2724e-04 - val_loss: 8.9929e-05\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2667e-04 - val_loss: 5.4784e-05\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0586e-04 - val_loss: 6.9282e-05\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2428e-04 - val_loss: 6.6631e-05\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8680e-05 - val_loss: 7.1406e-05\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0128e-04 - val_loss: 1.5376e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1973e-04 - val_loss: 5.1332e-05\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4903e-05 - val_loss: 1.8558e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1135e-04 - val_loss: 1.3014e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0804e-04 - val_loss: 7.1880e-05\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0305e-05 - val_loss: 9.2337e-05\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0807e-04 - val_loss: 8.2368e-05\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0166e-04 - val_loss: 5.4425e-05\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1217e-04 - val_loss: 7.7090e-05\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4002e-05 - val_loss: 1.4053e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7768e-05 - val_loss: 5.3420e-05\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2703e-05 - val_loss: 4.8717e-05\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0774e-04 - val_loss: 4.2565e-05\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.0027e-04 - val_loss: 2.1350e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0096e-04 - val_loss: 5.5257e-05\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3383e-04 - val_loss: 7.1268e-05\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8019e-05 - val_loss: 6.6560e-05\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0302e-04 - val_loss: 8.0453e-05\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8662e-05 - val_loss: 4.2484e-05\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.2811e-05 - val_loss: 7.6579e-05\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.3270e-05 - val_loss: 8.2831e-05\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0245e-04 - val_loss: 4.6369e-05\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.9516e-05 - val_loss: 7.5724e-05\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2581e-04 - val_loss: 1.8407e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1197e-04 - val_loss: 1.6564e-04\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 9.9965e-05 - val_loss: 1.2488e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5646e-05 - val_loss: 5.6182e-05\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.0710e-05 - val_loss: 4.8802e-05\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3124e-05 - val_loss: 5.2024e-05\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6045e-05 - val_loss: 4.2630e-05\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0889e-05 - val_loss: 8.0840e-05\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4394e-05 - val_loss: 4.3095e-05\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8285e-05 - val_loss: 4.8952e-05\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7151e-05 - val_loss: 5.0280e-05\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1895e-05 - val_loss: 5.3898e-05\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8098e-05 - val_loss: 5.5749e-05\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4857e-05 - val_loss: 1.3268e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.2666e-05 - val_loss: 3.8121e-05\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7518e-05 - val_loss: 1.2041e-04\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4595e-05 - val_loss: 1.6746e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2276e-05 - val_loss: 5.1710e-05\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3813e-05 - val_loss: 4.8113e-05\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3315e-05 - val_loss: 4.2937e-05\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0589e-04 - val_loss: 1.1122e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2833e-05 - val_loss: 5.1373e-05\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2108e-05 - val_loss: 7.0373e-05\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5214e-05 - val_loss: 5.0335e-05\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7704e-05 - val_loss: 1.4422e-04\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1160e-04 - val_loss: 1.3563e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8722e-05 - val_loss: 5.6855e-05\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6261e-05 - val_loss: 9.2441e-05\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7717e-05 - val_loss: 6.6962e-05\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0781e-05 - val_loss: 5.2979e-05\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2225e-05 - val_loss: 4.7404e-05\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8305e-05 - val_loss: 4.4515e-05\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.7723e-05 - val_loss: 4.2949e-05\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.0239e-05 - val_loss: 4.4930e-05\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.3372e-05 - val_loss: 5.3197e-05\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.9345e-05 - val_loss: 4.1974e-05\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.8028e-05 - val_loss: 4.2552e-05\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.0852e-05 - val_loss: 4.0635e-05\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 6.8548e-05 - val_loss: 5.6683e-05\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.7127e-05 - val_loss: 5.1371e-05\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.8589e-05 - val_loss: 8.8351e-05\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.8342e-05 - val_loss: 4.5425e-05\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 6.1715e-05 - val_loss: 4.2841e-05\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.9009e-05 - val_loss: 5.3188e-05\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0228e-05 - val_loss: 9.5374e-05\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7643e-05 - val_loss: 9.1086e-05\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.0715e-05 - val_loss: 4.4316e-05\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.1385e-05 - val_loss: 6.5550e-05\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.8389e-05 - val_loss: 9.6910e-05\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.2325e-05 - val_loss: 6.1367e-05\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.1548e-05 - val_loss: 1.7421e-04\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0084e-05 - val_loss: 4.7460e-05\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5791e-05 - val_loss: 6.0257e-05\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4753e-05 - val_loss: 1.4164e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.6954e-05 - val_loss: 4.8326e-05\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.6976e-05 - val_loss: 5.2435e-05\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.5056e-05 - val_loss: 5.7118e-05\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.1447e-05 - val_loss: 7.7675e-05\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 6.3791e-05 - val_loss: 9.3390e-05\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0185e-05 - val_loss: 4.8990e-05\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4677e-05 - val_loss: 5.9233e-05\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.7477e-05 - val_loss: 6.0057e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3833e-05 - val_loss: 4.5547e-05\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4518e-05 - val_loss: 1.3816e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.7608e-05 - val_loss: 5.5068e-05\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.6728e-05 - val_loss: 7.7755e-05\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.8218e-05 - val_loss: 4.2094e-05\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5227e-05 - val_loss: 4.1425e-05\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1956e-05 - val_loss: 9.5836e-05\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.4132e-05 - val_loss: 4.6172e-05\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.0284e-05 - val_loss: 5.3404e-05\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3040e-05 - val_loss: 7.6145e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.5760e-05 - val_loss: 1.7320e-04\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.7800e-05 - val_loss: 4.0193e-05\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.7625e-05 - val_loss: 8.9218e-05\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1517e-05 - val_loss: 5.7831e-05\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.2119e-05 - val_loss: 6.9349e-05\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.4570e-05 - val_loss: 5.8157e-05\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.0312e-05 - val_loss: 4.4094e-05\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.0063e-05 - val_loss: 4.8237e-05\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.8931e-05 - val_loss: 6.1024e-05\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.6791e-05 - val_loss: 5.5530e-05\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.1920e-05 - val_loss: 6.3182e-05\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3271e-05 - val_loss: 5.2956e-05\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5341e-05 - val_loss: 5.9065e-05\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.0185e-05 - val_loss: 1.2139e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.0237e-05 - val_loss: 5.2548e-05\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.0319e-05 - val_loss: 4.2892e-05\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.2115e-05 - val_loss: 4.8111e-05\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 7.9049e-05 - val_loss: 4.7179e-05\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2104e-05 - val_loss: 5.9281e-05\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0482e-05 - val_loss: 6.6008e-05\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.7969e-05 - val_loss: 4.4078e-05\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.8637e-05 - val_loss: 5.2366e-05\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0144e-05 - val_loss: 5.1111e-05\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.0234e-05 - val_loss: 4.5824e-05\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.1956e-05 - val_loss: 5.3917e-05\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.3859e-05 - val_loss: 7.3899e-05\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2864e-05 - val_loss: 3.7793e-05\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2801e-05 - val_loss: 4.8123e-05\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.7631e-05 - val_loss: 4.0102e-05\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.4608e-05 - val_loss: 6.0638e-05\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0409e-05 - val_loss: 7.7558e-05\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 7.2041e-05 - val_loss: 5.4553e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5774e-05 - val_loss: 4.6672e-05\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3930e-05 - val_loss: 5.5242e-05\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.4646e-05 - val_loss: 4.9032e-05\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.3522e-05 - val_loss: 4.9329e-05\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.8460e-05 - val_loss: 4.8078e-05\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.4096e-05 - val_loss: 6.0258e-05\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4947e-05 - val_loss: 6.3114e-05\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4241e-05 - val_loss: 4.8337e-05\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1329e-05 - val_loss: 1.1706e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.6538e-05 - val_loss: 2.0537e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.1829e-05 - val_loss: 1.1452e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7113e-05 - val_loss: 8.5104e-05\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7956e-05 - val_loss: 6.5061e-05\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5929e-05 - val_loss: 1.4674e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9666e-05 - val_loss: 1.4014e-04\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0035e-04 - val_loss: 6.4697e-05\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1441e-05 - val_loss: 4.2484e-05\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1808e-05 - val_loss: 5.2811e-05\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5933e-05 - val_loss: 5.1091e-05\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9253e-05 - val_loss: 4.9669e-05\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7354e-05 - val_loss: 4.8100e-05\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9570e-05 - val_loss: 7.5507e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4708e-05 - val_loss: 6.3410e-05\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 6.4791e-05 - val_loss: 4.5700e-05\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1900e-05 - val_loss: 1.1206e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3907e-05 - val_loss: 7.1863e-05\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/GOOGL/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0325 - val_loss: 0.0013\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.6019e-04 - val_loss: 3.6543e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 2.7767e-04 - val_loss: 2.1959e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.9401e-04 - val_loss: 2.1070e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.7730e-04 - val_loss: 1.3877e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4497e-04 - val_loss: 1.1804e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4793e-04 - val_loss: 2.0350e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5052e-04 - val_loss: 2.4693e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4086e-04 - val_loss: 1.0152e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3495e-04 - val_loss: 1.0659e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3130e-04 - val_loss: 1.7652e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0597e-04 - val_loss: 7.9609e-05\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1415e-04 - val_loss: 8.2742e-05\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4753e-04 - val_loss: 7.0736e-05\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3535e-04 - val_loss: 6.6820e-05\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1168e-04 - val_loss: 1.6337e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3936e-04 - val_loss: 7.4316e-05\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1413e-04 - val_loss: 6.8715e-05\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1985e-04 - val_loss: 1.2764e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1399e-04 - val_loss: 6.3274e-05\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1121e-04 - val_loss: 1.9631e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0931e-04 - val_loss: 5.7145e-05\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0830e-04 - val_loss: 1.3169e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0440e-04 - val_loss: 7.6092e-05\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3844e-04 - val_loss: 7.6381e-05\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1133e-04 - val_loss: 7.4305e-05\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4129e-05 - val_loss: 5.9011e-05\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8687e-05 - val_loss: 9.7839e-05\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3009e-04 - val_loss: 1.1187e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0954e-04 - val_loss: 5.7352e-05\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0564e-04 - val_loss: 7.2530e-05\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6727e-05 - val_loss: 1.8677e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7518e-05 - val_loss: 2.2607e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0751e-04 - val_loss: 5.4579e-05\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0334e-04 - val_loss: 7.5810e-05\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5633e-05 - val_loss: 5.6873e-05\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0426e-05 - val_loss: 1.0482e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0100e-04 - val_loss: 8.6194e-05\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0200e-04 - val_loss: 1.6146e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3333e-04 - val_loss: 7.6196e-05\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0209e-04 - val_loss: 2.1952e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5538e-05 - val_loss: 2.2013e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7382e-05 - val_loss: 1.1839e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2482e-04 - val_loss: 5.7526e-05\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0252e-05 - val_loss: 6.8337e-05\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0765e-05 - val_loss: 1.0105e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2225e-05 - val_loss: 3.5130e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0485e-05 - val_loss: 4.8633e-05\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4053e-05 - val_loss: 1.8340e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1335e-04 - val_loss: 4.8224e-05\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0813e-05 - val_loss: 5.6781e-05\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7422e-05 - val_loss: 7.6917e-05\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0482e-05 - val_loss: 1.1571e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5482e-05 - val_loss: 7.6138e-05\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5396e-05 - val_loss: 9.6370e-05\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0584e-05 - val_loss: 4.8287e-05\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2645e-05 - val_loss: 9.9197e-05\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.5083e-05 - val_loss: 5.2891e-05\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2699e-05 - val_loss: 4.5995e-05\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7008e-05 - val_loss: 8.7842e-05\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8988e-05 - val_loss: 6.1084e-05\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6267e-05 - val_loss: 7.7553e-05\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6691e-05 - val_loss: 1.1247e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8830e-05 - val_loss: 3.9414e-05\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2139e-05 - val_loss: 9.8564e-05\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.9562e-05 - val_loss: 4.9379e-05\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5356e-05 - val_loss: 6.2988e-05\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2444e-05 - val_loss: 1.1395e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4776e-05 - val_loss: 4.5813e-05\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5067e-05 - val_loss: 6.5565e-05\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0658e-05 - val_loss: 4.3529e-05\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9745e-05 - val_loss: 5.7849e-05\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1634e-04 - val_loss: 1.9611e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8173e-05 - val_loss: 5.4353e-05\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7619e-05 - val_loss: 4.1286e-05\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3006e-05 - val_loss: 7.9132e-05\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5779e-05 - val_loss: 1.2513e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7955e-05 - val_loss: 3.8250e-05\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0746e-05 - val_loss: 1.0643e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4029e-05 - val_loss: 2.2004e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9124e-05 - val_loss: 1.3474e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1151e-04 - val_loss: 8.6289e-05\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4884e-05 - val_loss: 5.5905e-05\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1159e-05 - val_loss: 1.1070e-04\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8204e-05 - val_loss: 9.5230e-05\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3471e-05 - val_loss: 5.3562e-05\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0220e-05 - val_loss: 5.3385e-05\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3331e-05 - val_loss: 5.3447e-05\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.2532e-05 - val_loss: 4.5887e-05\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0218e-05 - val_loss: 6.4794e-05\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4787e-05 - val_loss: 5.0764e-05\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6955e-05 - val_loss: 6.7452e-05\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0553e-05 - val_loss: 5.3813e-05\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.9155e-05 - val_loss: 5.0344e-05\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3428e-05 - val_loss: 4.7502e-05\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2637e-05 - val_loss: 6.0429e-05\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8560e-05 - val_loss: 5.2310e-05\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.0720e-05 - val_loss: 5.3110e-05\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8985e-05 - val_loss: 5.6277e-05\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1452e-05 - val_loss: 4.6444e-05\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0433e-05 - val_loss: 6.3202e-05\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2096e-05 - val_loss: 4.7328e-05\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7782e-05 - val_loss: 4.4855e-05\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0471e-05 - val_loss: 4.8153e-05\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9485e-05 - val_loss: 5.1321e-05\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5934e-05 - val_loss: 4.6337e-05\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0768e-05 - val_loss: 5.4073e-05\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8851e-05 - val_loss: 6.4563e-05\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4858e-05 - val_loss: 3.5121e-05\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3748e-05 - val_loss: 3.6680e-05\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8229e-05 - val_loss: 5.4411e-05\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.2190e-05 - val_loss: 5.8397e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8328e-05 - val_loss: 4.4547e-05\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8093e-05 - val_loss: 4.2874e-05\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9200e-05 - val_loss: 5.4941e-05\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5940e-05 - val_loss: 5.6715e-05\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.6990e-05 - val_loss: 6.3070e-05\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.2905e-05 - val_loss: 4.2229e-05\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.6966e-05 - val_loss: 3.2365e-05\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.7140e-05 - val_loss: 5.5397e-05\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.1077e-05 - val_loss: 4.3858e-05\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.4499e-05 - val_loss: 7.1009e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.5727e-05 - val_loss: 7.8155e-05\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.6011e-05 - val_loss: 6.2981e-05\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.4002e-05 - val_loss: 3.4181e-05\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7211e-05 - val_loss: 5.7279e-05\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0389e-05 - val_loss: 1.1387e-04\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4139e-05 - val_loss: 7.0634e-05\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.6915e-05 - val_loss: 5.7810e-05\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3477e-05 - val_loss: 7.6712e-05\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0063e-05 - val_loss: 6.3214e-05\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4365e-05 - val_loss: 7.2407e-05\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8794e-05 - val_loss: 5.3943e-05\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3025e-05 - val_loss: 4.8035e-05\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6338e-05 - val_loss: 1.2280e-04\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6956e-05 - val_loss: 6.4854e-05\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4493e-05 - val_loss: 4.8992e-05\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3883e-05 - val_loss: 3.7679e-05\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5668e-05 - val_loss: 7.2866e-05\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5879e-05 - val_loss: 6.1659e-05\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1314e-05 - val_loss: 7.4480e-05\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0162e-05 - val_loss: 7.7390e-05\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6757e-05 - val_loss: 3.7426e-05\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5606e-05 - val_loss: 5.2583e-05\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.8146e-05 - val_loss: 4.5916e-05\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6223e-05 - val_loss: 7.0277e-05\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1834e-05 - val_loss: 4.3743e-05\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7280e-05 - val_loss: 4.9945e-05\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4660e-05 - val_loss: 3.6053e-05\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8517e-05 - val_loss: 5.2629e-05\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4789e-05 - val_loss: 7.1358e-05\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.2767e-05 - val_loss: 7.8656e-05\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6424e-05 - val_loss: 6.4135e-05\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4058e-05 - val_loss: 3.7138e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3058e-05 - val_loss: 5.6770e-05\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4597e-05 - val_loss: 3.9532e-05\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8285e-05 - val_loss: 4.9863e-05\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3391e-05 - val_loss: 1.2235e-04\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3372e-05 - val_loss: 1.2893e-04\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7855e-05 - val_loss: 4.1962e-05\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.2501e-05 - val_loss: 4.6926e-05\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8792e-05 - val_loss: 1.4153e-04\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5860e-05 - val_loss: 5.2330e-05\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3482e-05 - val_loss: 6.0444e-05\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5680e-05 - val_loss: 3.8427e-05\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3909e-05 - val_loss: 4.4904e-05\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7980e-05 - val_loss: 6.1023e-05\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4346e-05 - val_loss: 4.1685e-05\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7669e-05 - val_loss: 7.6280e-05\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0214e-05 - val_loss: 5.0382e-05\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8849e-05 - val_loss: 1.3168e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3038e-05 - val_loss: 4.5751e-05\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8175e-05 - val_loss: 4.6251e-05\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7035e-05 - val_loss: 5.0187e-05\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3390e-05 - val_loss: 8.0864e-05\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1476e-05 - val_loss: 5.6626e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5009e-05 - val_loss: 5.6968e-05\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8343e-05 - val_loss: 5.5754e-05\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8725e-05 - val_loss: 1.6148e-04\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5379e-05 - val_loss: 5.1418e-05\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/MSFT/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0149 - val_loss: 6.4321e-04\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.2732e-04 - val_loss: 3.5702e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 3.1439e-04 - val_loss: 2.0050e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1330e-04 - val_loss: 2.0586e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8802e-04 - val_loss: 2.0380e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.7279e-04 - val_loss: 5.0759e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5998e-04 - val_loss: 1.4000e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7712e-04 - val_loss: 1.6894e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6677e-04 - val_loss: 1.3798e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4069e-04 - val_loss: 1.7849e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5691e-04 - val_loss: 1.7150e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5216e-04 - val_loss: 1.0428e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.5109e-04 - val_loss: 1.0434e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.4164e-04 - val_loss: 1.0552e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3431e-04 - val_loss: 2.4393e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4951e-04 - val_loss: 9.2096e-05\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1857e-04 - val_loss: 9.2861e-05\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1226e-04 - val_loss: 9.7754e-05\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.2129e-04 - val_loss: 1.0689e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2869e-04 - val_loss: 9.6522e-05\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2574e-04 - val_loss: 1.4812e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6137e-04 - val_loss: 1.1643e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5078e-04 - val_loss: 9.3751e-05\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1990e-04 - val_loss: 2.0530e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2098e-04 - val_loss: 1.0200e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2558e-04 - val_loss: 7.2876e-05\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0737e-04 - val_loss: 1.0465e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0223e-04 - val_loss: 8.5724e-05\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0815e-04 - val_loss: 1.1691e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8032e-05 - val_loss: 8.8108e-05\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0561e-04 - val_loss: 8.8992e-05\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0992e-04 - val_loss: 6.6422e-05\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2188e-04 - val_loss: 2.8891e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1907e-04 - val_loss: 7.7163e-05\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0121e-04 - val_loss: 1.3506e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1824e-04 - val_loss: 1.1344e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0526e-04 - val_loss: 2.4459e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1808e-04 - val_loss: 2.2340e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1222e-04 - val_loss: 9.1811e-05\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0837e-04 - val_loss: 2.2222e-04\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1887e-04 - val_loss: 7.9794e-05\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0768e-04 - val_loss: 8.2976e-05\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1458e-05 - val_loss: 8.5980e-05\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6603e-05 - val_loss: 1.2951e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4515e-05 - val_loss: 6.8694e-05\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3747e-05 - val_loss: 1.4769e-04\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2018e-05 - val_loss: 5.9843e-05\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3407e-05 - val_loss: 6.4138e-05\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8076e-05 - val_loss: 6.9074e-05\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0423e-04 - val_loss: 4.7214e-05\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0891e-04 - val_loss: 5.7327e-05\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.5806e-05 - val_loss: 4.8982e-05\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1221e-04 - val_loss: 5.6906e-05\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1695e-05 - val_loss: 9.0964e-05\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3751e-05 - val_loss: 5.4198e-05\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3536e-05 - val_loss: 9.5432e-05\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7743e-05 - val_loss: 7.2842e-05\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1177e-04 - val_loss: 7.5224e-05\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1821e-05 - val_loss: 6.8203e-05\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4567e-05 - val_loss: 6.0925e-05\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.8700e-05 - val_loss: 4.8948e-05\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2358e-05 - val_loss: 5.3318e-05\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7904e-05 - val_loss: 1.0014e-04\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.8425e-05 - val_loss: 6.4025e-05\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2560e-05 - val_loss: 5.7508e-05\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3099e-05 - val_loss: 8.9469e-05\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4211e-05 - val_loss: 6.7726e-05\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9985e-05 - val_loss: 1.0570e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6452e-05 - val_loss: 5.4088e-05\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8933e-05 - val_loss: 9.4660e-05\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.8361e-05 - val_loss: 4.8653e-05\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2620e-05 - val_loss: 4.1919e-05\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.8233e-05 - val_loss: 1.3279e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.6133e-05 - val_loss: 7.1312e-05\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5780e-05 - val_loss: 8.3074e-05\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3070e-05 - val_loss: 7.0373e-05\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8368e-05 - val_loss: 5.5771e-05\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0047e-05 - val_loss: 4.6082e-05\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1181e-05 - val_loss: 6.7872e-05\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9070e-05 - val_loss: 7.6876e-05\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9788e-05 - val_loss: 1.1854e-04\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1422e-04 - val_loss: 5.4032e-05\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1468e-05 - val_loss: 4.7809e-05\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5001e-05 - val_loss: 7.0266e-05\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7580e-05 - val_loss: 1.0921e-04\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5479e-05 - val_loss: 7.4853e-05\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0916e-05 - val_loss: 4.8383e-05\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4603e-05 - val_loss: 7.4542e-05\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1228e-05 - val_loss: 5.2421e-05\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9845e-05 - val_loss: 4.7595e-05\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9163e-05 - val_loss: 7.7918e-05\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6936e-05 - val_loss: 6.3864e-05\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3916e-05 - val_loss: 1.1863e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2112e-05 - val_loss: 6.2620e-05\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9446e-05 - val_loss: 5.5402e-05\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8450e-05 - val_loss: 7.5752e-05\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4256e-05 - val_loss: 6.7792e-05\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3434e-05 - val_loss: 4.7382e-05\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.2611e-05 - val_loss: 6.2678e-05\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.8611e-05 - val_loss: 6.1363e-05\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0021e-05 - val_loss: 8.3015e-05\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.3059e-05 - val_loss: 1.9024e-04\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5289e-05 - val_loss: 9.0436e-05\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6851e-05 - val_loss: 7.6244e-05\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4234e-05 - val_loss: 7.4566e-05\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5483e-05 - val_loss: 3.0231e-04\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2416e-05 - val_loss: 6.4645e-05\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4234e-05 - val_loss: 4.4846e-05\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8849e-05 - val_loss: 5.1370e-05\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.6796e-05 - val_loss: 2.1168e-04\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6841e-05 - val_loss: 7.3864e-05\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5978e-05 - val_loss: 7.7553e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2779e-05 - val_loss: 5.4148e-05\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1221e-05 - val_loss: 7.3011e-05\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.0741e-05 - val_loss: 7.4953e-05\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9515e-05 - val_loss: 6.2691e-05\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0127e-05 - val_loss: 6.6352e-05\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5683e-05 - val_loss: 4.1571e-05\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1749e-05 - val_loss: 1.1412e-04\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7040e-05 - val_loss: 4.0401e-05\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6332e-05 - val_loss: 4.3454e-05\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6631e-05 - val_loss: 5.4127e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9423e-05 - val_loss: 4.3392e-05\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0827e-05 - val_loss: 4.6339e-05\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7756e-05 - val_loss: 1.0728e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7076e-05 - val_loss: 4.3278e-05\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3492e-05 - val_loss: 5.9172e-05\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4133e-05 - val_loss: 3.8431e-05\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0379e-05 - val_loss: 4.7697e-05\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6191e-05 - val_loss: 4.1135e-05\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.9231e-05 - val_loss: 4.5371e-05\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6859e-05 - val_loss: 7.2273e-05\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1982e-05 - val_loss: 7.0516e-05\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5878e-05 - val_loss: 4.8605e-05\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3848e-05 - val_loss: 6.7642e-05\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.8730e-05 - val_loss: 7.3662e-05\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2763e-05 - val_loss: 5.8373e-05\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3278e-05 - val_loss: 5.4133e-05\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8641e-05 - val_loss: 1.0061e-04\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3691e-05 - val_loss: 1.3546e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6024e-05 - val_loss: 8.9516e-05\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3645e-05 - val_loss: 7.0953e-05\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7017e-05 - val_loss: 1.6428e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7160e-05 - val_loss: 1.4071e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3419e-05 - val_loss: 5.0055e-05\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7019e-05 - val_loss: 5.2831e-05\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.0775e-05 - val_loss: 5.3226e-05\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5246e-05 - val_loss: 4.7074e-05\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4638e-05 - val_loss: 4.9160e-05\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.4550e-05 - val_loss: 8.7182e-05\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2957e-05 - val_loss: 5.9257e-05\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8898e-05 - val_loss: 4.3698e-05\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5184e-05 - val_loss: 4.5495e-05\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5472e-05 - val_loss: 5.1294e-05\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3585e-05 - val_loss: 5.4413e-05\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.5350e-05 - val_loss: 6.7821e-05\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5208e-05 - val_loss: 6.8342e-05\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9288e-05 - val_loss: 4.0758e-05\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4935e-05 - val_loss: 8.5186e-05\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4486e-05 - val_loss: 6.1649e-05\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5635e-05 - val_loss: 8.9953e-05\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8637e-05 - val_loss: 7.6960e-05\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 6.3553e-05 - val_loss: 8.4775e-05\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6838e-05 - val_loss: 5.0341e-05\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4655e-05 - val_loss: 8.6403e-05\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4954e-05 - val_loss: 1.1205e-04\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.4974e-05 - val_loss: 5.0574e-05\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3657e-05 - val_loss: 6.6137e-05\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1452e-05 - val_loss: 6.5224e-05\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2350e-05 - val_loss: 1.3183e-04\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5703e-05 - val_loss: 5.7662e-05\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.9146e-05 - val_loss: 6.7655e-05\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9291e-05 - val_loss: 4.3008e-05\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0057e-05 - val_loss: 4.4798e-05\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.8100e-05 - val_loss: 4.3733e-05\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5073e-05 - val_loss: 7.2199e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1785e-05 - val_loss: 6.2893e-05\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9243e-05 - val_loss: 5.6505e-05\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6745e-05 - val_loss: 4.9687e-05\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6605e-05 - val_loss: 7.6750e-05\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/NVDA/ann\\assets\n",
      "Epoch 1/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 0.0172 - val_loss: 8.1909e-04\n",
      "Epoch 2/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 6.0904e-04 - val_loss: 5.7651e-04\n",
      "Epoch 3/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.8284e-04 - val_loss: 4.4216e-04\n",
      "Epoch 4/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 3.6945e-04 - val_loss: 6.5779e-04\n",
      "Epoch 5/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 3.4316e-04 - val_loss: 3.6705e-04\n",
      "Epoch 6/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.9431e-04 - val_loss: 5.6274e-04\n",
      "Epoch 7/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.7342e-04 - val_loss: 2.6741e-04\n",
      "Epoch 8/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.6522e-04 - val_loss: 2.7119e-04\n",
      "Epoch 9/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.3794e-04 - val_loss: 2.3158e-04\n",
      "Epoch 10/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.6240e-04 - val_loss: 2.6459e-04\n",
      "Epoch 11/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.6567e-04 - val_loss: 3.7108e-04\n",
      "Epoch 12/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.4475e-04 - val_loss: 2.3029e-04\n",
      "Epoch 13/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.2348e-04 - val_loss: 2.5276e-04\n",
      "Epoch 14/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.4102e-04 - val_loss: 3.3296e-04\n",
      "Epoch 15/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.2827e-04 - val_loss: 2.1802e-04\n",
      "Epoch 16/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0762e-04 - val_loss: 1.9630e-04\n",
      "Epoch 17/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1413e-04 - val_loss: 3.5545e-04\n",
      "Epoch 18/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8956e-04 - val_loss: 1.7572e-04\n",
      "Epoch 19/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8776e-04 - val_loss: 2.9285e-04\n",
      "Epoch 20/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.9827e-04 - val_loss: 1.6504e-04\n",
      "Epoch 21/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1051e-04 - val_loss: 2.7526e-04\n",
      "Epoch 22/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.1239e-04 - val_loss: 3.5278e-04\n",
      "Epoch 23/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8834e-04 - val_loss: 2.6258e-04\n",
      "Epoch 24/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.7807e-04 - val_loss: 5.8281e-04\n",
      "Epoch 25/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 2.0580e-04 - val_loss: 2.4950e-04\n",
      "Epoch 26/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8121e-04 - val_loss: 1.8227e-04\n",
      "Epoch 27/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6577e-04 - val_loss: 1.4515e-04\n",
      "Epoch 28/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8191e-04 - val_loss: 1.9191e-04\n",
      "Epoch 29/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8538e-04 - val_loss: 2.6086e-04\n",
      "Epoch 30/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5812e-04 - val_loss: 1.2700e-04\n",
      "Epoch 31/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6229e-04 - val_loss: 2.0096e-04\n",
      "Epoch 32/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5206e-04 - val_loss: 1.6838e-04\n",
      "Epoch 33/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5015e-04 - val_loss: 1.1207e-04\n",
      "Epoch 34/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4049e-04 - val_loss: 1.8496e-04\n",
      "Epoch 35/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4575e-04 - val_loss: 4.6090e-04\n",
      "Epoch 36/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3174e-04 - val_loss: 1.2401e-04\n",
      "Epoch 37/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6496e-04 - val_loss: 2.5779e-04\n",
      "Epoch 38/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5816e-04 - val_loss: 1.4969e-04\n",
      "Epoch 39/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5783e-04 - val_loss: 3.7219e-04\n",
      "Epoch 40/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.8448e-04 - val_loss: 9.0908e-05\n",
      "Epoch 41/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4989e-04 - val_loss: 1.7608e-04\n",
      "Epoch 42/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2318e-04 - val_loss: 1.4127e-04\n",
      "Epoch 43/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5981e-04 - val_loss: 1.4367e-04\n",
      "Epoch 44/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4752e-04 - val_loss: 1.4584e-04\n",
      "Epoch 45/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2417e-04 - val_loss: 1.0613e-04\n",
      "Epoch 46/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2577e-04 - val_loss: 8.9113e-05\n",
      "Epoch 47/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5368e-04 - val_loss: 1.0252e-04\n",
      "Epoch 48/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3129e-04 - val_loss: 1.6742e-04\n",
      "Epoch 49/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5291e-04 - val_loss: 2.4665e-04\n",
      "Epoch 50/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4851e-04 - val_loss: 1.1547e-04\n",
      "Epoch 51/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4791e-04 - val_loss: 1.9668e-04\n",
      "Epoch 52/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1094e-04 - val_loss: 8.9161e-05\n",
      "Epoch 53/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1733e-04 - val_loss: 1.6194e-04\n",
      "Epoch 54/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2177e-04 - val_loss: 2.7842e-04\n",
      "Epoch 55/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2856e-04 - val_loss: 9.7131e-05\n",
      "Epoch 56/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.6960e-04 - val_loss: 7.9024e-05\n",
      "Epoch 57/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3072e-04 - val_loss: 1.4680e-04\n",
      "Epoch 58/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1430e-04 - val_loss: 9.1487e-05\n",
      "Epoch 59/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2097e-04 - val_loss: 1.4074e-04\n",
      "Epoch 60/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4504e-04 - val_loss: 1.3924e-04\n",
      "Epoch 61/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4202e-04 - val_loss: 1.1193e-04\n",
      "Epoch 62/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3651e-04 - val_loss: 1.0567e-04\n",
      "Epoch 63/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2018e-04 - val_loss: 8.1221e-05\n",
      "Epoch 64/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4574e-04 - val_loss: 1.8257e-04\n",
      "Epoch 65/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0064e-04 - val_loss: 1.0835e-04\n",
      "Epoch 66/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3037e-04 - val_loss: 7.1482e-05\n",
      "Epoch 67/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1342e-04 - val_loss: 1.1647e-04\n",
      "Epoch 68/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3791e-04 - val_loss: 1.0841e-04\n",
      "Epoch 69/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2327e-04 - val_loss: 9.6853e-05\n",
      "Epoch 70/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2719e-04 - val_loss: 1.8593e-04\n",
      "Epoch 71/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.5868e-04 - val_loss: 1.1630e-04\n",
      "Epoch 72/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1373e-04 - val_loss: 1.2159e-04\n",
      "Epoch 73/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2698e-04 - val_loss: 1.7802e-04\n",
      "Epoch 74/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1685e-04 - val_loss: 1.1334e-04\n",
      "Epoch 75/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0630e-04 - val_loss: 8.5989e-05\n",
      "Epoch 76/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3109e-04 - val_loss: 2.2397e-04\n",
      "Epoch 77/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1207e-04 - val_loss: 1.0158e-04\n",
      "Epoch 78/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2934e-04 - val_loss: 6.9082e-05\n",
      "Epoch 79/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3187e-04 - val_loss: 1.2390e-04\n",
      "Epoch 80/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1456e-04 - val_loss: 1.1212e-04\n",
      "Epoch 81/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1016e-04 - val_loss: 7.3629e-05\n",
      "Epoch 82/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3297e-04 - val_loss: 7.3926e-05\n",
      "Epoch 83/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9213e-05 - val_loss: 1.1853e-04\n",
      "Epoch 84/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2658e-04 - val_loss: 8.7653e-05\n",
      "Epoch 85/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8972e-05 - val_loss: 7.1397e-05\n",
      "Epoch 86/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0570e-04 - val_loss: 1.1247e-04\n",
      "Epoch 87/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3747e-04 - val_loss: 6.8201e-05\n",
      "Epoch 88/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1475e-04 - val_loss: 7.6659e-05\n",
      "Epoch 89/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0316e-04 - val_loss: 1.0615e-04\n",
      "Epoch 90/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1386e-04 - val_loss: 1.6843e-04\n",
      "Epoch 91/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1795e-04 - val_loss: 1.1293e-04\n",
      "Epoch 92/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1687e-04 - val_loss: 9.4214e-05\n",
      "Epoch 93/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0058e-04 - val_loss: 2.8585e-04\n",
      "Epoch 94/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2096e-04 - val_loss: 7.5457e-05\n",
      "Epoch 95/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1863e-04 - val_loss: 1.1611e-04\n",
      "Epoch 96/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1700e-04 - val_loss: 6.7305e-05\n",
      "Epoch 97/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1141e-04 - val_loss: 8.3579e-05\n",
      "Epoch 98/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1664e-04 - val_loss: 2.1130e-04\n",
      "Epoch 99/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2441e-04 - val_loss: 2.9278e-04\n",
      "Epoch 100/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1621e-04 - val_loss: 1.5841e-04\n",
      "Epoch 101/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0226e-04 - val_loss: 7.6063e-05\n",
      "Epoch 102/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1009e-04 - val_loss: 8.1251e-05\n",
      "Epoch 103/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8313e-05 - val_loss: 1.8180e-04\n",
      "Epoch 104/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0629e-04 - val_loss: 1.1052e-04\n",
      "Epoch 105/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0844e-04 - val_loss: 1.0320e-04\n",
      "Epoch 106/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2936e-04 - val_loss: 9.3139e-05\n",
      "Epoch 107/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2688e-04 - val_loss: 6.8914e-05\n",
      "Epoch 108/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1828e-04 - val_loss: 1.1006e-04\n",
      "Epoch 109/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8662e-05 - val_loss: 6.5313e-05\n",
      "Epoch 110/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1539e-04 - val_loss: 6.6499e-05\n",
      "Epoch 111/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0154e-04 - val_loss: 7.6059e-05\n",
      "Epoch 112/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3216e-04 - val_loss: 9.1403e-05\n",
      "Epoch 113/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2923e-04 - val_loss: 6.3897e-05\n",
      "Epoch 114/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2282e-04 - val_loss: 1.8018e-04\n",
      "Epoch 115/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0727e-04 - val_loss: 9.4609e-05\n",
      "Epoch 116/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0040e-04 - val_loss: 7.0510e-05\n",
      "Epoch 117/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0494e-04 - val_loss: 1.1899e-04\n",
      "Epoch 118/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1063e-04 - val_loss: 1.1757e-04\n",
      "Epoch 119/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1900e-04 - val_loss: 9.0568e-05\n",
      "Epoch 120/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0741e-04 - val_loss: 1.1504e-04\n",
      "Epoch 121/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5931e-05 - val_loss: 8.4356e-05\n",
      "Epoch 122/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1177e-04 - val_loss: 8.7305e-05\n",
      "Epoch 123/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7303e-05 - val_loss: 8.0166e-05\n",
      "Epoch 124/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1167e-05 - val_loss: 7.6938e-05\n",
      "Epoch 125/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1147e-04 - val_loss: 1.1338e-04\n",
      "Epoch 126/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0476e-04 - val_loss: 6.4824e-05\n",
      "Epoch 127/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2865e-04 - val_loss: 6.8488e-05\n",
      "Epoch 128/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0147e-04 - val_loss: 5.9453e-05\n",
      "Epoch 129/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.4221e-04 - val_loss: 6.9929e-05\n",
      "Epoch 130/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2194e-04 - val_loss: 7.5719e-05\n",
      "Epoch 131/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0400e-05 - val_loss: 1.1580e-04\n",
      "Epoch 132/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1631e-04 - val_loss: 6.8627e-05\n",
      "Epoch 133/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1009e-04 - val_loss: 1.7124e-04\n",
      "Epoch 134/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.3509e-04 - val_loss: 2.6021e-04\n",
      "Epoch 135/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1227e-04 - val_loss: 7.4521e-05\n",
      "Epoch 136/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.2059e-04 - val_loss: 1.4786e-04\n",
      "Epoch 137/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0108e-04 - val_loss: 8.2994e-05\n",
      "Epoch 138/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0947e-04 - val_loss: 1.2080e-04\n",
      "Epoch 139/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0356e-04 - val_loss: 7.8500e-05\n",
      "Epoch 140/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5259e-05 - val_loss: 1.3047e-04\n",
      "Epoch 141/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.1690e-04 - val_loss: 1.3498e-04\n",
      "Epoch 142/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1131e-04 - val_loss: 8.4554e-05\n",
      "Epoch 143/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.2782e-04 - val_loss: 2.2585e-04\n",
      "Epoch 144/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.1033e-04 - val_loss: 1.6340e-04\n",
      "Epoch 145/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3202e-05 - val_loss: 1.2555e-04\n",
      "Epoch 146/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1.0385e-04 - val_loss: 7.0006e-05\n",
      "Epoch 147/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1517e-04 - val_loss: 7.6117e-05\n",
      "Epoch 148/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1062e-05 - val_loss: 8.5773e-05\n",
      "Epoch 149/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1093e-04 - val_loss: 8.8831e-05\n",
      "Epoch 150/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3367e-04 - val_loss: 1.7538e-04\n",
      "Epoch 151/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.1435e-04 - val_loss: 7.1508e-05\n",
      "Epoch 152/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 8.6865e-05 - val_loss: 7.2092e-05\n",
      "Epoch 153/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.9756e-05 - val_loss: 6.9882e-05\n",
      "Epoch 154/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.3347e-04 - val_loss: 1.0747e-04\n",
      "Epoch 155/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4150e-05 - val_loss: 1.2633e-04\n",
      "Epoch 156/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0947e-04 - val_loss: 8.2427e-05\n",
      "Epoch 157/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.5205e-05 - val_loss: 8.9189e-05\n",
      "Epoch 158/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.9140e-05 - val_loss: 7.5016e-05\n",
      "Epoch 159/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.6338e-05 - val_loss: 1.5132e-04\n",
      "Epoch 160/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.2392e-05 - val_loss: 6.4582e-05\n",
      "Epoch 161/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.8483e-05 - val_loss: 1.0531e-04\n",
      "Epoch 162/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.9143e-05 - val_loss: 9.2523e-05\n",
      "Epoch 163/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1239e-05 - val_loss: 1.0179e-04\n",
      "Epoch 164/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4621e-05 - val_loss: 1.5354e-04\n",
      "Epoch 165/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.0965e-04 - val_loss: 1.3721e-04\n",
      "Epoch 166/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1648e-05 - val_loss: 6.2163e-05\n",
      "Epoch 167/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 9.5296e-05 - val_loss: 9.8783e-05\n",
      "Epoch 168/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 9.5998e-05 - val_loss: 1.0668e-04\n",
      "Epoch 169/180\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 9.1089e-05 - val_loss: 8.0938e-05\n",
      "Epoch 170/180\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1.0895e-04 - val_loss: 7.1130e-05\n",
      "Epoch 171/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.2687e-05 - val_loss: 1.5365e-04\n",
      "Epoch 172/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.1118e-05 - val_loss: 1.1422e-04\n",
      "Epoch 173/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0067e-04 - val_loss: 4.7860e-04\n",
      "Epoch 174/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.3936e-04 - val_loss: 2.0186e-04\n",
      "Epoch 175/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0252e-04 - val_loss: 7.0493e-05\n",
      "Epoch 176/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.6481e-05 - val_loss: 7.1369e-05\n",
      "Epoch 177/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0452e-04 - val_loss: 1.2658e-04\n",
      "Epoch 178/180\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 1.0394e-04 - val_loss: 1.7561e-04\n",
      "Epoch 179/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 1.1114e-04 - val_loss: 8.2956e-05\n",
      "Epoch 180/180\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.5832e-05 - val_loss: 7.1720e-05\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/TSLA/ann\\assets\n"
     ]
    }
   ],
   "source": [
    "histories = {}\n",
    "\n",
    "def get_all_file_paths(directory):\n",
    "  \n",
    "    # initializing empty file paths list\n",
    "    file_paths = []\n",
    "  \n",
    "    # crawling through directory and subdirectories\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for fileName_model in files:\n",
    "            # join the two strings in order to form the full filepath.\n",
    "            filepath = os.path.join(root, fileName_model)\n",
    "            file_paths.append(filepath)\n",
    "  \n",
    "    # returning all file paths\n",
    "    return file_paths\n",
    "    \n",
    "for tick in data.keys():\n",
    "    stock_data = data[tick]\n",
    "    df = pd.DataFrame(stock_data).T\n",
    "    df['H-L'] = df['High'] - df['Low']\n",
    "    df['O-C'] = df['Open'] - df['Close']\n",
    "    df['7MA'] = df['Adj Close'].rolling(window=7).mean()\n",
    "    df['14MA'] = df['Adj Close'].rolling(window=14).mean()\n",
    "    df['21MA'] = df['Adj Close'].rolling(window=21).mean()\n",
    "    df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "\n",
    "    features = ['H-L','O-C','7MA','14MA','21MA','7SD','Volume','Close']\n",
    "    df = df[features].apply(pd.to_numeric)\n",
    "    df_final = df[20:].copy()\n",
    "    df_final['Close'] = df_final['Close'].shift(1)\n",
    "\n",
    "    features = ['H-L','O-C','7MA','14MA','21MA','7SD','Volume']\n",
    "    #https://stackoverflow.com/questions/36926140/how-to-convert-numpy-arrays-to-standard-tensorflow-format\n",
    "    X = np.asarray(df_final[1:][features], np.float32)\n",
    "    Y = np.asarray(df_final[1:]['Close'], np.float32)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    scaler_x = MinMaxScaler().fit(X_train)\n",
    "    scaler_y = MinMaxScaler().fit(y_train)\n",
    "\n",
    "    X_train = scaler_x.transform(X_train)\n",
    "    y_train = scaler_y.transform(y_train)\n",
    "    # Defining the Input layer and FIRST hidden layer, both are same!\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=50, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "    # Defining the Second layer of the model\n",
    "    # after the first layer we don't have to specify input_dim as keras configure it automatically\n",
    "    model.add(Dense(units=25, kernel_initializer='normal', activation='tanh'))\n",
    "\n",
    "    model.add(Dense(units=10, kernel_initializer='normal', activation='tanh'))\n",
    "    \n",
    "    # The output neuron is a single fully connected node \n",
    "    # Since we will be predicting a single number\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    # Fitting the ANN to the Training set\n",
    "    history = model.fit(X_train, y_train ,batch_size = 10, validation_split = 0.1, epochs = 180, verbose=1)\n",
    "\n",
    "    histories[tick] = history\n",
    "\n",
    "    filepath_model = \"../../../data/models/\" + tick + \"/ann\"\n",
    "    model.save(filepath_model)\n",
    "    file_paths = get_all_file_paths(filepath_model)\n",
    "\n",
    "    \n",
    "    #took help from this: https://www.geeksforgeeks.org/working-zip-files-python/\n",
    "    with ZipFile(filepath_model + \".zip\",'w') as zip:\n",
    "            # writing each file one by one\n",
    "            for file in file_paths:\n",
    "                zip.write(file)\n",
    "    \n",
    "    \n",
    "    fileName_model = \"ann.zip\"\n",
    "    bucket = storage.bucket()\n",
    "    #upload models\n",
    "    blob = bucket.blob(\"models/\" + tick + \"/\" + fileName_model)\n",
    "    blob.upload_from_filename(filepath_model+\".zip\")\n",
    "    \n",
    "    #upload normalizer training data\n",
    "    filepath_normalizer = \"../../../data/normalizers/\" + tick + \"/ann_x.pkl\"\n",
    "    pickle.dump(scaler_x, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "    filename_normalizer = \"ann_x.pkl\"\n",
    "    blob = bucket.blob(\"normalizers/\" + tick + \"/ann_x.pkl\")\n",
    "    blob.upload_from_filename(filepath_normalizer)\n",
    "\n",
    "    #upload normalizer predicted value\n",
    "    filepath_normalizer = \"../../../data/normalizers/\" + tick + \"/ann_y.pkl\"\n",
    "    pickle.dump(scaler_y, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "    filename_normalizer = \"ann_y.pkl\"\n",
    "    blob = bucket.blob(\"normalizers/\" + tick + \"/ann_y.pkl\")\n",
    "    blob.upload_from_filename(filepath_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2816e62dd90>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(histories.values())[0].history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000102086204"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_test = y_test.reshape(-1,1)\n",
    "y_test_transform = scaler_y.transform(y_test)\n",
    "X_test = scaler_x.transform(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "mean_squared_error(y_test_transform, y_pred)\n",
    "scaler_y.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick = 'NVDA'\n",
    "import pandas_datareader as pdr\n",
    "def getTestData(ticker, start):\n",
    "    data = pdr.get_data_yahoo(ticker, start=start, end=today)\n",
    "    # dataname= ticker+\"_\"+str(today)\n",
    "    return data[-100:]\n",
    "    \n",
    "from datetime import date  \n",
    "today = date.today()\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "start = d = today - timedelta(days=190)\n",
    "\n",
    "df = getTestData(tick,start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['H-L'] = df['High'] - df['Low']\n",
    "df['O-C'] = df['Open'] - df['Close']\n",
    "df['7MA'] = df['Adj Close'].rolling(window=7).mean()\n",
    "df['14MA'] = df['Adj Close'].rolling(window=14).mean()\n",
    "df['21MA'] = df['Adj Close'].rolling(window=21).mean()\n",
    "df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "\n",
    "test_data = np.asarray(df[-1:][features], np.float32)\n",
    "test = scaler_x.transform(test_data)\n",
    "pred = model.predict(test)  \n",
    "pred_price = scaler_y.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate LSTM with more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import os\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from firebase_admin import storage\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas_ta as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histories = {}\n",
    "for tick in data.keys():\n",
    "    stock_data = data[tick]\n",
    "    stock_data = files[1][0]\n",
    "    df = pd.DataFrame(stock_data).T\n",
    "    df['H-L'] = df['High'] - df['Low']\n",
    "    df['O-C'] = df['Open'] - df['Close']\n",
    "    df['5MA'] = df['Adj Close'].rolling(window=5).mean()\n",
    "    df['10MA'] = df['Adj Close'].rolling(window=10).mean()\n",
    "    df['20MA'] = df['Adj Close'].rolling(window=20).mean()\n",
    "    df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "    df[\"EMA8\"] = df['Adj Close'].ewm(span=8).mean()\n",
    "    df[\"EMA21\"] = df['Adj Close'].ewm(span=21).mean()\n",
    "    df['EMA34'] = df['Adj Close'].ewm(span=34).mean()\n",
    "    df['EMA55'] = df['Adj Close'].ewm(span=55).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    df['Returns'] = df['Close'] / df['Close'].shift(1)\n",
    "    df['Returns'] -= 1\n",
    "    df.dropna(inplace=True)\n",
    "    df.ta.rsi(close='Close', length=14, append=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    features = ['Adj Close','H-L','O-C','5MA','10MA','20MA','7SD','RSI_14', 'EMA8','EMA21','EMA34','EMA55','Returns','Volume']\n",
    "    df = df[features].apply(pd.to_numeric)\n",
    "    train_data = int(0.9*len(df))\n",
    "    val_data = int(0.05*len(df))\n",
    "    train_df,val_df, test_df = df[1:train_data], df[train_data:-val_data], df[-val_data:]\n",
    "    sc = MinMaxScaler()\n",
    "\n",
    "    train = sc.fit_transform(train_df)\n",
    "    val = sc.transform(val_df)\n",
    "    test = sc.transform(test_df)\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_val = []\n",
    "    Y_val = []\n",
    "\n",
    "    timesteps = 50\n",
    "    hl = [40,35,50]\n",
    "    batch = 32\n",
    "    epochs = 8\n",
    "\n",
    "    # Loop for training data\n",
    "    for i in range(timesteps,train.shape[0]):\n",
    "        X_train.append(train[i-timesteps:i])\n",
    "        Y_train.append(train[i][0])\n",
    "    X_train,Y_train = np.array(X_train),np.array(Y_train)\n",
    "\n",
    "    # Loop for val data\n",
    "    for i in range(timesteps,val.shape[0]):\n",
    "        X_val.append(val[i-timesteps:i])\n",
    "        Y_val.append(val[i][0])\n",
    "    X_val,Y_val = np.array(X_val),np.array(Y_val)\n",
    "\n",
    "    # Adding Layers to the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(X_train.shape[2],input_shape = (X_train.shape[1],X_train.shape[2]),return_sequences = True,\n",
    "                    activation = 'relu'))\n",
    "    for i in range(len(hl)-1):        \n",
    "        model.add(LSTM(hl[i],activation = 'relu',return_sequences = True))\n",
    "    model.add(LSTM(hl[-1],activation = 'relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "    #print(model.summary())\n",
    "\n",
    "    # Training the data\n",
    "    history = model.fit(X_train,Y_train,epochs = epochs,batch_size = batch,validation_data = (X_val, Y_val),\n",
    "                        shuffle = False)\n",
    "    histories[tick] = history\n",
    "    model.reset_states()\n",
    "\n",
    "    filepath_model = \"../../../data/models/\" + tick + \"/multi_lstm\"\n",
    "    model.save(filepath_model)\n",
    "    file_paths = get_all_file_paths(filepath_model)\n",
    "\n",
    "    \n",
    "    #took help from this: https://www.geeksforgeeks.org/working-zip-files-python/\n",
    "    with ZipFile(filepath_model + \".zip\",'w') as zip:\n",
    "            # writing each file one by one\n",
    "            for file in file_paths:\n",
    "                zip.write(file)\n",
    "    \n",
    "    \n",
    "    fileName_model = \"multi_lstm.zip\"\n",
    "    bucket = storage.bucket()\n",
    "    #upload models\n",
    "    blob = bucket.blob(\"models/\" + tick + \"/\" + fileName_model)\n",
    "    blob.upload_from_filename(filepath_model+\".zip\")\n",
    "    \n",
    "    #upload normalizer training data\n",
    "    filepath_normalizer = \"../../../data/normalizers/\" + tick + \"/multi_lstm.pkl\"\n",
    "    pickle.dump(sc, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "    filename_normalizer = \"multi_lstm.pkl\"\n",
    "    blob = bucket.blob(\"normalizers/\" + tick + \"/multi_lstm.pkl\")\n",
    "    blob.upload_from_filename(filepath_normalizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "71/71 [==============================] - 21s 97ms/step - loss: 2.0676e-04 - val_loss: 1.0693\n",
      "Epoch 2/4\n",
      "71/71 [==============================] - 6s 79ms/step - loss: 1.6010e-04 - val_loss: 1.0449\n",
      "Epoch 3/4\n",
      "71/71 [==============================] - 6s 80ms/step - loss: 1.0193e-04 - val_loss: 0.1253\n",
      "Epoch 4/4\n",
      "71/71 [==============================] - 5s 75ms/step - loss: 2.4640e-05 - val_loss: 0.0335\n"
     ]
    }
   ],
   "source": [
    "#the model with 7 trading days, and without pca\n",
    "histories = {}\n",
    "# for tick in data.keys():\n",
    "for tick in data.keys():\n",
    "    stock_data = data[tick]\n",
    "    stock_data = files[1][0]\n",
    "    df = pd.DataFrame(stock_data).T\n",
    "# df = pd.DataFrame(stock_data).T\n",
    "    df['H-L'] = df['High'] - df['Low']\n",
    "    df['O-C'] = df['Open'] - df['Close']\n",
    "    df['5MA'] = df['Adj Close'].rolling(window=5).mean()\n",
    "    df['10MA'] = df['Adj Close'].rolling(window=10).mean()\n",
    "    df['20MA'] = df['Adj Close'].rolling(window=20).mean()\n",
    "    df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "    df[\"EMA8\"] = df['Adj Close'].ewm(span=8).mean()\n",
    "    df[\"EMA21\"] = df['Adj Close'].ewm(span=21).mean()\n",
    "    df['EMA34'] = df['Adj Close'].ewm(span=34).mean()\n",
    "    df['EMA55'] = df['Adj Close'].ewm(span=55).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    df['Returns'] = df['Close'] / df['Close'].shift(1)\n",
    "    df['Returns'] -= 1\n",
    "    df.dropna(inplace=True)\n",
    "    df.ta.rsi(close='Close', length=14, append=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    features = ['Adj Close','H-L','O-C','5MA','10MA','20MA','7SD','RSI_14', 'EMA8','EMA21','EMA34','EMA55','Returns','Volume']\n",
    "    df = df[features].apply(pd.to_numeric)\n",
    "    train_data = int(0.9*len(df))\n",
    "\n",
    "    train_df,val_df = df[1:train_data], df[train_data:]\n",
    "    sc_x = MinMaxScaler()\n",
    "    sc_y = MinMaxScaler()\n",
    "\n",
    "\n",
    "    features_x = ['H-L','O-C','5MA','10MA','20MA','7SD','RSI_14', 'EMA8','EMA21','EMA34','EMA55','Returns','Volume']\n",
    "    X_tr = train_df[features_x]\n",
    "    Y_tr = train_df['Adj Close']\n",
    "    x_train_scaled = sc_x.fit_transform(X_tr)\n",
    "    y_train_scaled = sc_y.fit_transform(np.array(Y_tr).reshape(-1, 1))\n",
    "\n",
    "    X_ts = val_df[features_x]\n",
    "    Y_ts = val_df['Adj Close']\n",
    "    x_val_scaled = sc_x.transform(X_ts)\n",
    "    y_val_scaled = sc_y.transform(np.array(Y_ts).reshape(-1, 1))\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_val = []\n",
    "    Y_val = []\n",
    "\n",
    "    timesteps = 7\n",
    "            # Loop for training data\n",
    "    for i in range(timesteps,train.shape[0]):\n",
    "            \n",
    "        X_train.append(x_train_scaled[i-timesteps:i])\n",
    "        Y_train.append(y_train_scaled[i][0])\n",
    "            \n",
    "    X_train,Y_train = np.array(X_train),np.array(Y_train)\n",
    "\n",
    "            # Loop for val data\n",
    "    for i in range(timesteps,val.shape[0]):\n",
    "\n",
    "        X_val.append(x_val_scaled[i-timesteps:i])\n",
    "        Y_val.append(y_val_scaled[i][0])\n",
    "\n",
    "    X_val,Y_val = np.array(X_val),np.array(Y_val)\n",
    "\n",
    "    hl = [40,35,40,50]\n",
    "    batch = 16\n",
    "    epochs = 4\n",
    "\n",
    "\n",
    "    # Adding Layers to the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(40,input_shape = (X_train.shape[1],X_train.shape[2]),return_sequences = True,\n",
    "                            activation = 'relu'))\n",
    "    for i in range(len(hl)-1):        \n",
    "        model.add(LSTM(hl[i],activation = 'relu',return_sequences = True))\n",
    "    model.add(LSTM(hl[-1],activation = 'relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "            #print(model.summary())\n",
    "\n",
    "            # Training the data\n",
    "    history = model.fit(X_train,Y_train,epochs = epochs,batch_size = batch,validation_data = (X_val, Y_val))\n",
    "    histories[tick] = history\n",
    "    model.reset_states()\n",
    "\n",
    "    # tick = 'AAPL'\n",
    "    filepath_model = f\"../../../data/models/{tick}/multi_lstm\"\n",
    "    model.save(filepath_model)\n",
    "    file_paths = get_all_file_paths(filepath_model)\n",
    "\n",
    "\n",
    "    #took help from this: https://www.geeksforgeeks.org/working-zip-files-python/\n",
    "    with ZipFile(filepath_model + \".zip\",'w') as zip:\n",
    "            # writing each file one by one\n",
    "            for file in file_paths:\n",
    "                zip.write(file)\n",
    "\n",
    "\n",
    "    fileName_model = \"multi_lstm.zip\"\n",
    "    bucket = storage.bucket()\n",
    "    #upload models\n",
    "    blob = bucket.blob(f\"models/{tick}/\" + fileName_model)\n",
    "    blob.upload_from_filename(filepath_model+\".zip\")\n",
    "\n",
    "    #upload normalizer training data\n",
    "    filepath_normalizer = f\"../../../data/normalizers/{tick}/multi_lstm_x.pkl\"\n",
    "    pickle.dump(sc_x, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "    filename_normalizer = \"multi_lstm_x.pkl\"\n",
    "    blob = bucket.blob(f\"normalizers/{tick}/multi_lstm_x.pkl\")\n",
    "    blob.upload_from_filename(filepath_normalizer)\n",
    "\n",
    "    #upload normalizer training data\n",
    "    filepath_normalizer = f\"../../../data/normalizers/{tick}/multi_lstm_y.pkl\"\n",
    "    pickle.dump(sc_y, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "    filename_normalizer = \"multi_lstm_y.pkl\"\n",
    "    blob = bucket.blob(f\"normalizers/{tick}/multi_lstm_y.pkl\")\n",
    "    blob.upload_from_filename(filepath_normalizer)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model with 7 trading days with pca\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "tick = 'GOOG'\n",
    "stock_data = data[tick]\n",
    "# stock_data = files[1][0]\n",
    "df = pd.DataFrame(stock_data).T\n",
    "# df = pd.DataFrame(stock_data).T\n",
    "df['H-L'] = df['High'] - df['Low']\n",
    "df['O-C'] = df['Open'] - df['Close']\n",
    "df['5MA'] = df['Adj Close'].rolling(window=5).mean()\n",
    "df['10MA'] = df['Adj Close'].rolling(window=10).mean()\n",
    "df['20MA'] = df['Adj Close'].rolling(window=20).mean()\n",
    "df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "df[\"EMA8\"] = df['Adj Close'].ewm(span=8).mean()\n",
    "df[\"EMA21\"] = df['Adj Close'].ewm(span=21).mean()\n",
    "df['EMA34'] = df['Adj Close'].ewm(span=34).mean()\n",
    "df['EMA55'] = df['Adj Close'].ewm(span=55).mean()\n",
    "df.dropna(inplace=True)\n",
    "df['Returns'] = df['Close'] / df['Close'].shift(1)\n",
    "df['Returns'] -= 1\n",
    "df.dropna(inplace=True)\n",
    "df.ta.rsi(close='Close', length=14, append=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "features = ['Adj Close','H-L','O-C','5MA','10MA','20MA','7SD','RSI_14', 'EMA8','EMA21','EMA34','EMA55','Returns','Volume']\n",
    "df = df[features].apply(pd.to_numeric)\n",
    "train_data = int(0.9*len(df))\n",
    "\n",
    "train_df,val_df = df[1:train_data], df[train_data:]\n",
    "sc_x = MinMaxScaler()\n",
    "sc_y = MinMaxScaler()\n",
    "\n",
    "\n",
    "features_x = ['H-L','O-C','5MA','10MA','20MA','7SD','RSI_14', 'EMA8','EMA21','EMA34','EMA55','Returns','Volume']\n",
    "X_tr = train_df[features_x]\n",
    "Y_tr = train_df['Adj Close']\n",
    "x_train_scaled = sc_x.fit_transform(X_tr)\n",
    "y_train_scaled = sc_y.fit_transform(np.array(Y_tr).reshape(-1, 1))\n",
    "\n",
    "X_ts = val_df[features_x]\n",
    "Y_ts = val_df['Adj Close']\n",
    "x_val_scaled = sc_x.transform(X_ts)\n",
    "y_val_scaled = sc_y.transform(np.array(Y_ts).reshape(-1, 1))\n",
    "\n",
    "x_train_pca = pca.fit_transform(x_train_scaled)\n",
    "x_val_pca = pca.transform(x_val_scaled)\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_val = []\n",
    "Y_val = []\n",
    "\n",
    "timesteps = 7\n",
    "        # Loop for training data\n",
    "for i in range(timesteps,train_df.shape[0]):\n",
    "        \n",
    "    X_train.append(x_train_pca[i-timesteps:i])\n",
    "    Y_train.append(y_train_scaled[i][0])\n",
    "        \n",
    "X_train,Y_train = np.array(X_train),np.array(Y_train)\n",
    "\n",
    "        # Loop for val data\n",
    "for i in range(timesteps,val_df.shape[0]):\n",
    "\n",
    "    X_val.append(x_val_pca[i-timesteps:i])\n",
    "    Y_val.append(y_val_scaled[i][0])\n",
    "\n",
    "X_val,Y_val = np.array(X_val),np.array(Y_val)\n",
    "\n",
    "hl = [40,35,40,50]\n",
    "batch = 16\n",
    "epochs = 8\n",
    "\n",
    "\n",
    "# Adding Layers to the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(40,input_shape = (X_train.shape[1],X_train.shape[2]),return_sequences = True,\n",
    "                        activation = 'relu'))\n",
    "for i in range(len(hl)-1):        \n",
    "    model.add(LSTM(hl[i],activation = 'relu',return_sequences = True))\n",
    "model.add(LSTM(hl[-1],activation = 'relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "#print(model.summary())\n",
    "\n",
    "# Training the data\n",
    "history = model.fit(X_train,Y_train,epochs = epochs,batch_size = batch,validation_data = (X_val, Y_val))\n",
    "\n",
    "histories[tick] = history\n",
    "model.reset_states()\n",
    "\n",
    "# tick = 'AAPL'\n",
    "filepath_model = f\"../../../data/models/{tick}/multi_lstm\"\n",
    "model.save(filepath_model)\n",
    "file_paths = get_all_file_paths(filepath_model)\n",
    "\n",
    "\n",
    "#took help from this: https://www.geeksforgeeks.org/working-zip-files-python/\n",
    "with ZipFile(filepath_model + \".zip\",'w') as zip:\n",
    "        # writing each file one by one\n",
    "        for file in file_paths:\n",
    "            zip.write(file)\n",
    "\n",
    "\n",
    "fileName_model = \"multi_lstm.zip\"\n",
    "bucket = storage.bucket()\n",
    "#upload models\n",
    "blob = bucket.blob(f\"models/{tick}/\" + fileName_model)\n",
    "blob.upload_from_filename(filepath_model+\".zip\")\n",
    "\n",
    "# upload normalizer training data\n",
    "filepath_normalizer = f\"../../../data/normalizers/{tick}/multi_lstm_x.pkl\"\n",
    "pickle.dump(sc_x, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "filename_normalizer = \"multi_lstm_x.pkl\"\n",
    "blob = bucket.blob(f\"normalizers/{tick}/multi_lstm_x.pkl\")\n",
    "blob.upload_from_filename(filepath_normalizer)\n",
    "\n",
    "#upload normalizer training data\n",
    "filepath_normalizer = f\"../../../data/normalizers/{tick}/multi_lstm_y.pkl\"\n",
    "pickle.dump(sc_y, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "filename_normalizer = \"multi_lstm_y.pkl\"\n",
    "blob = bucket.blob(f\"normalizers/{tick}/multi_lstm_y.pkl\")\n",
    "blob.upload_from_filename(filepath_normalizer)\n",
    "\n",
    "#upload pca file\n",
    "filepath_pca = f\"../../../data/normalizers/{tick}/multi_lstm_pca.pkl\"\n",
    "pickle.dump(pca, open(filepath_pca, 'wb'))\n",
    "\n",
    "filename_pca = \"multi_lstm_pca.pkl\"\n",
    "blob = bucket.blob(f\"normalizers/{tick}/multi_lstm_pca.pkl\")\n",
    "blob.upload_from_filename(filepath_pca)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving plot images for the frontend\n",
    "def plot_error(train_loss,val_loss,file_path,tick):\n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.title(tick + ' loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(file_path)\n",
    "    plt.show()\n",
    "folder_path = '../../frontend_react/public/images/multi_lstm/'\n",
    "for tick in histories.keys():\n",
    "    plot_error(histories[tick].history['loss'],histories[tick].history['val_loss'],folder_path + tick + '_with_pca_loss.jpg',tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [00:14<00:00, 15.69it/s, Completed]                    \n",
      "Generate report structure: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.05s/it]\n",
      "Render HTML: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.33s/it]\n",
      "Export report to file: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 91.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df, title=\"Pandas Profiling Report\", explorative=True)\n",
    "\n",
    "profile.to_file(output_file='get_info.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [00:04, 12.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSLA Profit using LSTM  1006.2399217622491 39 20\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [00:04, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL Profit using LSTM  1085.9567733908993 44 15\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [00:05, 10.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOG Profit using LSTM  790.7952728446987 39 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as pdr\n",
    "from keras.models import load_model\n",
    "import pandas_datareader as pdr\n",
    "from datetime import date\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "tickers = ['TSLA', 'AAPL', 'GOOG']\n",
    "weights = [0.087,0.301,0.602]\n",
    "\n",
    "for ind,tick in enumerate(tickers):\n",
    "    weight = weights[ind]\n",
    "    rate = 0.008\n",
    "\n",
    "    #return_obj = {}\n",
    "\n",
    "    today = date.today()\n",
    "    data = yf.download(tickers=tick, period='60d', interval='5m')\n",
    "\n",
    "    data['date'] = data.index.date\n",
    "\n",
    "    start_date = '2021-8-26'\n",
    "    # end_date= \"2022-1-17\"\n",
    "    df = pdr.get_data_yahoo(tick, start=start_date, end=today)\n",
    "\n",
    "    df = df[-81:]\n",
    "\n",
    "\n",
    "    df['H-L'] = df['High'] - df['Low']\n",
    "    df['O-C'] = df['Open'] - df['Close']\n",
    "    df['7MA'] = df['Adj Close'].rolling(window=7).mean()\n",
    "    df['14MA'] = df['Adj Close'].rolling(window=14).mean()\n",
    "    df['21MA'] = df['Adj Close'].rolling(window=21).mean()\n",
    "    df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "    features = ['H-L','O-C','7MA','14MA','21MA','7SD','Volume']\n",
    "\n",
    "    scaler_path = \"../../../data/normalizers/\" + tick + \"/ann_x.pkl\"\n",
    "    model_path = \"../../../data/models/\" + tick + \"/ann\"\n",
    "\n",
    "    if(os.path.exists(model_path)):\n",
    "        model = load_model(model_path)\n",
    "    if(os.path.exists(scaler_path)):\n",
    "        with open(scaler_path, \"rb\") as input_file:\n",
    "            scaler_x = pickle.load(input_file)\n",
    "\n",
    "    scaler_path = \"../../../data/normalizers/\" + tick + \"/ann_y.pkl\"\n",
    "    if(os.path.exists(scaler_path)):\n",
    "        with open(scaler_path, \"rb\") as input_file:\n",
    "            scaler_y = pickle.load(input_file)\n",
    "\n",
    "\n",
    "    profit = 0\n",
    "    wins = 0\n",
    "    loss = 0\n",
    "    index = 0\n",
    "\n",
    "    portfolio_value = 10000\n",
    "    # error_pct = -1.5707761096672937\n",
    "    returns = []\n",
    "    open_prices = []\n",
    "    close_prices = []\n",
    "    groups = data.groupby(data.date)\n",
    "\n",
    "    for row in tqdm(df[21:-1][features].iterrows()):\n",
    "\n",
    "        present_close_price = df['Close'][index+21]\n",
    "\n",
    "        row = np.asarray(df[index +21:index+22][features], np.float32)\n",
    "        scaled_row = scaler_x.transform(row)\n",
    "        prediction = model.predict(scaled_row)  \n",
    "        prediction_value = scaler_y.inverse_transform(prediction)[0][0]\n",
    "\n",
    "        next_group = groups.get_group(data.date[index+1])\n",
    "        next_open_price = df['Open'][index+22]\n",
    "        next_close_price = df['Close'][index+22]\n",
    "        next_high_price = next_group['High'].max()\n",
    "        next_low_price = next_group['Low'].min()\n",
    "\n",
    "        weighted_value = weight*(portfolio_value+profit)\n",
    "        n_shares = weighted_value/present_close_price\n",
    "        # print(n_shares)\n",
    "\n",
    "        if(prediction_value > present_close_price):\n",
    "            boolean = False\n",
    "\n",
    "            for row in next_group.iloc():\n",
    "                if(row['High'] >= prediction_value):\n",
    "                    difference = (prediction_value - next_open_price)*n_shares\n",
    "\n",
    "                    returns.append((prediction_value-rate*prediction_value)/next_open_price)\n",
    "                    \n",
    "\n",
    "                    # differences.append(percent_change)\n",
    "                    profit += difference\n",
    "                    profit -= rate*present_close_price*n_shares\n",
    "                    if(difference > 0):\n",
    "                        wins += 1\n",
    "                    else:\n",
    "                        loss += 1\n",
    "                    \n",
    "                    boolean = True\n",
    "                    break\n",
    "            if(not boolean):\n",
    "                difference = (next_close_price - next_open_price)*n_shares\n",
    "\n",
    "                returns.append((next_close_price-rate*next_close_price)/next_open_price)\n",
    "\n",
    "                # differences.append(percent_change)\n",
    "                profit += difference\n",
    "                profit -= rate*present_close_price*n_shares\n",
    "                if(difference > 0):\n",
    "                    wins += 1\n",
    "                else:\n",
    "                    loss += 1\n",
    "        \n",
    "        else:\n",
    "            for row in next_group.iloc():\n",
    "                boolean = False\n",
    "                if(row['Low'] <= prediction_value):\n",
    "\n",
    "                    difference = (next_open_price - prediction_value)*n_shares\n",
    "                    \n",
    "                    returns.append((next_open_price-rate*next_open_price)/prediction_value)\n",
    "                    # differences.append(percent_change)\n",
    "\n",
    "                    profit += difference\n",
    "                    profit -= rate*present_close_price*n_shares\n",
    "                    if(difference > 0):\n",
    "                        wins += 1\n",
    "                    else:\n",
    "                        loss += 1\n",
    "                    \n",
    "                    boolean = True\n",
    "                    break\n",
    "            if(not boolean):\n",
    "\n",
    "                difference = (next_open_price - next_close_price)*n_shares\n",
    "                    \n",
    "                returns.append((next_open_price-rate*next_open_price)/next_close_price)\n",
    "                # differences.append(percent_change)\n",
    "\n",
    "                profit += difference\n",
    "                profit -= rate*present_close_price*n_shares\n",
    "                if(difference > 0):\n",
    "                    wins += 1\n",
    "                else:\n",
    "                    loss += 1\n",
    "\n",
    "        index +=1\n",
    "                \n",
    "\n",
    "    print(f'{tick} Profit using LSTM ', profit, wins, loss)\n",
    "    \n",
    "\n",
    "\n",
    "return_obj[tick] = returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.93145235, 0.36614816, 5.20677842])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_returns = pd.DataFrame(return_obj)\n",
    "log_return = np.log(df_returns)\n",
    "# weights = [0.087,0.311,0.602]\n",
    "\n",
    "def get_ret_vol_sr(weights): \n",
    "    weights = np.array(weights)\n",
    "    ret = np.sum(log_return.mean() * weights) * 252\n",
    "    vol = np.sqrt(np.dot(weights.T,np.dot(log_return.cov()*252,weights)))\n",
    "    risk_free_rate = 0.025\n",
    "    sr = (ret-risk_free_rate)/vol \n",
    "    return np.array([ret,vol,sr])\n",
    "\n",
    "get_ret_vol_sr(weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_obj_lstm = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "TSLA Profit using LSTM  1433.7394481471229 45 14\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "AAPL Profit using LSTM  -167.03675766487433 36 23\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "GOOG Profit using LSTM  -108.62851300315634 35 24\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as pdr\n",
    "from keras.models import load_model\n",
    "import pandas_datareader as pdr\n",
    "from datetime import date\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "tickers = ['TSLA', 'AAPL', 'GOOG']\n",
    "weights = [0.087,0.301,0.602]\n",
    "\n",
    "for ind,tick in enumerate(tickers):\n",
    "    weight = weights[ind]\n",
    "    rate = 0.008\n",
    "\n",
    "\n",
    "    data = yf.download(tickers=tick, period='60d', interval='5m')\n",
    "\n",
    "    data['date'] = data.index.date\n",
    "\n",
    "    start_date = '2021-8-26'\n",
    "    end_date= \"2022-1-17\"\n",
    "    data_new = pdr.get_data_yahoo(tick, start=start_date, end=end_date)\n",
    "\n",
    "    closing_prices = list(data_new['Close'])\n",
    "    open_prices = list(data_new['Open'])\n",
    "\n",
    "    closing_prices_present = []\n",
    "    open_prices_present = []\n",
    "\n",
    "    for name, group in data.groupby(data.date):\n",
    "    \n",
    "        closing_prices_present.append(list(group[-1:]['Close'])[-1])\n",
    "        open_prices_present.append(list(group[:1]['Open'])[-1])\n",
    "\n",
    "    closing_prices.extend(closing_prices_present)\n",
    "    open_prices.extend(open_prices_present)\n",
    "\n",
    "\n",
    "    scaler_path = \"../../../data/normalizers/\" + tick + \"/lstm.pkl\"\n",
    "    model_path = \"../../../data/models/\" + tick + \"/lstm\"\n",
    "\n",
    "\n",
    "    if(os.path.exists(model_path)):\n",
    "        model = load_model(model_path)\n",
    "    if(os.path.exists(scaler_path)):\n",
    "        with open(scaler_path, \"rb\") as input_file:\n",
    "            scaler = pickle.load(input_file)\n",
    "\n",
    "    dataset = scaler.transform(np.array(closing_prices).reshape(-1,1))\n",
    "\n",
    "    time_step = 100\n",
    "    profit = 0\n",
    "    wins = 0\n",
    "    loss = 0\n",
    "\n",
    "    portfolio_value = 10000\n",
    "    # error_pct = -1.5707761096672937\n",
    "    returns = []\n",
    "\n",
    "\n",
    "    groups = data.groupby(data.date)\n",
    "    \n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        present_close_price = closing_prices[i+time_step-1]\n",
    "\n",
    "        next_group = groups.get_group(data.date[i+1])\n",
    "        next_open_price = open_prices[i+time_step]\n",
    "        next_close_price = closing_prices[i+time_step]\n",
    "        next_high_price = next_group['High'].max()\n",
    "        next_low_price = next_group['Low'].min()\n",
    "\n",
    "        data_pred = dataset[i:(i+time_step), 0]\n",
    "        test_data = data_pred.reshape(-1,1)\n",
    "        test_data = np.array([test_data,])\n",
    "        prediction = model.predict(test_data)\n",
    "        prediction_value = scaler.inverse_transform(prediction)[0][0]\n",
    "\n",
    "        weighted_value = weight*(portfolio_value+profit)\n",
    "        n_shares = weighted_value/present_close_price\n",
    "        # print(n_shares)\n",
    "\n",
    "        if(prediction_value > present_close_price):\n",
    "            boolean = False\n",
    "\n",
    "            for row in next_group.iloc():\n",
    "                if(row['High'] >= prediction_value):\n",
    "                    difference = (prediction_value - next_open_price)*n_shares\n",
    "                    returns.append((prediction_value-rate*prediction_value)/next_open_price)\n",
    "\n",
    "                    # differences.append(percent_change)\n",
    "                    profit += difference\n",
    "                    profit -= rate*present_close_price*n_shares\n",
    "                    if(difference > 0):\n",
    "                        wins += 1\n",
    "                    else:\n",
    "                        loss += 1\n",
    "                    \n",
    "                    boolean = True\n",
    "                    break\n",
    "            if(not boolean):\n",
    "                difference = (next_close_price - next_open_price)*n_shares\n",
    "                returns.append((next_close_price-rate*next_close_price)/next_open_price)\n",
    "\n",
    "                # differences.append(percent_change)\n",
    "                profit += difference\n",
    "                profit -= rate*present_close_price*n_shares\n",
    "                if(difference > 0):\n",
    "                    wins += 1\n",
    "                else:\n",
    "                    loss += 1\n",
    "        \n",
    "        else:\n",
    "            for row in next_group.iloc():\n",
    "                boolean = False\n",
    "                if(row['Low'] <= prediction_value):\n",
    "\n",
    "                    difference = (next_open_price - prediction_value)*n_shares\n",
    "                    \n",
    "                    returns.append((next_open_price-rate*next_open_price)/prediction_value)\n",
    "                    # differences.append(percent_change)\n",
    "\n",
    "                    profit += difference\n",
    "                    profit -= rate*present_close_price*n_shares\n",
    "                    if(difference > 0):\n",
    "                        wins += 1\n",
    "                    else:\n",
    "                        loss += 1\n",
    "                    \n",
    "                    boolean = True\n",
    "                    break\n",
    "            if(not boolean):\n",
    "\n",
    "                difference = (next_open_price - next_close_price)*n_shares\n",
    "                    \n",
    "                returns.append((next_open_price-rate*next_open_price)/next_close_price)\n",
    "                # differences.append(percent_change)\n",
    "\n",
    "                profit += difference\n",
    "                profit -= rate*present_close_price*n_shares\n",
    "                if(difference > 0):\n",
    "                    wins += 1\n",
    "                else:\n",
    "                    loss += 1\n",
    "                \n",
    "\n",
    "    print(f'{tick} Profit using LSTM ', profit, wins, loss)\n",
    "    \n",
    "\n",
    "    returns_obj_lstm[tick] = returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46233293, 0.33061453, 1.32278799])"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_returns = pd.DataFrame(returns_obj_lstm)\n",
    "log_return = np.log(df_returns)\n",
    "\n",
    "def get_ret_vol_sr(weights): \n",
    "    weights = np.array(weights)\n",
    "    ret = np.sum(log_return.mean() * weights) * 252\n",
    "    vol = np.sqrt(np.dot(weights.T,np.dot(log_return.cov()*252,weights)))\n",
    "    risk_free_rate = 0.025\n",
    "    sr = (ret-risk_free_rate)/vol \n",
    "    return np.array([ret,vol,sr])\n",
    "\n",
    "get_ret_vol_sr(weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining NLP and Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "36/36 [==============================] - 18s 160ms/step - loss: 0.1626 - val_loss: 0.0621\n",
      "Epoch 2/15\n",
      "36/36 [==============================] - 3s 91ms/step - loss: 0.0708 - val_loss: 0.0305\n",
      "Epoch 3/15\n",
      "36/36 [==============================] - 4s 112ms/step - loss: 0.0592 - val_loss: 0.0820\n",
      "Epoch 4/15\n",
      "36/36 [==============================] - 3s 95ms/step - loss: 0.0263 - val_loss: 0.0178\n",
      "Epoch 5/15\n",
      "36/36 [==============================] - 3s 93ms/step - loss: 0.0213 - val_loss: 0.0101\n",
      "Epoch 6/15\n",
      "36/36 [==============================] - 3s 94ms/step - loss: 0.0088 - val_loss: 0.0168\n",
      "Epoch 7/15\n",
      "36/36 [==============================] - 3s 96ms/step - loss: 0.0066 - val_loss: 0.0141\n",
      "Epoch 8/15\n",
      "36/36 [==============================] - 3s 92ms/step - loss: 0.0056 - val_loss: 0.0117\n",
      "Epoch 9/15\n",
      "36/36 [==============================] - 3s 92ms/step - loss: 0.0054 - val_loss: 0.0106\n",
      "Epoch 10/15\n",
      "36/36 [==============================] - 3s 91ms/step - loss: 0.0055 - val_loss: 0.0100\n",
      "Epoch 11/15\n",
      "36/36 [==============================] - 4s 101ms/step - loss: 0.0057 - val_loss: 0.0096\n",
      "Epoch 12/15\n",
      "36/36 [==============================] - 3s 92ms/step - loss: 0.0060 - val_loss: 0.0094\n",
      "Epoch 13/15\n",
      "36/36 [==============================] - 3s 93ms/step - loss: 0.0064 - val_loss: 0.0087\n",
      "Epoch 14/15\n",
      "36/36 [==============================] - 3s 90ms/step - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 15/15\n",
      "36/36 [==============================] - 3s 97ms/step - loss: 0.0062 - val_loss: 0.0078\n",
      "INFO:tensorflow:Assets written to: ../../../data/models/TSLA/combination_model1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../../data/models/TSLA/combination_model1\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTM object at 0x000001866F2F6610> has the same name 'LSTM' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTM'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTM object at 0x00000186C77E15E0> has the same name 'LSTM' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTM'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTM object at 0x000001864B104340> has the same name 'LSTM' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTM'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTM object at 0x00000186C78024F0> has the same name 'LSTM' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTM'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000186C6648F70> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000186C3BC5C10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000186C7B620D0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001866FB75A60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import os\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from firebase_admin import storage\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas_ta as ta\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from pandas_datareader import data as pdr\n",
    "from datetime import date\n",
    "import yfinance as yf\n",
    "\n",
    "yf.pdr_override()\n",
    "import pandas as pd\n",
    "\n",
    "ticker_list=[\"AAPL\", \"MSFT\", \"AMZN\", \"TSLA\", \"GOOGL\"]\n",
    "today = date.today()\n",
    "# We can get data by our choice by giving days bracket\n",
    "start_date= \"2015-01-01\"\n",
    "end_date=\"2020-11-30\"\n",
    "\n",
    "files=[]\n",
    "def getData(ticker):\n",
    "    data = pdr.get_data_yahoo(ticker, start=start_date, end=today)\n",
    "    # dataname= ticker+\"_\"+str(today)\n",
    "    files.append((data,ticker))\n",
    "    \n",
    "for tik in ticker_list:\n",
    "    getData(tik)\n",
    "\n",
    "histories = {}\n",
    "for tick in data.keys():\n",
    "    # stock_data = data[tick]\n",
    "    stock_data = files[1][0]\n",
    "    df = pd.DataFrame(stock_data).T\n",
    "    df['H-L'] = df['High'] - df['Low']\n",
    "    df['O-C'] = df['Open'] - df['Close']\n",
    "    df['5MA'] = df['Adj Close'].rolling(window=5).mean()\n",
    "    df['10MA'] = df['Adj Close'].rolling(window=10).mean()\n",
    "    df['20MA'] = df['Adj Close'].rolling(window=20).mean()\n",
    "    df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "    df[\"EMA8\"] = df['Adj Close'].ewm(span=8).mean()\n",
    "    df[\"EMA21\"] = df['Adj Close'].ewm(span=21).mean()\n",
    "    df['EMA34'] = df['Adj Close'].ewm(span=34).mean()\n",
    "    df['EMA55'] = df['Adj Close'].ewm(span=55).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    df['Returns'] = df['Close'] / df['Close'].shift(1)\n",
    "    df['Returns'] -= 1\n",
    "    df.dropna(inplace=True)\n",
    "    df.ta.rsi(close='Close', length=14, append=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    features = ['Adj Close','H-L','O-C','5MA','10MA','20MA','7SD','RSI_14', 'EMA8','EMA21','EMA34','EMA55','Returns','Volume']\n",
    "    df = df[features].apply(pd.to_numeric)\n",
    "\n",
    "    #make the index as a column and rename it to 'Date'\n",
    "    df.reset_index(inplace = True)\n",
    "    df.rename(columns = {'index': 'Date'}, inplace = True)\n",
    "\n",
    "    #read the news sentiment data given by Varun\n",
    "    news_sentiment = pd.read_csv(f'../../../data/sentiment data/{tick}_data.csv')\n",
    "    news_sentiment.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "    #merge on date\n",
    "    df_comb = pd.merge(news_sentiment, df, how='inner', on='Date')\n",
    "\n",
    "    #replace index to date again\n",
    "    df_comb.set_index('Date', inplace = True)\n",
    "\n",
    "    #drop the buy sell columns\n",
    "    df_comb.drop(['Buy', 'Sell'], inplace = True, axis = 1)\n",
    "\n",
    "    train_data = int(0.9*len(df_comb))\n",
    "\n",
    "    train_df,val_df = df_comb[1:train_data], df_comb[train_data:]\n",
    "    sc_x = MinMaxScaler()\n",
    "    sc_y = MinMaxScaler()\n",
    "\n",
    "\n",
    "    features_x = ['H-L','O-C','5MA','10MA','20MA','7SD','RSI_14', 'EMA8','EMA21','EMA34','EMA55','Returns','Volume', 'Vander_Score']\n",
    "    X_tr = train_df[features_x]\n",
    "    Y_tr = train_df['Adj Close']\n",
    "    x_train_scaled = sc_x.fit_transform(X_tr)\n",
    "    y_train_scaled = sc_y.fit_transform(np.array(Y_tr).reshape(-1, 1))\n",
    "\n",
    "    X_ts = val_df[features_x]\n",
    "    Y_ts = val_df['Adj Close']\n",
    "    x_val_scaled = sc_x.transform(X_ts)\n",
    "    y_val_scaled = sc_y.transform(np.array(Y_ts).reshape(-1, 1))\n",
    "\n",
    "    pca = PCA(n_components=4)\n",
    "    x_train_pca = pca.fit_transform(x_train_scaled)\n",
    "    x_val_pca = pca.transform(x_val_scaled)\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_val = []\n",
    "    Y_val = []\n",
    "\n",
    "    timesteps = 7\n",
    "        # Loop for training data\n",
    "    for i in range(timesteps,x_train_scaled.shape[0]):\n",
    "        \n",
    "        X_train.append(x_train_pca[i-timesteps:i])\n",
    "        Y_train.append(y_train_scaled[i][0])\n",
    "        \n",
    "    X_train,Y_train = np.array(X_train),np.array(Y_train)\n",
    "\n",
    "        # Loop for val data\n",
    "    for i in range(timesteps,x_val_scaled.shape[0]):\n",
    "\n",
    "        X_val.append(x_val_pca[i-timesteps:i])\n",
    "        Y_val.append(y_val_scaled[i][0])\n",
    "\n",
    "    X_val,Y_val = np.array(X_val),np.array(Y_val)\n",
    "\n",
    "    hl = [40,35,50]\n",
    "    batch = 32\n",
    "    epochs = 15\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(40,input_shape = (X_train.shape[1],X_train.shape[2]),return_sequences = True,\n",
    "                        activation = 'relu'))\n",
    "    for i in range(len(hl)-1):        \n",
    "        model.add(LSTM(hl[i],activation = 'relu',return_sequences = True))\n",
    "    model.add(LSTM(hl[-1],activation = 'relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "        #print(model.summary())\n",
    "\n",
    "    # Training the data\n",
    "    history = model.fit(X_train,Y_train,epochs = epochs,batch_size = batch,validation_data = (X_val, Y_val),\n",
    "                            shuffle = False)\n",
    "\n",
    "    histories[tick] = history\n",
    "    model.reset_states()\n",
    "\n",
    "    filepath_model = \"../../../data/models/\" + tick + \"/combination_model1\"\n",
    "    model.save(filepath_model)\n",
    "    file_paths = get_all_file_paths(filepath_model)\n",
    "\n",
    "\n",
    "    #took help from this: https://www.geeksforgeeks.org/working-zip-files-python/\n",
    "    with ZipFile(filepath_model + \".zip\",'w') as zip:\n",
    "            # writing each file one by one\n",
    "            for file in file_paths:\n",
    "                zip.write(file)\n",
    "\n",
    "\n",
    "    fileName_model = \"combination_model1.zip\"\n",
    "    bucket = storage.bucket()\n",
    "    #upload models\n",
    "    blob = bucket.blob(\"models/\" + tick + \"/\" + fileName_model)\n",
    "    blob.upload_from_filename(filepath_model+\".zip\")\n",
    "\n",
    "    #upload normalizer features data\n",
    "    filepath_normalizer = \"../../../data/normalizers/\" + tick + \"/combination_model1_X.pkl\"\n",
    "    pickle.dump(sc_x, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "    filename_normalizer = \"combination_model1.pkl\"\n",
    "    blob = bucket.blob(\"normalizers/\" + tick + \"/combination_model1_X.pkl\")\n",
    "    blob.upload_from_filename(filepath_normalizer)\n",
    "\n",
    "    #upload normalizer labels data\n",
    "    filepath_normalizer = \"../../../data/normalizers/\" + tick + \"/combination_model1_Y.pkl\"\n",
    "    pickle.dump(sc_y, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "    filename_normalizer = \"combination_model1.pkl\"\n",
    "    blob = bucket.blob(\"normalizers/\" + tick + \"/combination_model1_Y.pkl\")\n",
    "    blob.upload_from_filename(filepath_normalizer)\n",
    "\n",
    "    #upload normalizer labels data\n",
    "    filepath_normalizer = \"../../../data/normalizers/\" + tick + \"/combination_model1_pca.pkl\"\n",
    "    pickle.dump(pca, open(filepath_normalizer, 'wb'))\n",
    "\n",
    "    filename_normalizer = \"combination_model1.pkl\"\n",
    "    blob = bucket.blob(\"normalizers/\" + tick + \"/combination_model1_pca.pkl\")\n",
    "    blob.upload_from_filename(filepath_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#saving plot images for the frontend\n",
    "def plot_error(train_loss,val_loss,file_path,tick):\n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.title(tick + ' loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(file_path)\n",
    "    plt.show()\n",
    "folder_path = '../../frontend_react/public/images/combined_model1/'\n",
    "for tick in histories.keys():\n",
    "    plot_error(histories[tick].history['loss'],histories[tick].history['val_loss'],folder_path + tick + '_loss.jpg',tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi lstm model loading\n",
      "loaded models\n",
      "got data till 7 days\n",
      "read sentiment score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Buy'"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from flask import Flask, request,jsonify\n",
    "import os\n",
    "from pandas_datareader import data as pdr\n",
    "from datetime import date, timedelta\n",
    "import yfinance as yf\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas_ta as ta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def get_signal(tick):\n",
    "    # tick = 'AAPL'\n",
    "    today = date.today()\n",
    "    print('multi lstm model loading')\n",
    "    scaler_x_path = \"../../../data/normalizers/\" + tick + \"/combination_model1_X.pkl\"\n",
    "    scaler_y_path = \"../../../data/normalizers/\" + tick + \"/combination_model1_Y.pkl\"\n",
    "    model_path = \"../../../data/models/\" + tick + \"/combination_model1\"\n",
    "    pca_path = \"../../../data/normalizers/\" + tick + \"/combination_model1_pca.pkl\"\n",
    "\n",
    "    if(os.path.exists(model_path)):\n",
    "        model = load_model(model_path)\n",
    "\n",
    "    if(os.path.exists(scaler_x_path)):\n",
    "        with open(scaler_x_path, \"rb\") as input_file:\n",
    "            scaler_x = pickle.load(input_file)\n",
    "            \n",
    "    if(os.path.exists(scaler_y_path)):\n",
    "        with open(scaler_y_path, \"rb\") as input_file:\n",
    "            scaler_y = pickle.load(input_file)\n",
    "    if(os.path.exists(pca_path)):\n",
    "        with open(pca_path, \"rb\") as input_file:\n",
    "            pca = pickle.load(input_file)\n",
    "\n",
    "    print('loaded models')\n",
    "\n",
    "    def getTestData(ticker, start): \n",
    "        data = pdr.get_data_yahoo(ticker, start=start, end=today)\n",
    "        # dataname= ticker+\"_\"+str(today)\n",
    "        return data[-350:-1]\n",
    "                    \n",
    "            \n",
    "    start = today - timedelta(days=500)\n",
    "\n",
    "    df = getTestData(tick,start) \n",
    "    storing_data = df['Close'].copy().to_json() \n",
    "\n",
    "    df['H-L'] = df['High'] - df['Low']\n",
    "    df['O-C'] = df['Open'] - df['Close']\n",
    "    df['5MA'] = df['Adj Close'].rolling(window=5).mean()\n",
    "    df['10MA'] = df['Adj Close'].rolling(window=10).mean()\n",
    "    df['20MA'] = df['Adj Close'].rolling(window=20).mean()\n",
    "    df['7SD'] = df['Adj Close'].rolling(window=7).std()\n",
    "    df[\"EMA8\"] = df['Adj Close'].ewm(span=8).mean()\n",
    "    df[\"EMA21\"] = df['Adj Close'].ewm(span=21).mean()\n",
    "    df['EMA34'] = df['Adj Close'].ewm(span=34).mean()\n",
    "    df['EMA55'] = df['Adj Close'].ewm(span=55).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    df['Returns'] = df['Close'] / df['Close'].shift(1)\n",
    "    df['Returns'] -= 1\n",
    "    df.dropna(inplace=True)\n",
    "    df.ta.rsi(close='Close', length=14, append=True)\n",
    "    df.dropna(inplace=True)\n",
    "    previous_closing_price = list(df['Close'])[-1]\n",
    "\n",
    "    features = ['Adj Close','H-L','O-C','5MA','10MA','20MA','7SD','RSI_14', 'EMA8','EMA21','EMA34','EMA55','Returns','Volume']\n",
    "    df = df[features].apply(pd.to_numeric)\n",
    "\n",
    "    df = df[-7:]\n",
    "\n",
    "    # date_today = '2022-04-13'\n",
    "    print('got data till 7 days')\n",
    "    # tick = 'GOOG'\n",
    "    df_sentiment = pd.read_csv(f'{today}_{tick}.csv')\n",
    "    df['Vander_Score']= list(df_sentiment['Score'])\n",
    "    print('read sentiment score')\n",
    "    features_x = ['H-L','O-C','5MA','10MA','20MA','7SD','RSI_14', 'EMA8','EMA21','EMA34','EMA55','Returns','Volume', 'Vander_Score']\n",
    "    scaled_x_data = scaler_x.transform(df[features_x])\n",
    "\n",
    "    x_test_pca = pca.transform(scaled_x_data)\n",
    "\n",
    "    scaled_data = x_test_pca.reshape((1,7,4))\n",
    "\n",
    "    test_data = np.asarray(scaled_data, np.float32)\n",
    "    pred = model.predict(test_data)  \n",
    "    prediction_value = scaler_y.inverse_transform(pred)\n",
    "    \n",
    "    \n",
    "    if(prediction_value > previous_closing_price):\n",
    "        return 'Buy'\n",
    "    else:\n",
    "        return 'Sell'\n",
    "\n",
    "get_signal('AAPL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca1edcbf756503ef84b6dec62a8a514ccc7037ff00e4b32f050febc64cc6ba90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('FYP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
